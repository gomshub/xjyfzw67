Here is your complete Python script with:
	•	Configurable queue name, vhost, host, etc.
	•	File type (XML or JSON) parsing
	•	Exchange and queue binding
	•	Queue arguments (x-consumer, x-version, content-type)
	•	Deduplication of messages
	•	Automatic compression of final XML
	•	Graceful shutdown with cleanup

⸻

✅ Full Script

import pika
import xml.etree.ElementTree as ET
import hashlib
import gzip
import json
from datetime import datetime
import time
import argparse
import signal
import sys

# =================== CONFIGURATION ====================
RABBITMQ_HOST = 'localhost'
RABBITMQ_PORT = 5672
RABBITMQ_USER = 'guest'
RABBITMQ_PASSWORD = 'guest'
RABBITMQ_VHOST = '/'

EXCHANGE_NAME = 'my_exchange'
EXCHANGE_TYPE = 'headers'  # 'direct', 'topic', 'fanout', 'headers'
ROUTING_KEY = 'my.routing.key'  # Used if not using headers exchange

FILE_TYPE = 'xml'  # 'xml' or 'json'
IDLE_TIMEOUT = 10  # seconds with no messages to exit
POLL_INTERVAL = 0.5  # polling interval for new messages

QUEUE_ARGUMENTS = {
    "x-consumer": "my-consumer",
    "x-version": "v1.2",
    "content-type": "application/xml"
}

BINDING_ARGUMENTS = {
    "x-match": "all",
    "x-consumer": "my-consumer",
    "x-version": "v1.2",
    "content-type": "application/xml"
}
# =======================================================

connection = None
channel = None
queue_name = None
message_hashes = set()
messages_root = ET.Element("Messages")
total_received = 0

def extract_from_xml(body):
    try:
        root = ET.fromstring(body)
    except ET.ParseError as e:
        print(f"Invalid XML, skipping: {e}")
        return []

    messages = []
    if root.tag == "Message":
        messages.append(root)
    elif root.tag == "Messages":
        messages.extend(root.findall("Message"))
    else:
        messages.extend(root.findall(".//Message"))
    return messages

def extract_from_json(body):
    try:
        json_data = json.loads(body)
    except json.JSONDecodeError as e:
        print(f"Invalid JSON, skipping: {e}")
        return []

    content = json_data.get("message") or json_data.get("body") or json_data
    message_elem = ET.Element("Message")
    message_elem.text = json.dumps(content) if isinstance(content, (dict, list)) else str(content)
    return [message_elem]

def get_message_hash(elem):
    return hashlib.sha256(ET.tostring(elem, encoding='utf-8')).hexdigest()

def consume_messages(max_count=None):
    global total_received
    last_message_time = time.time()
    print("\nPolling messages...")

    while True:
        method_frame, header_frame, body = channel.basic_get(queue=queue_name, auto_ack=True)

        if method_frame:
            try:
                body_str = body.decode('utf-8')
            except UnicodeDecodeError:
                print("Non-UTF-8 message, skipping.")
                continue

            if FILE_TYPE == 'xml':
                messages = extract_from_xml(body_str)
            elif FILE_TYPE == 'json':
                messages = extract_from_json(body_str)
            else:
                print(f"Unsupported FILE_TYPE: {FILE_TYPE}")
                break

            for msg in messages:
                msg_hash = get_message_hash(msg)
                if msg_hash not in message_hashes:
                    messages_root.append(msg)
                    message_hashes.add(msg_hash)
                    total_received += 1
                    print(f"[{total_received}] Message ingested.")

            last_message_time = time.time()
            if max_count and total_received >= max_count:
                print("Reached specified count. Stopping...")
                break
        else:
            if not max_count and (time.time() - last_message_time > IDLE_TIMEOUT):
                print("No more messages. Ending...")
                break
            time.sleep(POLL_INTERVAL)

def compress_and_save_xml():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"messages_{total_received}_msgs_{timestamp}.xml.gz"
    xml_bytes = ET.tostring(messages_root, encoding="utf-8", xml_declaration=True)

    with gzip.open(filename, "wb") as f:
        f.write(xml_bytes)

    print(f"\nTotal messages ingested: {total_received}")
    print(f"Compressed file saved to: {filename}")

def cleanup_and_exit(signum=None, frame=None):
    print("\nCleaning up...")
    try:
        if channel and queue_name:
            channel.queue_delete(queue=queue_name)
            print(f"Deleted temporary queue: {queue_name}")
        if connection:
            connection.close()
    except Exception as e:
        print(f"Error during cleanup: {e}")
    compress_and_save_xml()
    sys.exit(0)

def main():
    global connection, channel, queue_name

    parser = argparse.ArgumentParser(description="Consume messages from RabbitMQ and convert to compressed XML.")
    parser.add_argument('--count', type=int, help="Max number of messages to ingest.")
    args = parser.parse_args()

    signal.signal(signal.SIGINT, cleanup_and_exit)
    signal.signal(signal.SIGTERM, cleanup_and_exit)

    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASSWORD)
    params = pika.ConnectionParameters(
        host=RABBITMQ_HOST,
        port=RABBITMQ_PORT,
        virtual_host=RABBITMQ_VHOST,
        credentials=credentials
    )

    connection = pika.BlockingConnection(params)
    channel = connection.channel()

    # Declare exchange
    channel.exchange_declare(exchange=EXCHANGE_NAME, exchange_type=EXCHANGE_TYPE, durable=True)

    # Declare queue with arguments
    result = channel.queue_declare(queue='', exclusive=True, auto_delete=True, arguments=QUEUE_ARGUMENTS)
    queue_name = result.method.queue

    # Bind with binding arguments (used in headers exchange)
    if EXCHANGE_TYPE == "headers":
        channel.queue_bind(exchange=EXCHANGE_NAME, queue=queue_name, arguments=BINDING_ARGUMENTS)
    else:
        channel.queue_bind(exchange=EXCHANGE_NAME, queue=queue_name, routing_key=ROUTING_KEY)

    print(f"Queue '{queue_name}' bound to exchange '{EXCHANGE_NAME}' with routing key or headers.")

    consume_messages(max_count=args.count)
    cleanup_and_exit()

if __name__ == "__main__":
    main()


⸻

✅ To Run

Basic run (auto-detects end):

python ingest_rabbitmq.py

Test run with count:

python ingest_rabbitmq.py --count 50


⸻

Let me know if you want to:
	•	Save as JSON instead of XML
	•	Send acknowledgment manually
	•	Push results to S3 or database after compression

I’m happy to adapt this further!

=#=#=

import pika
import xml.etree.ElementTree as ET
import hashlib
import gzip
from datetime import datetime
import time
import argparse
import signal
import sys

# =================== CONFIGURABLE PARAMETERS ====================
RABBITMQ_HOST = 'localhost'
RABBITMQ_PORT = 5672
RABBITMQ_USER = 'guest'
RABBITMQ_PASSWORD = 'guest'
RABBITMQ_VHOST = '/'
QUEUE_NAME_PREFIX = 'temp_xml_queue_'  # will be suffixed randomly
IDLE_TIMEOUT = 10   # seconds to wait before declaring "no more messages"
POLL_INTERVAL = 0.5 # seconds between polling for messages
# ===============================================================

# Global state
connection = None
channel = None
queue_name = None
message_hashes = set()
messages_root = ET.Element("Messages")
total_received = 0

def extract_messages_only(body):
    try:
        root = ET.fromstring(body)
    except ET.ParseError as e:
        print(f"Invalid XML, skipping: {e}")
        return []

    messages = []
    if root.tag == "Message":
        messages.append(root)
    elif root.tag == "Messages":
        messages.extend(root.findall("Message"))
    else:
        messages.extend(root.findall(".//Message"))

    return messages

def get_message_hash(elem):
    return hashlib.sha256(ET.tostring(elem, encoding='utf-8')).hexdigest()

def consume_messages(max_count=None):
    global total_received
    last_message_time = time.time()

    print("\nPolling messages...")
    while True:
        method_frame, header_frame, body = channel.basic_get(queue=queue_name, auto_ack=True)

        if method_frame:
            messages = extract_messages_only(body)
            for msg in messages:
                msg_hash = get_message_hash(msg)
                if msg_hash not in message_hashes:
                    messages_root.append(msg)
                    message_hashes.add(msg_hash)
                    total_received += 1
                    print(f"[{total_received}] Message ingested.")

            last_message_time = time.time()

            if max_count and total_received >= max_count:
                print("Reached specified count. Stopping...")
                break
        else:
            if not max_count and (time.time() - last_message_time > IDLE_TIMEOUT):
                print("No more messages. Ending...")
                break
            time.sleep(POLL_INTERVAL)

def compress_and_save_xml():
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"messages_{total_received}_msgs_{timestamp}.xml.gz"
    xml_bytes = ET.tostring(messages_root, encoding="utf-8", xml_declaration=True)

    with gzip.open(filename, "wb") as f:
        f.write(xml_bytes)

    print(f"\nTotal messages ingested: {total_received}")
    print(f"Compressed file saved to: {filename}")

def cleanup_and_exit(signum=None, frame=None):
    print("\nCleaning up...")
    try:
        if channel and queue_name:
            channel.queue_delete(queue=queue_name)
            print(f"Deleted temporary queue: {queue_name}")
        if connection:
            connection.close()
    except Exception as e:
        print(f"Error during cleanup: {e}")
    compress_and_save_xml()
    sys.exit(0)

def main():
    global connection, channel, queue_name

    parser = argparse.ArgumentParser(description="Consume messages from a RabbitMQ queue and frame into compressed XML.")
    parser.add_argument('--count', type=int, help="Max number of messages to ingest. Optional.")
    args = parser.parse_args()

    signal.signal(signal.SIGINT, cleanup_and_exit)
    signal.signal(signal.SIGTERM, cleanup_and_exit)

    credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASSWORD)
    params = pika.ConnectionParameters(
        host=RABBITMQ_HOST,
        port=RABBITMQ_PORT,
        virtual_host=RABBITMQ_VHOST,
        credentials=credentials
    )

    connection = pika.BlockingConnection(params)
    channel = connection.channel()

    result = channel.queue_declare(queue='', exclusive=True, auto_delete=True)
    queue_name = result.method.queue
    print(f"Created temporary queue: {queue_name}")

    consume_messages(max_count=args.count)
    cleanup_and_exit()

if __name__ == "__main__":
    main()


=#=##
import pika
import xml.etree.ElementTree as ET
from datetime import datetime
import time
import argparse
import gzip

# RabbitMQ settings
RABBITMQ_HOST = 'localhost'
QUEUE_NAME = 'xml_queue'
IDLE_TIMEOUT = 10  # seconds

# XML root setup
root = ET.Element("Root")  # temporary; will hold Header + Messages
header_added = False
messages_elem = ET.SubElement(root, "Messages")
total_received = 0

def parse_message_and_extract_parts(body):
    try:
        msg_root = ET.fromstring(body)
    except ET.ParseError as e:
        print(f"Skipping invalid XML: {e}")
        return None, []

    header = None
    messages = []

    # Extract Header if exists
    if msg_root.tag == "Header":
        header = msg_root
    else:
        for child in msg_root:
            if child.tag == "Header" and header is None:
                header = child
            elif child.tag == "Messages":
                for msg in child.findall("Message"):
                    messages.append(msg)
            elif child.tag == "Message":
                messages.append(child)

    return header, messages

def consume_messages(max_count=None):
    global total_received, header_added

    connection = pika.BlockingConnection(pika.ConnectionParameters(host=RABBITMQ_HOST))
    channel = connection.channel()
    channel.queue_declare(queue=QUEUE_NAME)

    last_message_time = time.time()

    while True:
        method_frame, header_frame, body = channel.basic_get(queue=QUEUE_NAME, auto_ack=True)

        if method_frame:
            header, message_elements = parse_message_and_extract_parts(body)

            # Add header only once
            if not header_added and header is not None:
                root.insert(0, header)
                header_added = True

            for msg in message_elements:
                messages_elem.append(msg)
                total_received += 1
                print(f"Received message #{total_received}")

            last_message_time = time.time()

            if max_count and total_received >= max_count:
                print(f"Reached test limit of {max_count} messages.")
                break
        else:
            if not max_count and time.time() - last_message_time > IDLE_TIMEOUT:
                print("Queue idle. Stopping.")
                break
            time.sleep(0.1)

    connection.close()
    write_to_compressed_file(max_count)

def write_to_compressed_file(count=None):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    mode = f"{count}_expected" if count else "all"
    filename = f"framed_{mode}_{total_received}_msgs_{timestamp}.xml.gz"

    xml_bytes = ET.tostring(root, encoding="utf-8", xml_declaration=True)

    with gzip.open(filename, "wb") as f:
        f.write(xml_bytes)

    print(f"\nTotal messages ingested: {total_received}")
    print(f"Compressed XML saved to: {filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Consume XML messages from RabbitMQ and frame into compressed XML file.")
    parser.add_argument('--count', type=int, help="Number of messages to read (test mode). Leave empty to consume all.")
    args = parser.parse_args()

    consume_messages(max_count=args.count)


=##=
Great — let’s add compression support so the final XML is written as a .xml.gz file using Gzip.

⸻

Final Script (with .xml.gz compression)

import pika
import xml.etree.ElementTree as ET
from datetime import datetime
import time
import argparse
import gzip

# RabbitMQ settings
RABBITMQ_HOST = 'localhost'
QUEUE_NAME = 'xml_queue'
IDLE_TIMEOUT = 10  # seconds

# Root of the output XML
root = ET.Element("Messages")

def wrap_message_with_metadata(body, routing_key):
    try:
        original_xml = ET.fromstring(body)
    except ET.ParseError as e:
        print(f"Skipping invalid XML: {e}")
        return None

    message_elem = ET.Element("Message")

    # Metadata
    meta = ET.SubElement(message_elem, "Metadata")
    timestamp = ET.SubElement(meta, "Timestamp")
    timestamp.text = datetime.utcnow().isoformat() + "Z"

    rk_elem = ET.SubElement(meta, "RoutingKey")
    rk_elem.text = routing_key or ""

    # Payload
    payload = ET.SubElement(message_elem, "Payload")
    payload.append(original_xml)

    return message_elem

def consume_messages(max_count=None):
    connection = pika.BlockingConnection(pika.ConnectionParameters(host=RABBITMQ_HOST))
    channel = connection.channel()
    channel.queue_declare(queue=QUEUE_NAME)

    total_received = 0
    last_message_time = time.time()

    while True:
        method_frame, header_frame, body = channel.basic_get(queue=QUEUE_NAME, auto_ack=True)

        if method_frame:
            routing_key = method_frame.routing_key
            wrapped = wrap_message_with_metadata(body, routing_key)
            if wrapped is not None:
                root.append(wrapped)
                total_received += 1
                print(f"Received message #{total_received}")
            last_message_time = time.time()

            if max_count and total_received >= max_count:
                print(f"Reached test limit of {max_count} messages.")
                break
        else:
            if not max_count and time.time() - last_message_time > IDLE_TIMEOUT:
                print("Queue idle. Stopping.")
                break
            time.sleep(0.1)

    connection.close()
    write_to_compressed_file(max_count)

def write_to_compressed_file(count=None):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    mode = f"{count}_messages" if count else "all_messages"
    filename = f"framed_{mode}_{timestamp}.xml.gz"

    tree = ET.ElementTree(root)
    xml_bytes = ET.tostring(root, encoding="utf-8", xml_declaration=True)

    with gzip.open(filename, "wb") as f:
        f.write(xml_bytes)

    print(f"Compressed XML saved to: {filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Consume XML messages from RabbitMQ and frame into compressed XML file.")
    parser.add_argument('--count', type=int, help="Number of messages to read (test mode). Leave empty to consume all.")
    args = parser.parse_args()

    consume_messages(max_count=args.count)


⸻

Output
	•	Output files will now be like:
	•	framed_all_messages_20250513_XXXXXX.xml.gz
	•	framed_10_messages_20250513_XXXXXX.xml.gz

⸻

Optional Add-ons:

Would you like to:
	•	Upload the .gz file to S3 automatically?
	•	Also keep an uncompressed .xml copy for debugging?

Just let me know and I’ll add it!



=#=#=#


Correct — in an init load scenario, your Python script pulls data from the DB, formats each row into XML, and either:
	1.	Publishes each XML message to RabbitMQ (single-message mode), or
	2.	Collects multiple rows and writes them into one large XML file (batch mode).

Below are both versions of the script:

⸻

Version 1: Single Message Pull (One XML message at a time)

Steps Highlighted:
	•	Connect to MySQL
	•	Query one record
	•	Convert to XML
	•	(Optional) Publish to RabbitMQ or print/save

import mysql.connector
from dicttoxml import dicttoxml

# Step 1: Connect to MySQL
db = mysql.connector.connect(
    host="localhost",
    user="youruser",
    password="yourpassword",
    database="yourdb"
)

cursor = db.cursor(dictionary=True)

# Step 2: Query 1 row
cursor.execute("SELECT * FROM your_table WHERE id = %s", (1,))
row = cursor.fetchone()

# Step 3: Convert to XML
if row:
    xml_output = dicttoxml(row, custom_root='record', attr_type=False)
    print(xml_output.decode())  # Output 1 XML message
else:
    print("No record found")

db.close()


⸻

Version 2: Full Init Load - All Rows to Single XML File

Steps Highlighted:
	•	Connect to DB
	•	Pull all rows
	•	Convert each row to XML
	•	Wrap in root tag and write to file

import mysql.connector
from dicttoxml import dicttoxml

# Step 1: Connect to DB
db = mysql.connector.connect(
    host="localhost",
    user="youruser",
    password="yourpassword",
    database="yourdb"
)

cursor = db.cursor(dictionary=True)

# Step 2: Get all rows
cursor.execute("SELECT * FROM your_table")
rows = cursor.fetchall()

# Step 3: Convert each row to XML and collect
xml_records = [dicttoxml(row, custom_root='record', attr_type=False).decode() for row in rows]

# Step 4: Wrap in root tag
final_xml = "<records>\n" + "\n".join(xml_records) + "\n</records>"

# Step 5: Save to file
with open("init_load.xml", "w") as f:
    f.write(final_xml)

print("Saved init_load.xml with", len(rows), "records.")
db.close()


⸻

Would you like the XML to follow a specific schema (e.g., specific tag names or namespaces)?