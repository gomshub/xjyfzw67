To automate the archival process weekly for data older than 3 years from the current date to the oldest year (e.g., 2020) from linked tables on the main database to the archive database, follow these steps:

1. **Determine the Date Range for Archival**: Calculate the date range for data older than 3 years.
2. **Extract Data from Main DB**: Extract data from the main database.
3. **Load Data into Archive DB**: Load the extracted data into the archive database.
4. **Verify Data Transfer**: Verify that the data transfer is successful.
5. **Purge Data from Main DB**: Delete the data from the main database if the archival is successful.
6. **Automate the Process**: Schedule the process to run weekly using a cron job.

### Example Implementation

Assuming the use of MySQL for both the main and archive databases:

#### 1. Shell Script for Weekly Archival

```sh
#!/bin/bash

# Database connection details
MAIN_USER="main_username"
MAIN_PASS="main_password"
MAIN_HOST="main_db_host"
MAIN_DB="main_db"

ARCHIVE_USER="archive_username"
ARCHIVE_PASS="archive_password"
ARCHIVE_HOST="archive_db_host"
ARCHIVE_DB="archive_db"

# Email notification details
EMAIL="your-email@example.com"

# Determine the date range for archival
THREE_YEARS_AGO=$(date -d '3 years ago' +%Y-%m-%d)
START_DATE="2020-01-01"
END_DATE=$(date -d "$THREE_YEARS_AGO -1 day" +%Y-%m-%d)

# File paths for data extraction
ORDERS_FILE="/tmp/orders_$(date +%F).csv"
ORDER_ITEMS_FILE="/tmp/order_items_$(date +%F).csv"

# Extract data from main database to CSV files
mysql -u $MAIN_USER -p$MAIN_PASS -h $MAIN_HOST -D $MAIN_DB -e "
SELECT * INTO OUTFILE '$ORDERS_FILE'
FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'
LINES TERMINATED BY '\n'
FROM orders
WHERE order_date >= '$START_DATE' AND order_date <= '$END_DATE';
"

mysql -u $MAIN_USER -p$MAIN_PASS -h $MAIN_HOST -D $MAIN_DB -e "
SELECT oi.* INTO OUTFILE '$ORDER_ITEMS_FILE'
FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'
LINES TERMINATED BY '\n'
FROM order_items oi
JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_date >= '$START_DATE' AND o.order_date <= '$END_DATE';
"

# Transfer data to archive database
mysql -u $ARCHIVE_USER -p$ARCHIVE_PASS -h $ARCHIVE_HOST -D $ARCHIVE_DB -e "
ALTER TABLE archived_orders DISABLE KEYS;

LOAD DATA INFILE '$ORDERS_FILE'
INTO TABLE archived_orders
FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'
LINES TERMINATED BY '\n';

LOAD DATA INFILE '$ORDER_ITEMS_FILE'
INTO TABLE archived_order_items
FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '\"'
LINES TERMINATED BY '\n';

ALTER TABLE archived_orders ENABLE KEYS;
"

# Verify row counts
MAIN_COUNT_ORDERS=$(mysql -u $MAIN_USER -p$MAIN_PASS -h $MAIN_HOST -D $MAIN_DB -sse "
SELECT COUNT(*) FROM orders
WHERE order_date >= '$START_DATE' AND order_date <= '$END_DATE';
")

ARCHIVE_COUNT_ORDERS=$(mysql -u $ARCHIVE_USER -p$ARCHIVE_PASS -h $ARCHIVE_HOST -D $ARCHIVE_DB -sse "
SELECT COUNT(*) FROM archived_orders
WHERE order_date >= '$START_DATE' AND order_date <= '$END_DATE';
")

MAIN_COUNT_ORDER_ITEMS=$(mysql -u $MAIN_USER -p$MAIN_PASS -h $MAIN_HOST -D $MAIN_DB -sse "
SELECT COUNT(*) FROM order_items oi
JOIN orders o ON oi.order_id = o.order_id
WHERE o.order_date >= '$START_DATE' AND o.order_date <= '$END_DATE';
")

ARCHIVE_COUNT_ORDER_ITEMS=$(mysql -u $ARCHIVE_USER -p$ARCHIVE_PASS -h $ARCHIVE_HOST -D $ARCHIVE_DB -sse "
SELECT COUNT(*) FROM archived_order_items
WHERE order_date >= '$START_DATE' AND order_date <= '$END_DATE';
")

if [ "$MAIN_COUNT_ORDERS" -eq "$ARCHIVE_COUNT_ORDERS" ] && [ "$MAIN_COUNT_ORDER_ITEMS" -eq "$ARCHIVE_COUNT_ORDER_ITEMS" ]; then
    # Purge data from main database
    mysql -u $MAIN_USER -p$MAIN_PASS -h $MAIN_HOST -D $MAIN_DB -e "
    DELETE FROM order_items
    WHERE order_id IN (SELECT order_id FROM orders WHERE order_date >= '$START_DATE' AND order_date <= '$END_DATE');
    "

    mysql -u $MAIN_USER -p$MAIN_PASS -h $MAIN_HOST -D $MAIN_DB -e "
    DELETE FROM orders
    WHERE order_date >= '$START_DATE' AND order_date <= '$END_DATE';
    "
else
    echo "Archival verification failed for data older than 3 years." | mail -s "Archival Process Failed" $EMAIL
    exit 1
fi

# Send email notification
echo "Weekly archival process completed successfully." | mail -s "Archival Process Completed" $EMAIL
```

### 2. Schedule the Script to Run Weekly

Use a cron job to schedule the script to run weekly:

1. Open the crontab file for editing:

```sh
crontab -e
```

2. Add the following line to schedule the script to run at midnight every Sunday:

```sh
0 0 * * 0 /path/to/your/script.sh
```

### 3. Create Linked Tables in Archive DB with Partitioning, Compression, and Indexing

Create the tables in the archive database with partitioning, compression, and indexing:

```sql
CREATE TABLE archived_orders (
    order_id INT NOT NULL,
    customer_id INT,
    order_date DATE,
    total_amount DECIMAL(10, 2),
    PRIMARY KEY (order_id, order_date)
) ENGINE=InnoDB
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2017 VALUES LESS THAN (2018),
    PARTITION p2018 VALUES LESS THAN (2019),
    PARTITION p2019 VALUES LESS THAN (2020),
    PARTITION p2020 VALUES LESS THAN (2021)
)
ROW_FORMAT=COMPRESSED
KEY_BLOCK_SIZE=8;

CREATE INDEX idx_customer_id ON archived_orders (customer_id);

CREATE TABLE archived_order_items (
    order_id INT NOT NULL,
    product_id INT,
    quantity INT,
    price DECIMAL(10, 2),
    order_date DATE,
    PRIMARY KEY (order_id, product_id, order_date)
) ENGINE=InnoDB
PARTITION BY RANGE (YEAR(order_date)) (
    PARTITION p2017 VALUES LESS THAN (2018),
    PARTITION p2018 VALUES LESS THAN (2019),
    PARTITION p2019 VALUES LESS THAN (2020),
    PARTITION p2020 VALUES LESS THAN (2021)
)
ROW_FORMAT=COMPRESSED
KEY_BLOCK_SIZE=8;
```

### Summary

- **Automate the archival process** with a shell script scheduled to run weekly.
- **Extract data** from the main database and **load it** into the archive database.
- **Verify the data transfer** before purging data from the main database.
- Use **cron jobs** to schedule the process.
- Ensure the archive database tables are created with **partitioning, compression, and indexing** to manage data efficiently.