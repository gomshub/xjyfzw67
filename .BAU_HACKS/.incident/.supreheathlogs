
Below is an example Kubernetes Deployment YAML that uses a rolling update strategy. This strategy helps ensure that new pods are gradually rolled out while maintaining service availability. You can adjust parameters such as maxSurge and maxUnavailable to control how many pods can be added or taken down during the update.

Example Deployment YAML with Rolling Update Strategy

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-service
  labels:
    app: my-service
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # At most one extra pod can be created above the desired count.
      maxUnavailable: 1  # At most one pod can be unavailable during the update.
  selector:
    matchLabels:
      app: my-service
  template:
    metadata:
      labels:
        app: my-service
    spec:
      containers:
      - name: my-container
        image: myregistry.com/my-image:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 5000
        env:
          - name: LOG_LEVEL
            value: "info"
          - name: JDBC_JAR_PATH
            value: "/app/utils/lib/ojdbc6.jar"
        livenessProbe:
          httpGet:
            path: /healthz
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 20
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 10
          failureThreshold: 3
        resources:
          requests:
            cpu: "500m"
            memory: "256Mi"
          limits:
            cpu: "1"
            memory: "512Mi"
      restartPolicy: Always

Explanation of Key Fields
	â€¢	strategy.type: RollingUpdate
Specifies that updates should be performed in a rolling fashion rather than recreating all pods at once.
	â€¢	rollingUpdate.maxSurge: 1
Allows up to 1 extra pod (more than the desired number of replicas) to be created during the update. This extra capacity can help maintain service availability.
	â€¢	rollingUpdate.maxUnavailable: 1
Specifies that at most 1 pod can be unavailable during the update process. This ensures that most of your replicas remain available to serve traffic.
	â€¢	livenessProbe and readinessProbe
Ensure that Kubernetes can determine which pods are healthy and ready to accept traffic during the rolling update.
	â€¢	restartPolicy: Always
Ensures that if a pod fails, Kubernetes will automatically restart it.

How Rolling Updates Work
	1.	Start of Update:
When a new version of the image is deployed (for example, by updating the image tag in your values file), Kubernetes creates new pods with the new version.
	2.	Gradual Replacement:
With maxSurge: 1, Kubernetes creates one additional pod, and with maxUnavailable: 1, it ensures that no more than one pod is unavailable at any time. This means that while one new pod is starting, one old pod may be terminated.
	3.	Probes and Health Checks:
The readiness probe ensures that a new pod is only added to the Service endpoints once it is fully operational. The liveness probe will restart any pod that becomes unresponsive.
	4.	Completion:
Once all old pods have been replaced and the new pods pass the health checks, the rolling update is complete, and the updated application is running with minimal downtime.

This rolling update strategy helps maintain high availability and minimizes disruption during updates. Let me know if you need further customization or explanation!


To retain high availability while suppressing health check logs, you should:
	1.	Keep livenessProbe and readinessProbe for auto-recovery.
	2.	Suppress logs for health checks (/healthz) at the application level.
	3.	Ensure multiple replicas for fault tolerance.
	4.	Set restartPolicy: Always so failed containers auto-restart.

1. Flask: Suppress Health Check Logs

Modify routes.py to avoid logging /healthz, but keep health checks operational.

import logging
from flask import Flask, jsonify, request

app = Flask(__name__)

# Configure logging
logging.basicConfig(
    level=logging.INFO,  # Keep INFO level logs
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger()

@app.before_request
def suppress_healthz_logs():
    """Suppress logs for health check requests."""
    if request.path in ["/healthz", "/ready"]:
        log = logging.getLogger('werkzeug')
        log.setLevel(logging.ERROR)  # Suppress access logs for health endpoints

@app.route('/healthz', methods=['GET'])
def health_check():
    return jsonify({"status": "healthy"}), 200

@app.route('/ready', methods=['GET'])
def readiness_check():
    return jsonify({"status": "ready"}), 200

@app.route('/process', methods=['POST'])
def process_data():
    logger.info("Processing data request")
    return jsonify({"message": "Data processed successfully"}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)

2. Dockerfile: Set Gunicorn Logging Level

Ensure that only necessary logs appear in containers.

ENV GUNICORN_CMD_ARGS="--log-level info"

3. Kubernetes Deployment with HA
	â€¢	Replicas = 3 (for high availability).
	â€¢	Liveness Probe to restart the container if it hangs.
	â€¢	Readiness Probe to remove unhealthy pods from the service.
	â€¢	Suppress Health Check Logs to reduce noise.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-service
spec:
  replicas: 3  # Ensure high availability
  selector:
    matchLabels:
      app: my-service
  template:
    metadata:
      labels:
        app: my-service
    spec:
      containers:
      - name: my-container
        image: myregistry.com/my-image:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 5000
        env:
        - name: LOG_LEVEL
          value: "info"  # Ensures app logs at INFO level
        livenessProbe:
          httpGet:
            path: /healthz
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 20
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 5000
          initialDelaySeconds: 5
          periodSeconds: 10
          failureThreshold: 3
        resources:
          requests:
            cpu: "500m"
            memory: "256Mi"
          limits:
            cpu: "1"
            memory: "512Mi"
      restartPolicy: Always  # Ensures auto-restart on failure

4. Kubernetes Service (Optional)

Ensure traffic is routed properly to only healthy pods.

apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-service
  ports:
    - protocol: TCP
      port: 80
      targetPort: 5000
  type: ClusterIP

âœ… Summary

âœ” Suppresses health check logs but keeps probes active.
âœ” Ensures high availability with 3 replicas and auto-restart on failure.
âœ” Readiness probes ensure only healthy pods receive traffic.
âœ” Liveness probes restart stuck pods.

Would you like additional tuning, such as graceful shutdown or startup delays? ðŸš€


import logging
from flask import Flask, jsonify, request

app = Flask(__name__)

# Set up logging
logging.basicConfig(
    level=logging.INFO,  # Ensure INFO logs are shown
    format="%(asctime)s - %(levelname)s - %(message)s"
)

logger = logging.getLogger()

@app.before_request
def suppress_healthz_logs():
    """Ignore logs for health check requests."""
    if request.path == "/healthz":
        log = logging.getLogger('werkzeug')
        log.setLevel(logging.ERROR)  # Suppress access logs for /healthz

@app.route('/healthz', methods=['GET'])
def health_check():
    return jsonify({"status": "ok"}), 200  # No logs for health check

@app.route('/process', methods=['POST'])
def process_data():
    logger.info("Processing data request")  # INFO logs will always be visible
    return jsonify({"message": "Data processed successfully"}), 200

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)