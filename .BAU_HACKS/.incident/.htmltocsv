
Below is an example Bash script that does the following:
	1.	Reads a list of HTML URLs from a text file (one URL per line).
	2.	For each URL, it downloads the HTML content and removes newline characters so that each row is on a single line.
	3.	Extracts the <h1> title from the page.
	4.	Finds all <tr>...</tr> blocks and—for each row—checks if the first <td> cell matches the pattern
p01-ABC-XX (where “XX” are two digits).
	5.	For each matching <tr>, it extracts all <td> cell values, then outputs a CSV row with the page’s title in the first column followed by the extracted cells.
	6.	All rows from all URLs are appended into a single CSV file.

You can adjust the regular expressions if your HTML structure differs.

#!/bin/bash
# File: extract_rows.sh

# File containing one URL per line.
INPUT_FILE="urls.txt"
# Output CSV file.
OUTPUT_FILE="output.csv"

# Overwrite (or create) the output CSV file.
> "$OUTPUT_FILE"

# (Optional) Write a header row.
# echo "title,col1,col2,col3,..." >> "$OUTPUT_FILE"

# Loop over each URL from the input file.
while IFS= read -r url; do
    echo "Processing URL: $url"
    
    # Fetch HTML content and remove newlines for easier processing.
    content=$(curl -s "$url" | tr '\n' ' ')
    
    # Extract the <h1> title (assuming one <h1> tag exists).
    title=$(echo "$content" | grep -oP '(?<=<h1>).*?(?=</h1>)')
    
    # Extract all <tr>...</tr> blocks.
    # Using grep -oP with a non-greedy match.
    echo "$content" | grep -oP '<tr>.*?</tr>' | while read -r tr_block; do
        # Extract all <td> values from this row.
        # This produces one line per <td>.
        td_values=($(echo "$tr_block" | grep -oP '(?<=<td>).*?(?=</td>)' | sed 's/<[^>]*>//g' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//'))
        
        # Only continue if there is at least one <td>.
        if [ ${#td_values[@]} -gt 0 ]; then
            # Check if the first cell matches the pattern "p01-ABC-XX" (XX = two digits)
            if [[ ${td_values[0]} =~ ^p01-ABC-[0-9]{2}$ ]]; then
                # Join all td values with commas.
                # We use printf to join array elements with a comma.
                joined=$(printf ",%s" "${td_values[@]}")
                joined=${joined:1}  # Remove leading comma.
                
                # Output a CSV row with the title as the first column.
                echo "\"$title\",$joined" >> "$OUTPUT_FILE"
            fi
        fi
    done

done < "$INPUT_FILE"

echo "CSV output written to $OUTPUT_FILE"

How It Works
	1.	Reading URLs:
The script reads each URL from urls.txt.
	2.	Downloading & Flattening HTML:
It downloads the HTML content using curl -s and uses tr '\n' ' ' to collapse the content into a single line.
	3.	Extracting the Title:
It uses a PCRE (Perl-compatible) regular expression with grep -oP to get the content between <h1> and </h1>.
	4.	Processing Table Rows:
	•	It extracts each <tr>...</tr> block with grep -oP.
	•	Then, it extracts all <td> cell values within the block.
	•	It checks if the first <td> value matches the regex ^p01-ABC-[0-9]{2}$. Only rows with a first cell that fits this pattern are processed.
	•	The <td> values are joined into a comma-separated string.
	5.	Writing CSV:
Each matching row is written to the CSV with the page title as the first column, followed by the <td> values.

Usage Instructions
	1.	Save the Script:
Save the script above as extract_rows.sh.
	2.	Prepare the URL File:
Create a file named urls.txt in the same directory, for example:

http://example.com/page1.html
http://example.com/page2.html


	3.	Make the Script Executable:

chmod +x extract_rows.sh


	4.	Run the Script:

./extract_rows.sh


	5.	Review the CSV:
The output will be saved in output.csv, with one CSV row per matching table row (each row includes the page title as its first column).

Feel free to adjust the regular expressions or CSV formatting as needed for your HTML pages.


=#=

Below is an example shell script that:
	1.	Reads a list of HTML URLs from a file (e.g., urls.txt),
	2.	For each URL, downloads its content,
	3.	Extracts the <h1> title, and
	4.	Extracts all <td> cell values (from all <tr> rows) and “flattens” them into a single CSV row (with the title as the first column),
	5.	Appends the result as one row in an output CSV file.

You can adjust the selectors if your HTML structure is more complex.

#!/bin/bash
# File: extract_flattened_csv.sh

# Input file containing one URL per line.
INPUT_FILE="urls.txt"
# Output CSV file.
OUTPUT_FILE="output.csv"

# Clear (or create) the output file.
> "$OUTPUT_FILE"

# Optionally, write a header. Since the number of <td> columns can vary, a generic header is used.
# Uncomment the following line if you want a header row.
# echo "title,columns" > "$OUTPUT_FILE"

# Loop over each URL from the input file.
while IFS= read -r url; do
    echo "Processing: $url"

    # Download the HTML content and remove newlines (to ease regex processing).
    content=$(curl -s "$url" | tr '\n' ' ')

    # Extract the <h1> title. (Assumes there is one <h1> tag.)
    title=$(echo "$content" | grep -oP '(?<=<h1>).*?(?=</h1>)')

    # Extract all <td> values.
    # This looks for content inside <td> ... </td>,
    # trims surrounding whitespace, and then joins them with commas.
    tds=$(echo "$content" \
           | grep -oP '(?<=<td>).*?(?=</td>)' \
           | sed 's/^[[:space:]]*//;s/[[:space:]]*$//' \
           | paste -sd "," -)

    # Output the CSV row.
    # Wrap the title in double quotes to help with any commas or spaces.
    echo "\"$title\",$tds" >> "$OUTPUT_FILE"

done < "$INPUT_FILE"

echo "CSV output written to $OUTPUT_FILE"

How to Use
	1.	Save the Script:
Save the code above into a file (e.g., extract_flattened_csv.sh).
	2.	Prepare the URL List:
Create a file named urls.txt in the same directory. Each line should contain one HTML URL, for example:

http://example.com/page1.html
http://example.com/page2.html


	3.	Make the Script Executable:

chmod +x extract_flattened_csv.sh


	4.	Run the Script:

./extract_flattened_csv.sh


	5.	Result:
The script creates (or overwrites) output.csv with one row per URL. Each row starts with the page’s <h1> title followed by all <td> values found in the page, separated by commas.

Note
	•	This script assumes that the HTML is well-formed and that the <h1> and <td> tags do not span multiple lines (or that the tr removal of newlines makes them appear on one line).
	•	For more complex HTML, consider using an HTML parser (e.g., using Python with BeautifulSoup) for a more robust solution.
	•	CSV escaping is kept simple here by wrapping only the title in quotes. If your <td> values might contain commas or quotes, you might need additional escaping.

Feel free to adjust the script based on your exact HTML structure and CSV requirements!


#!/bin/bash

# URL of the page to fetch
URL="http://example.com/page.html"

# Fetch the page content
HTML_CONTENT=$(curl -s "$URL")

# Extract all <tr> rows
echo "$HTML_CONTENT" | awk '
    BEGIN { in_table=0; }
    /<table>/ { in_table=1; }
    /<\/table>/ { in_table=0; }
    in_table && /<tr>/ { row=""; next }
    in_table && /<td>/ {
        gsub(/<\/?td>/, "", $0); # Remove <td> tags
        gsub(/^\s+|\s+$/, "", $0); # Trim spaces
        row = (row == "" ? $0 : row "," $0);
    }
    in_table && /<\/tr>/ { print row; row=""; }
' > output.csv

echo "CSV data saved in output.csv"

Here’s a shell script that uses curl, grep, sed, and awk to extract the <h1> title and the <td> value corresponding to NDH_Task_code, then outputs the result as CSV.

Script:

#!/bin/bash

# URL of the page to fetch
URL="http://example.com/page.html"

# Fetch the page content
HTML_CONTENT=$(curl -s "$URL")

# Extract <h1> title
TITLE=$(echo "$HTML_CONTENT" | grep -oP '(?<=<h1>).*?(?=</h1>)')

# Extract the NDH_Task_code's right-side column value
TASK_CODE=$(echo "$HTML_CONTENT" | grep -A1 '<td>NDH_Task_code</td>' | tail -n1 | sed 's/<[^>]*>//g' | xargs)

# Output in CSV format
echo "title,task_code"
echo "\"$TITLE\",\"$TASK_CODE\""

Explanation:
	1.	Uses curl -s to fetch the HTML page without showing progress.
	2.	Extracts the <h1> content using grep -oP.
	3.	Finds the <td>NDH_Task_code</td>, grabs the next <td>, and removes HTML tags.
	4.	Formats the output as CSV.

Run the script:

Save it as extract_data.sh, then execute:

chmod +x extract_data.sh
./extract_data.sh

This will print the result in CSV format:

title,task_code
"My Page Title","12345"

Let me know if you need modifications!