
Perfect ‚Äî you want to test this Lambda end-to-end in a Docker container that simulates the Lambda runtime, connects directly to Snowflake, and avoids AWS resources (no SSM, no S3, no GitHub).

Let‚Äôs set up a local Docker-based testing flow üí™

‚∏ª

üß© Overview

You‚Äôll:
	1.	üê≥ Build a Docker image that mimics Lambda‚Äôs Python runtime
	2.	üß∞ Inject local Snowflake creds and YAML queries
	3.	üß† Run your Lambda handler locally inside Docker
	4.	üìä Save ‚ÄúS3 uploads‚Äù into a local folder

‚∏ª

üóÇ Folder structure

lambda-snowflake-local/
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ lambda_function.py       # your lambda code (modified for local mode)
‚îú‚îÄ‚îÄ local_creds.json         # Snowflake credentials
‚îú‚îÄ‚îÄ queries.yml              # test SQL queries
‚îú‚îÄ‚îÄ test_local.py            # invokes lambda_handler
‚îî‚îÄ‚îÄ local_s3/                # created automatically


‚∏ª

üß∞ 1. Update lambda_function.py (add local mode)

Add this block at the top before boto3 initialization:

import os, json, io, csv, yaml, logging
from datetime import datetime
import snowflake.connector

IS_LOCAL = os.getenv('LOCAL_MODE', 'false').lower() == 'true'

logger = logging.getLogger()
logger.setLevel(logging.INFO)

if IS_LOCAL:
    class MockS3Client:
        def put_object(self, Bucket, Key, Body):
            local_path = f"./local_s3/{Key}"
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            with open(local_path, 'wb') as f:
                f.write(Body)
            print(f"[LOCAL S3] File saved at {local_path}")

    class MockSSMClient:
        def get_parameter(self, Name, WithDecryption):
            with open('local_creds.json', 'r') as f:
                creds = json.load(f)
            return {'Parameter': {'Value': json.dumps(creds)}}

    s3 = MockS3Client()
    ssm = MockSSMClient()
    OUTPUT_S3_BUCKET = "local-test-bucket"
    LOCAL_CONFIG_PATH = 'queries.yml'
else:
    import boto3
    s3 = boto3.client('s3')
    ssm = boto3.client('ssm')
    OUTPUT_S3_BUCKET = os.getenv('OUTPUT_S3_BUCKET')
    SSM_SF_PARAM = os.getenv('SSM_SF_PARAM')
    LOCAL_CONFIG_PATH = '/app/config/queries.yml'

‚úÖ This enables dual mode:
	‚Ä¢	Local: uses JSON + folder
	‚Ä¢	AWS: uses SSM + S3

‚∏ª

üß© 2. test_local.py

import os
from lambda_function import lambda_handler

os.environ['LOCAL_MODE'] = 'true'

event = {}
context = {}
print("Running Lambda locally...")
result = lambda_handler(event, context)
print("\n=== Lambda Output ===")
print(result)


‚∏ª

üß© 3. local_creds.json

{
  "user": "YOUR_USER",
  "password": "YOUR_PASSWORD",
  "account": "YOUR_ACCOUNT",
  "warehouse": "YOUR_WAREHOUSE",
  "database": "YOUR_DATABASE",
  "schema": "YOUR_SCHEMA",
  "role": "YOUR_ROLE"
}


‚∏ª

üß© 4. queries.yml

queries:
  test_version: "SELECT CURRENT_VERSION()"


‚∏ª

üß± 5. Dockerfile

FROM public.ecr.aws/lambda/python:3.9

# Set working dir
WORKDIR /app

# Copy all files
COPY . .

# Install dependencies
RUN pip install --no-cache-dir snowflake-connector-python boto3 pyyaml

# Set environment for local mode
ENV LOCAL_MODE=true

# Run the test
CMD ["python", "test_local.py"]


‚∏ª

üöÄ 6. Build and Run

Build

docker build -t lambda-snowflake-local .

Run

docker run --rm -it \
    -e LOCAL_MODE=true \
    -v $(pwd)/local_s3:/app/local_s3 \
    lambda-snowflake-local

üí° The -v mount ensures any ‚ÄúS3 output‚Äù CSVs appear on your host under local_s3/.

‚∏ª

üßæ 7. Expected Output

Running Lambda locally...
[LOCAL S3] File saved at ./local_s3/test_version/2025-10-30/test_version_162503.csv

=== Lambda Output ===
{'status': 'ok', 'results': {'test_version': {'rows': 1, 's3_key': 'test_version/2025-10-30/test_version_162503.csv'}}}

Then you‚Äôll see a file like:

local_s3/test_version/2025-10-30/test_version_162503.csv

with the query result inside.

‚∏ª

‚úÖ Summary

Component	Local Substitute
AWS SSM	local_creds.json
AWS S3	local_s3/ mounted folder
Lambda Runtime	Docker container
Config YAML	queries.yml
Event Input	test_local.py


‚∏ª

Would you like me to modify the Dockerfile so you can mount your YAML, creds, and output dynamically (so you don‚Äôt rebuild the image every time)? It‚Äôs great for iterative testing.

###


Perfect üëç ‚Äî you already have a complete AWS Lambda that:
	1.	Reads Snowflake credentials from AWS SSM Parameter Store,
	2.	Executes SQL queries from a YAML config,
	3.	Writes results to S3 as CSV files.

Now, you want to test this Lambda locally (from your laptop), without GitHub Actions or AWS resources (like SSM or S3) ‚Äî only connecting to Snowflake.

Let‚Äôs go through this step-by-step setup üëá

‚∏ª

üß© 1. What You‚Äôll Simulate Locally

We‚Äôll mock:
	‚Ä¢	AWS SSM ‚Üí local JSON file for Snowflake creds
	‚Ä¢	AWS S3 ‚Üí local folder instead of S3 bucket
	‚Ä¢	Lambda handler invocation ‚Üí local script

You‚Äôll still connect to Snowflake for real, but without AWS dependencies.

‚∏ª

üß∞ 2. Create Local Test Files

‚úÖ a) local_creds.json

Store your Snowflake credentials here (replace placeholders):

{
  "user": "YOUR_SNOWFLAKE_USER",
  "password": "YOUR_SNOWFLAKE_PASSWORD",
  "account": "YOUR_SNOWFLAKE_ACCOUNT",
  "warehouse": "YOUR_SNOWFLAKE_WAREHOUSE",
  "database": "YOUR_SNOWFLAKE_DATABASE",
  "schema": "YOUR_SNOWFLAKE_SCHEMA",
  "role": "YOUR_SNOWFLAKE_ROLE"
}


‚∏ª

‚úÖ b) queries.yml

Example SQLs for testing:

queries:
  sample_query: "SELECT CURRENT_VERSION()"
  rowcount_query: "SELECT COUNT(*) AS total FROM YOUR_TABLE_NAME"


‚∏ª

‚öôÔ∏è 3. Modify Your Lambda for Local Testing Mode

We‚Äôll add a ‚Äúlocal mode‚Äù switch to skip boto3 and use local mocks.

Add these lines at the top of your script:

IS_LOCAL = os.getenv('LOCAL_MODE', 'false').lower() == 'true'

if IS_LOCAL:
    class MockS3Client:
        def put_object(self, Bucket, Key, Body):
            local_path = f"./local_s3/{Key}"
            os.makedirs(os.path.dirname(local_path), exist_ok=True)
            with open(local_path, 'wb') as f:
                f.write(Body)
            print(f"[LOCAL S3] Saved file to {local_path}")

    class MockSSMClient:
        def get_parameter(self, Name, WithDecryption):
            with open('local_creds.json', 'r') as f:
                creds = json.load(f)
            return {'Parameter': {'Value': json.dumps(creds)}}

    s3 = MockS3Client()
    ssm = MockSSMClient()
    OUTPUT_S3_BUCKET = "local-test-bucket"
    LOCAL_CONFIG_PATH = 'queries.yml'

That‚Äôs it ‚Äî if LOCAL_MODE=true, it‚Äôll:
	‚Ä¢	Read credentials from local_creds.json
	‚Ä¢	Read queries from queries.yml
	‚Ä¢	Save CSVs under ./local_s3/

‚∏ª

üß™ 4. Create a Local Test Runner

test_local.py

import os
from lambda_function import lambda_handler

# Enable local mode
os.environ['LOCAL_MODE'] = 'true'

# Call handler with dummy event/context
event = {}
context = {}

result = lambda_handler(event, context)
print("\n=== Lambda Result ===")
print(result)


‚∏ª

üöÄ 5. Run Locally

Run these commands:

pip install snowflake-connector-python pyyaml boto3
python test_local.py

Expected output:

[LOCAL S3] Saved file to ./local_s3/sample_query/2025-10-30/sample_query_163012.csv
=== Lambda Result ===
{'status': 'ok', 'results': {'sample_query': {'rows': 1, 's3_key': 'sample_query/...'}}}


‚∏ª

üßæ 6. Local Folder Output

After running:

local_s3/
  ‚îú‚îÄ‚îÄ sample_query/
  ‚îÇ   ‚îî‚îÄ‚îÄ 2025-10-30/
  ‚îÇ       ‚îî‚îÄ‚îÄ sample_query_163012.csv

Each CSV file will have query results ‚Äî exactly as it would appear in S3.

‚∏ª

‚úÖ Summary

AWS Service	Local Equivalent
SSM	local_creds.json
S3	local_s3/ folder
Lambda	python test_local.py
Config file	queries.yml

You can now test any SQL logic, error handling, or CSV formatting entirely offline, while still executing queries in Snowflake.

‚∏ª

Would you like me to show you how to re-enable AWS SSM/S3 only when LOCAL_MODE is false, so you can use the same script both locally and in AWS Lambda?