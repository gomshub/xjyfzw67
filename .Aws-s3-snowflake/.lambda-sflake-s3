Perfect ‚Äî this is a common AWS data ingestion automation:
‚úÖ Lambda runs daily ‚Üí ‚úÖ Executes a Snowflake query ‚Üí ‚úÖ Stores results in S3.

Below I‚Äôll give you:
	1.	Flow overview (README)
	2.	Lambda Python code (Snowflake ‚Üí S3)
	3.	Terraform code for:
	‚Ä¢	Lambda deployment
	‚Ä¢	IAM roles/policies
	‚Ä¢	Environment variables
	‚Ä¢	CloudWatch Event (cron scheduler)

‚∏ª

üß≠ 1. Flow Overview (README.md)

üìò Objective

This setup runs a daily AWS Lambda that:
	1.	Connects to Snowflake using credentials from AWS Secrets Manager or environment variables.
	2.	Executes a long SQL query (e.g., aggregation/report).
	3.	Fetches the results as a Pandas DataFrame.
	4.	Writes them to S3 as a CSV or Parquet file.

‚öôÔ∏è Architecture

CloudWatch Event (cron daily)
          ‚Üì
      AWS Lambda
          ‚Üì
  Snowflake ‚Üí Query ‚Üí DataFrame
          ‚Üì
          S3 bucket (/daily/YYYY-MM-DD/results.csv)

üß© Setup Steps
	1.	Create or update terraform.tfvars with your details:

snowflake_user        = "USER"
snowflake_password    = "PASSWORD"
snowflake_account     = "ACCOUNT_REGION"
snowflake_database    = "MY_DB"
snowflake_schema      = "PUBLIC"
snowflake_warehouse   = "COMPUTE_WH"
s3_bucket_name        = "my-snowflake-daily-results"
schedule_expression   = "cron(0 2 * * ? *)" # runs daily at 2 AM UTC


	2.	Run Terraform:

terraform init
terraform apply -auto-approve


	3.	Lambda will automatically trigger daily and push query results to:

s3://my-snowflake-daily-results/daily/YYYY-MM-DD/results.csv



‚∏ª

üêç 2. Lambda Python Code (lambda_function.py)

import os
import boto3
import snowflake.connector
import pandas as pd
from io import StringIO
from datetime import datetime

def lambda_handler(event, context):
    # Snowflake connection info from env vars
    SNOWFLAKE_USER = os.environ['SNOWFLAKE_USER']
    SNOWFLAKE_PASSWORD = os.environ['SNOWFLAKE_PASSWORD']
    SNOWFLAKE_ACCOUNT = os.environ['SNOWFLAKE_ACCOUNT']
    SNOWFLAKE_DATABASE = os.environ['SNOWFLAKE_DATABASE']
    SNOWFLAKE_SCHEMA = os.environ['SNOWFLAKE_SCHEMA']
    SNOWFLAKE_WAREHOUSE = os.environ['SNOWFLAKE_WAREHOUSE']
    QUERY = os.environ['SNOWFLAKE_QUERY']
    S3_BUCKET = os.environ['S3_BUCKET']

    # Connect to Snowflake
    conn = snowflake.connector.connect(
        user=SNOWFLAKE_USER,
        password=SNOWFLAKE_PASSWORD,
        account=SNOWFLAKE_ACCOUNT,
        warehouse=SNOWFLAKE_WAREHOUSE,
        database=SNOWFLAKE_DATABASE,
        schema=SNOWFLAKE_SCHEMA
    )

    cur = conn.cursor()
    cur.execute(QUERY)
    df = cur.fetch_pandas_all()
    cur.close()
    conn.close()

    # Write CSV to S3
    s3_client = boto3.client('s3')
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    today = datetime.utcnow().strftime("%Y-%m-%d")
    key = f"daily/{today}/results.csv"

    s3_client.put_object(
        Bucket=S3_BUCKET,
        Key=key,
        Body=csv_buffer.getvalue()
    )

    return {"status": "success", "s3_key": key}


‚∏ª

üß± 3. Terraform Code

main.tf

provider "aws" {
  region = "us-east-1"
}

variable "snowflake_user" {}
variable "snowflake_password" {}
variable "snowflake_account" {}
variable "snowflake_database" {}
variable "snowflake_schema" {}
variable "snowflake_warehouse" {}
variable "s3_bucket_name" {}
variable "schedule_expression" {}
variable "snowflake_query" {
  default = "SELECT * FROM MY_TABLE LIMIT 1000"
}

# Create S3 bucket for data
resource "aws_s3_bucket" "data" {
  bucket = var.s3_bucket_name
}

# Create IAM Role for Lambda
resource "aws_iam_role" "lambda_role" {
  name = "lambda_snowflake_s3_role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = "sts:AssumeRole"
        Effect = "Allow"
        Principal = {
          Service = "lambda.amazonaws.com"
        }
      }
    ]
  })
}

# Attach policies
resource "aws_iam_role_policy_attachment" "lambda_basic_execution" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

resource "aws_iam_role_policy_attachment" "lambda_s3_access" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
}

# Package Lambda
data "archive_file" "lambda_zip" {
  type        = "zip"
  source_dir  = "${path.module}/lambda"
  output_path = "${path.module}/lambda.zip"
}

# Lambda function
resource "aws_lambda_function" "snowflake_to_s3" {
  function_name = "snowflake_to_s3"
  role          = aws_iam_role.lambda_role.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = "python3.12"
  filename      = data.archive_file.lambda_zip.output_path
  timeout       = 900 # 15 minutes max for long queries

  environment {
    variables = {
      SNOWFLAKE_USER      = var.snowflake_user
      SNOWFLAKE_PASSWORD  = var.snowflake_password
      SNOWFLAKE_ACCOUNT   = var.snowflake_account
      SNOWFLAKE_DATABASE  = var.snowflake_database
      SNOWFLAKE_SCHEMA    = var.snowflake_schema
      SNOWFLAKE_WAREHOUSE = var.snowflake_warehouse
      S3_BUCKET           = var.s3_bucket_name
      SNOWFLAKE_QUERY     = var.snowflake_query
    }
  }
}

# CloudWatch schedule rule
resource "aws_cloudwatch_event_rule" "daily_trigger" {
  name                = "lambda_snowflake_to_s3_daily"
  schedule_expression = var.schedule_expression
}

# CloudWatch event target
resource "aws_cloudwatch_event_target" "trigger_lambda" {
  rule      = aws_cloudwatch_event_rule.daily_trigger.name
  target_id = "snowflake_to_s3_lambda"
  arn       = aws_lambda_function.snowflake_to_s3.arn
}

# Lambda permission for CloudWatch to invoke
resource "aws_lambda_permission" "allow_cloudwatch" {
  statement_id  = "AllowExecutionFromCloudWatch"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.snowflake_to_s3.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.daily_trigger.arn
}

Directory Structure

snowflake-to-s3/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ terraform.tfvars
‚îî‚îÄ‚îÄ lambda/
    ‚îî‚îÄ‚îÄ lambda_function.py


‚∏ª

‚úÖ 4. Deployment & Execution

cd snowflake-to-s3/
terraform init
terraform apply -auto-approve

	‚Ä¢	Check Lambda logs in CloudWatch Logs ‚Üí /aws/lambda/snowflake_to_s3
	‚Ä¢	Results appear daily in:

s3://<your-bucket>/daily/YYYY-MM-DD/results.csv



‚∏ª

Would you like me to include Snowflake credentials stored in AWS Secrets Manager (instead of plaintext in Terraform) for better security?
If yes, I can extend the Terraform code accordingly.