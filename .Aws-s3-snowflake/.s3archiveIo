Here’s the updated Flask service that:
✅ Uploads both the original and Parquet file to S3
✅ Retrieves the original file by default
✅ Retrieves the Parquet version if explicitly requested (format=parquet)
✅ Converts Parquet back to the original format if the original file is missing

Updated Flask Service (app.py)

import os
import boto3
import datetime
import pandas as pd
from flask import Flask, request, jsonify

app = Flask(__name__)

# AWS S3 Configurations
S3_BUCKET = "your-s3-bucket-name"
S3_REGION = "your-region"
AWS_ACCESS_KEY = "your-access-key"
AWS_SECRET_KEY = "your-secret-key"

s3_client = boto3.client(
    "s3",
    region_name=S3_REGION,
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY
)

def convert_to_parquet(file_path):
    """Convert file to Parquet and return new filename"""
    try:
        file_stem, file_ext = os.path.splitext(file_path)
        file_ext = file_ext.replace(".", "_")  # Convert `.csv` to `_csv`
        parquet_path = f"{file_stem}{file_ext}_parquet.parquet"

        if file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
        elif file_path.endswith((".xls", ".xlsx")):
            df = pd.read_excel(file_path)
        elif file_path.endswith(".xml"):
            df = pd.read_xml(file_path)
        elif file_path.endswith(".out"):
            df = pd.read_csv(file_path, delimiter="\t", header=None)  # Adjust delimiter if needed
        else:
            return None  # Unsupported type

        df.to_parquet(parquet_path, engine="pyarrow")
        return parquet_path
    except Exception as e:
        print(f"Error converting {file_path} to Parquet: {e}")
        return None

def convert_parquet_to_original(parquet_path, original_extension):
    """Convert Parquet back to the original format"""
    try:
        df = pd.read_parquet(parquet_path)
        original_path = parquet_path.replace("_parquet.parquet", original_extension)

        if original_extension == ".csv":
            df.to_csv(original_path, index=False)
        elif original_extension in (".xls", ".xlsx"):
            df.to_excel(original_path, index=False)
        elif original_extension == ".xml":
            df.to_xml(original_path)
        elif original_extension == ".out":
            df.to_csv(original_path, sep="\t", index=False, header=False)
        else:
            return None

        return original_path
    except Exception as e:
        print(f"Error converting Parquet back to original: {e}")
        return None

@app.route("/upload", methods=["POST"])
def upload_file():
    try:
        file_path = request.json.get("file_path")

        if not file_path or not os.path.exists(file_path):
            return jsonify({"error": "Invalid file path"}), 400

        base_folder = os.path.basename(os.path.dirname(file_path))
        file_date = datetime.datetime.fromtimestamp(os.path.getmtime(file_path))
        date_str = file_date.strftime("%d-%b-%Y")

        filename = os.path.basename(file_path)
        file_stem, file_ext = os.path.splitext(filename)
        file_ext = file_ext.replace(".", "_")  
        
        s3_original_key = f"{base_folder}/{date_str}/{filename}"
        s3_parquet_key = f"{base_folder}/{date_str}/{file_stem}{file_ext}_parquet.parquet"

        # Upload original file
        s3_client.upload_file(file_path, S3_BUCKET, s3_original_key)

        # Convert and upload Parquet file
        parquet_file = convert_to_parquet(file_path)
        if parquet_file:
            s3_client.upload_file(parquet_file, S3_BUCKET, s3_parquet_key)

        return jsonify({
            "message": "File uploaded successfully",
            "original_s3_key": s3_original_key,
            "parquet_s3_key": s3_parquet_key
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/get-file", methods=["GET"])
def get_file():
    try:
        folder_name = request.args.get("folder")  
        date_str = request.args.get("date")  
        filename = request.args.get("filename")  
        file_format = request.args.get("format", "original")  

        if not folder_name or not date_str or not filename:
            return jsonify({"error": "Missing parameters"}), 400

        file_stem, file_ext = os.path.splitext(filename)
        file_ext = file_ext.replace(".", "_")  
        
        s3_original_key = f"{folder_name}/{date_str}/{filename}"
        s3_parquet_key = f"{folder_name}/{date_str}/{file_stem}{file_ext}_parquet.parquet"

        if file_format == "parquet":
            s3_key = s3_parquet_key
        else:
            s3_key = s3_original_key

        try:
            s3_client.head_object(Bucket=S3_BUCKET, Key=s3_key)
            presigned_url = s3_client.generate_presigned_url(
                "get_object",
                Params={"Bucket": S3_BUCKET, "Key": s3_key},
                ExpiresIn=3600
            )
            return jsonify({"file_url": presigned_url}), 200
        except:
            if file_format == "original":
                try:
                    s3_client.download_file(S3_BUCKET, s3_parquet_key, "/tmp/temp.parquet")
                    converted_file = convert_parquet_to_original("/tmp/temp.parquet", os.path.splitext(filename)[1])
                    if converted_file:
                        s3_client.upload_file(converted_file, S3_BUCKET, s3_original_key)
                        presigned_url = s3_client.generate_presigned_url(
                            "get_object",
                            Params={"Bucket": S3_BUCKET, "Key": s3_original_key},
                            ExpiresIn=3600
                        )
                        return jsonify({"file_url": presigned_url, "message": "File converted from Parquet"}), 200
                except:
                    return jsonify({"error": "File not found and Parquet conversion failed"}), 404

        return jsonify({"error": "File not found"}), 404

    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(debug=True)

Shell Script for Upload (upload_files.sh)

#!/bin/bash

FLASK_URL="http://127.0.0.1:5000/upload"
FOLDER_LIST=("/test/io/abc" "/test/io/cde")

for folder in "${FOLDER_LIST[@]}"; do
    echo "Processing folder: $folder"

    find "$folder" -type f | while read -r file; do
        echo "Uploading file: $file"

        response=$(curl -s -X POST "$FLASK_URL" -H "Content-Type: application/json" \
            -d "{\"file_path\": \"$file\"}")

        if echo "$response" | grep -q "File uploaded successfully"; then
            echo "Successfully uploaded: $file"
            rm -f "$file"
        else
            echo "Failed to upload: $file. Response: $response"
        fi
    done
done

echo "Upload and cleanup completed."

Shell Script for Retrieval (get_file.sh)

#!/bin/bash

FLASK_URL="http://127.0.0.1:5000/get-file"

read -p "Enter folder name (e.g., abc): " folder
read -p "Enter file date (DD-MMM-YYYY format): " date_str
read -p "Enter original filename (e.g., file1.xml): " filename
read -p "Enter format (original/parquet, default: original): " format

response=$(curl -s -X GET "$FLASK_URL?folder=$folder&date=$date_str&filename=$filename&format=$format")

if echo "$response" | grep -q "file_url"; then
    file_url=$(echo "$response" | jq -r '.file_url')
    echo "Download URL: $file_url"
else
    echo "Failed to retrieve file. Response: $response"
fi

Summary

✅ Uploads both original & Parquet files to S3
✅ Retrieves original by default
✅ Retrieves Parquet if requested
✅ Converts Parquet back to original if missing

This ensures data integrity while allowing Parquet optimization. Let me know if you need changes!