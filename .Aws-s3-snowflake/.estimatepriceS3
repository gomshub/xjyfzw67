Hereâ€™s a complete Flask service that accepts:
	â€¢	bucket â€“ Cloudian S3 bucket name
	â€¢	prefix â€“ folder or subfolder path (with or without trailing slash)
	â€¢	years â€“ how many years old the files should be (e.g., 3 or 5)

It returns:
	â€¢	file count
	â€¢	total size (bytes and GB)
	â€¢	average file size

â¸»

âœ… Flask Service Code

from flask import Flask, request, jsonify
import boto3
from datetime import datetime, timedelta

app = Flask(__name__)

# Cloudian S3 configuration
CLOUDIAN_ENDPOINT = "https://cloudian.example.com"  # Replace with your endpoint
ACCESS_KEY = "your-access-key"
SECRET_KEY = "your-secret-key"

def get_cloudian_client():
    session = boto3.session.Session()
    client = session.client(
        's3',
        endpoint_url=CLOUDIAN_ENDPOINT,
        aws_access_key_id=ACCESS_KEY,
        aws_secret_access_key=SECRET_KEY
    )
    return client

def get_old_files_stats(client, bucket, prefix, older_than_years):
    cutoff = datetime.utcnow() - timedelta(days=older_than_years * 365)
    paginator = client.get_paginator('list_objects_v2')

    total_size = 0
    file_count = 0

    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):
        for obj in page.get('Contents', []):
            if obj['LastModified'] < cutoff:
                total_size += obj['Size']
                file_count += 1

    avg_size = total_size / file_count if file_count > 0 else 0

    return {
        "bucket": bucket,
        "prefix": prefix,
        "older_than_years": older_than_years,
        "file_count": file_count,
        "total_size_bytes": total_size,
        "total_size_gb": round(total_size / (1024**3), 2),
        "average_file_size_bytes": round(avg_size, 2),
        "average_file_size_mb": round(avg_size / (1024**2), 2)
    }

@app.route('/cloudian/old-file-stats', methods=['GET'])
def old_file_stats():
    bucket = request.args.get('bucket')
    prefix = request.args.get('prefix', '')
    years = request.args.get('years', type=int)

    if not bucket or years is None:
        return jsonify({"error": "Missing required parameters: 'bucket' and 'years'"}), 400

    try:
        client = get_cloudian_client()
        stats = get_old_files_stats(client, bucket, prefix, years)
        return jsonify(stats)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# To run the Flask app
if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001)


â¸»

âœ… Example Call (via curl or browser)

curl "http://localhost:5001/cloudian/old-file-stats?bucket=my-bucket&prefix=data/archive/&years=5"

Or open in browser:

http://localhost:5001/cloudian/old-file-stats?bucket=my-bucket&prefix=data/archive/&years=5


â¸»

ðŸ”’ Notes:
	â€¢	Your Cloudian bucket user must have s3:ListBucket permissions.
	â€¢	Use HTTPS + IAM key vault if deploying to prod.
	â€¢	You can easily export this to a dashboard or store the JSON result for further processing (e.g., pricing estimate, archiving, etc.).

â¸»

Would you like a second endpoint that takes this output and estimates AWS S3 cost based on this data too?

=#=#
import boto3

def get_cloudian_client(endpoint_url, access_key, secret_key):
    session = boto3.session.Session()
    client = session.client(
        's3',
        endpoint_url=endpoint_url,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
    )
    return client

def list_objects_recursive(client, bucket_name, prefix):
    paginator = client.get_paginator('list_objects_v2')
    total_size = 0
    total_objects = 0

    for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):
        for obj in page.get('Contents', []):
            total_size += obj['Size']
            total_objects += 1

    return total_size, total_objects

def estimate_aws_s3_sync_cost(total_size_bytes, total_objects, duration_months=1,
                              storage_class='STANDARD'):
    # Prices per GB per month (update as needed)
    PRICE_PER_GB_STANDARD = 0.023
    PRICE_PER_1000_PUT_REQUESTS = 0.005  # approx

    size_gb = total_size_bytes / (1024 ** 3)
    storage_cost = size_gb * PRICE_PER_GB_STANDARD * duration_months

    put_requests_cost = (total_objects / 1000) * PRICE_PER_1000_PUT_REQUESTS

    total_cost = storage_cost + put_requests_cost

    return {
        "storage_gb": round(size_gb, 2),
        "total_objects": total_objects,
        "storage_cost_usd": round(storage_cost, 2),
        "put_requests_cost_usd": round(put_requests_cost, 2),
        "total_cost_usd": round(total_cost, 2)
    }

if __name__ == "__main__":
    # Your Cloudian endpoint and credentials
    CLOUDIAN_ENDPOINT = "https://cloudian.example.com"
    ACCESS_KEY = "your-access-key"
    SECRET_KEY = "your-secret-key"
    BUCKET_NAME = "your-bucket-name"
    FOLDERS_TO_SYNC = ["folder1/", "folder2/subfolder/"]  # Add trailing slash for prefix
    
    client = get_cloudian_client(CLOUDIAN_ENDPOINT, ACCESS_KEY, SECRET_KEY)
    
    total_size_bytes = 0
    total_objects = 0

    for folder_prefix in FOLDERS_TO_SYNC:
        size, count = list_objects_recursive(client, BUCKET_NAME, folder_prefix)
        print(f"Folder '{folder_prefix}' total size: {size} bytes, objects: {count}")
        total_size_bytes += size
        total_objects += count

    cost_estimate = estimate_aws_s3_sync_cost(total_size_bytes, total_objects)

    print("\nEstimated AWS S3 Sync Costs:")
    print(f"Storage (GB): {cost_estimate['storage_gb']}")
    print(f"Number of objects (PUT requests): {cost_estimate['total_objects']}")
    print(f"Storage cost (USD): ${cost_estimate['storage_cost_usd']}")
    print(f"PUT requests cost (USD): ${cost_estimate['put_requests_cost_usd']}")
    print(f"Total estimated cost (USD): ${cost_estimate['total_cost_usd']}")