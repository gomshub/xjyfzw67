Excellent ğŸ‘ â€” letâ€™s make this production-grade:
âœ… Lambda runs in private VPC (via PrivateLink)
âœ… Query read from queries.yml config
âœ… Credentials pulled from AWS Secrets Manager (no plaintext password)
âœ… Includes a clear README explaining every variable and setup step.

â¸»

ğŸ§­ Architecture Overview

CloudWatch (Daily Cron)
        â†“
AWS Lambda (in VPC)
        â†“
PrivateLink + Private DNS
        â†“
Snowflake (Private Endpoint)
        â†“
Query from queries.yml
        â†“
S3 Bucket (daily/YYYY-MM-DD/<query>.csv)


â¸»

ğŸ—‚ï¸ Folder Structure

snowflake-to-s3/
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ terraform.tfvars
â”œâ”€â”€ queries.yml
â””â”€â”€ lambda/
    â”œâ”€â”€ lambda_function.py
    â””â”€â”€ requirements.txt


â¸»

ğŸ Lambda Code: lambda/lambda_function.py

This version:
	â€¢	Loads Snowflake credentials from Secrets Manager
	â€¢	Loads query dynamically from queries.yml
	â€¢	Connects to Snowflake via PrivateLink

import os
import boto3
import yaml
import snowflake.connector
import pandas as pd
from io import StringIO
from datetime import datetime
import json

def lambda_handler(event, context):
    secret_name = os.environ["SECRET_NAME"]
    region = os.environ["AWS_REGION"]
    query_name = os.environ.get("QUERY_NAME", "daily_sales_report")
    s3_bucket = os.environ["S3_BUCKET"]

    # --- Fetch credentials from Secrets Manager ---
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region)
    secret_value = client.get_secret_value(SecretId=secret_name)
    creds = json.loads(secret_value['SecretString'])

    # --- Load Queries from config ---
    with open(os.path.join(os.path.dirname(__file__), "queries.yml"), "r") as f:
        config = yaml.safe_load(f)
    query = config['queries'][query_name]

    # --- Connect to Snowflake ---
    conn = snowflake.connector.connect(
        user=creds["snowflake_user"],
        password=creds["snowflake_password"],
        account=creds["snowflake_account"],
        warehouse=creds["snowflake_warehouse"],
        database=creds["snowflake_database"],
        schema=creds["snowflake_schema"]
    )

    cur = conn.cursor()
    cur.execute(query)
    df = cur.fetch_pandas_all()
    cur.close()
    conn.close()

    # --- Upload to S3 ---
    s3 = boto3.client('s3')
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    today = datetime.utcnow().strftime("%Y-%m-%d")
    key = f"daily/{today}/{query_name}.csv"

    s3.put_object(Bucket=s3_bucket, Key=key, Body=csv_buffer.getvalue())

    return {
        "status": "success",
        "query_name": query_name,
        "s3_key": key
    }


â¸»

ğŸ§± Terraform Code

main.tf

provider "aws" {
  region = "us-east-1"
}

# -------------------------------
# Variables
# -------------------------------
variable "s3_bucket_name" {}
variable "schedule_expression" {}
variable "secret_name" {}
variable "query_name" {}
variable "private_snowflake_account" {}
variable "lambda_subnet_ids" {
  type = list(string)
}
variable "lambda_security_group_ids" {
  type = list(string)
}

# -------------------------------
# Create S3 bucket
# -------------------------------
resource "aws_s3_bucket" "data" {
  bucket = var.s3_bucket_name
}

# -------------------------------
# IAM Role for Lambda
# -------------------------------
resource "aws_iam_role" "lambda_role" {
  name = "lambda_snowflake_s3_role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

# Attach basic Lambda + S3 + Secrets Manager permissions
resource "aws_iam_role_policy_attachment" "lambda_basic_execution" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

resource "aws_iam_role_policy_attachment" "lambda_s3_access" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
}

resource "aws_iam_role_policy_attachment" "lambda_secrets_access" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/SecretsManagerReadWrite"
}

# -------------------------------
# Lambda Packaging
# -------------------------------
data "archive_file" "lambda_zip" {
  type        = "zip"
  source_dir  = "${path.module}/lambda"
  output_path = "${path.module}/lambda.zip"
}

# -------------------------------
# Lambda Function
# -------------------------------
resource "aws_lambda_function" "snowflake_to_s3" {
  function_name = "snowflake_to_s3_vpc"
  role          = aws_iam_role.lambda_role.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = "python3.12"
  filename      = data.archive_file.lambda_zip.output_path
  timeout       = 900

  environment {
    variables = {
      AWS_REGION        = "us-east-1"
      S3_BUCKET         = var.s3_bucket_name
      QUERY_NAME        = var.query_name
      SECRET_NAME       = var.secret_name
    }
  }

  vpc_config {
    subnet_ids         = var.lambda_subnet_ids
    security_group_ids = var.lambda_security_group_ids
  }
}

# -------------------------------
# CloudWatch Scheduler
# -------------------------------
resource "aws_cloudwatch_event_rule" "daily_trigger" {
  name                = "lambda_snowflake_to_s3_daily"
  schedule_expression = var.schedule_expression
}

resource "aws_cloudwatch_event_target" "trigger_lambda" {
  rule      = aws_cloudwatch_event_rule.daily_trigger.name
  target_id = "snowflake_to_s3_lambda"
  arn       = aws_lambda_function.snowflake_to_s3.arn
}

resource "aws_lambda_permission" "allow_cloudwatch" {
  statement_id  = "AllowExecutionFromCloudWatch"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.snowflake_to_s3.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.daily_trigger.arn
}


â¸»

ğŸ” Secrets Manager Setup (before Terraform apply)

Manually create a secret in AWS Secrets Manager (or use Terraform if you prefer):

Secret Name: snowflake/lambda/creds
Secret JSON Value:

{
  "snowflake_user": "LAMBDA_CI",
  "snowflake_password": "StrongPassword!",
  "snowflake_account": "abc12345.us-east-1.privatelink",
  "snowflake_database": "MY_DB",
  "snowflake_schema": "PUBLIC",
  "snowflake_warehouse": "COMPUTE_WH"
}

âœ… Stored securely
âœ… Encrypted at rest
âœ… Lambda can read only this secret

â¸»

ğŸ“˜ variables.tf

variable "s3_bucket_name" {
  description = "S3 bucket to store query results"
  type        = string
}

variable "schedule_expression" {
  description = "CloudWatch cron schedule (e.g., cron(0 2 * * ? *))"
  type        = string
}

variable "secret_name" {
  description = "AWS Secrets Manager name containing Snowflake credentials"
  type        = string
}

variable "query_name" {
  description = "Name of the query in queries.yml to execute"
  type        = string
  default     = "daily_sales_report"
}

variable "private_snowflake_account" {
  description = "PrivateLink-enabled Snowflake account (e.g., abc12345.us-east-1.privatelink)"
  type        = string
}

variable "lambda_subnet_ids" {
  description = "List of subnet IDs for Lambda VPC config"
  type        = list(string)
}

variable "lambda_security_group_ids" {
  description = "List of security group IDs for Lambda"
  type        = list(string)
}


â¸»

ğŸ§¾ terraform.tfvars Example

s3_bucket_name              = "my-snowflake-daily-results"
schedule_expression         = "cron(0 2 * * ? *)" # 2 AM UTC daily
secret_name                 = "snowflake/lambda/creds"
query_name                  = "daily_sales_report"
private_snowflake_account   = "abc12345.us-east-1.privatelink"
lambda_subnet_ids           = ["subnet-0abc12345", "subnet-0def67890"]
lambda_security_group_ids   = ["sg-0123456789abcdef"]


â¸»

ğŸ“˜ README.md

ğŸ§© Overview

This Terraform + Lambda setup automates daily data extraction from Snowflake using a private, secure VPC connection and uploads the query results into S3.

âš™ï¸ Components

Component	Purpose
Lambda	Executes SQL queries and writes to S3
Secrets Manager	Stores Snowflake credentials securely
PrivateLink (VPC)	Ensures no public internet traffic
CloudWatch Event	Triggers Lambda daily
S3 Bucket	Stores CSV results


â¸»

ğŸ› ï¸ Steps to Deploy
	1.	Create Snowflake Secret

{
  "snowflake_user": "LAMBDA_CI",
  "snowflake_password": "StrongPassword!",
  "snowflake_account": "abc12345.us-east-1.privatelink",
  "snowflake_database": "MY_DB",
  "snowflake_schema": "PUBLIC",
  "snowflake_warehouse": "COMPUTE_WH"
}

Save as: snowflake/lambda/creds

	2.	Edit queries.yml
Define your query inside:

queries:
  daily_sales_report: |
    SELECT * FROM ORDERS WHERE ORDER_DATE >= CURRENT_DATE() - 1;


	3.	Configure Variables
Edit terraform.tfvars with your subnet IDs, security group, etc.
	4.	Deploy

terraform init
terraform apply -auto-approve


	5.	Verify
	â€¢	Check CloudWatch logs for Lambda output.
	â€¢	Check S3 for CSV:

s3://my-snowflake-daily-results/daily/2025-10-10/daily_sales_report.csv



â¸»

âœ… Best Practices
	â€¢	Rotate Snowflake secret in AWS Secrets Manager regularly.
	â€¢	Use a minimal-permission Snowflake â€œCIâ€ user.
	â€¢	Keep Lambda in private subnets (no NAT required for PrivateLink).
	â€¢	Restrict S3 bucket with IAM policy to only this Lambdaâ€™s role.

â¸»

Would you like me to include Terraform code to automatically create and populate the Secrets Manager secret as well (so you donâ€™t have to do it manually)?