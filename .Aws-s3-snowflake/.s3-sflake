
Excellent question â€” and yes, we can absolutely simplify the scheduling part ğŸ˜

CloudWatch Event Rule (EventBridge) is the AWS-native way to do a â€œcron jobâ€ in the cloud â€” but if you find it too verbose in Terraform, we can reduce it to a very small, simple block instead of multiple resources.

â¸»

âœ… Simplified Cron-based Lambda Trigger in Terraform

Hereâ€™s the minimal and clean way to schedule your Lambda on a cron expression:

ğŸ§± Updated Terraform (Simplified)

Replace the CloudWatch section in your existing main.tf with this â¬‡ï¸

# -------------------------------
# Simple Cron-based Lambda Trigger
# -------------------------------
resource "aws_cloudwatch_event_rule" "daily_cron" {
  name                = "snowflake_daily_trigger"
  description         = "Triggers Lambda to fetch Snowflake data daily"
  schedule_expression = "cron(0 2 * * ? *)" # Runs daily at 2 AM UTC
}

resource "aws_cloudwatch_event_target" "lambda_target" {
  rule      = aws_cloudwatch_event_rule.daily_cron.name
  target_id = "lambda_snowflake_target"
  arn       = aws_lambda_function.snowflake_to_s3.arn
}

resource "aws_lambda_permission" "allow_cloudwatch_trigger" {
  statement_id  = "AllowExecutionFromCloudWatch"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.snowflake_to_s3.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.daily_cron.arn
}

Thatâ€™s it â€” only 3 short resources, easy to understand:
	â€¢	One rule (aws_cloudwatch_event_rule) defines the cron schedule.
	â€¢	One target (aws_cloudwatch_event_target) links it to your Lambda.
	â€¢	One permission (aws_lambda_permission) lets CloudWatch invoke it.

â¸»

ğŸ•“ Common Cron Examples

Schedule	Expression	Meaning
Every day at 2 AM UTC	cron(0 2 * * ? *)	âœ… Good for daily data jobs
Every 6 hours	rate(6 hours)	Simple repeating schedule
Every Monday at 5 AM	cron(0 5 ? * MON *)	Weekly trigger
Every 30 minutes	rate(30 minutes)	Continuous smaller jobs


â¸»

ğŸ§­ Simple Explanation

If you come from a Linux background â€”
Think of this as your crontab line, but defined in Terraform:

0 2 * * *   â†’   cron(0 2 * * ? *)

It means:
	â€¢	Minute = 0
	â€¢	Hour = 2
	â€¢	Day of Month = *
	â€¢	Month = *
	â€¢	Day of Week = ?
	â€¢	Year = *

AWS adds the ? and * syntax to make it CloudWatch-compatible.

â¸»

ğŸ§¾ Updated README (Trigger Section)

ğŸ•’ Scheduling

The Lambda is automatically triggered daily at 2 AM UTC via a CloudWatch cron expression:

schedule_expression = "cron(0 2 * * ? *)"

If you want to change it, just update this line and reapply Terraform:

terraform apply -auto-approve

You can use any standard cron schedule or use simpler rate() syntax like:

schedule_expression = "rate(1 day)"


â¸»

Would you like me to modify the full Terraform + README (including this simpler cron) so you can copy and apply in one go?


Excellent ğŸ‘ â€” letâ€™s make this production-grade:
âœ… Lambda runs in private VPC (via PrivateLink)
âœ… Query read from queries.yml config
âœ… Credentials pulled from AWS Secrets Manager (no plaintext password)
âœ… Includes a clear README explaining every variable and setup step.

â¸»

ğŸ§­ Architecture Overview

CloudWatch (Daily Cron)
        â†“
AWS Lambda (in VPC)
        â†“
PrivateLink + Private DNS
        â†“
Snowflake (Private Endpoint)
        â†“
Query from queries.yml
        â†“
S3 Bucket (daily/YYYY-MM-DD/<query>.csv)


â¸»

ğŸ—‚ï¸ Folder Structure

snowflake-to-s3/
â”œâ”€â”€ main.tf
â”œâ”€â”€ variables.tf
â”œâ”€â”€ terraform.tfvars
â”œâ”€â”€ queries.yml
â””â”€â”€ lambda/
    â”œâ”€â”€ lambda_function.py
    â””â”€â”€ requirements.txt


â¸»

ğŸ Lambda Code: lambda/lambda_function.py

This version:
	â€¢	Loads Snowflake credentials from Secrets Manager
	â€¢	Loads query dynamically from queries.yml
	â€¢	Connects to Snowflake via PrivateLink

import os
import boto3
import yaml
import snowflake.connector
import pandas as pd
from io import StringIO
from datetime import datetime
import json

def lambda_handler(event, context):
    secret_name = os.environ["SECRET_NAME"]
    region = os.environ["AWS_REGION"]
    query_name = os.environ.get("QUERY_NAME", "daily_sales_report")
    s3_bucket = os.environ["S3_BUCKET"]

    # --- Fetch credentials from Secrets Manager ---
    session = boto3.session.Session()
    client = session.client(service_name='secretsmanager', region_name=region)
    secret_value = client.get_secret_value(SecretId=secret_name)
    creds = json.loads(secret_value['SecretString'])

    # --- Load Queries from config ---
    with open(os.path.join(os.path.dirname(__file__), "queries.yml"), "r") as f:
        config = yaml.safe_load(f)
    query = config['queries'][query_name]

    # --- Connect to Snowflake ---
    conn = snowflake.connector.connect(
        user=creds["snowflake_user"],
        password=creds["snowflake_password"],
        account=creds["snowflake_account"],
        warehouse=creds["snowflake_warehouse"],
        database=creds["snowflake_database"],
        schema=creds["snowflake_schema"]
    )

    cur = conn.cursor()
    cur.execute(query)
    df = cur.fetch_pandas_all()
    cur.close()
    conn.close()

    # --- Upload to S3 ---
    s3 = boto3.client('s3')
    csv_buffer = StringIO()
    df.to_csv(csv_buffer, index=False)
    today = datetime.utcnow().strftime("%Y-%m-%d")
    key = f"daily/{today}/{query_name}.csv"

    s3.put_object(Bucket=s3_bucket, Key=key, Body=csv_buffer.getvalue())

    return {
        "status": "success",
        "query_name": query_name,
        "s3_key": key
    }


â¸»

ğŸ§± Terraform Code

main.tf

provider "aws" {
  region = "us-east-1"
}

# -------------------------------
# Variables
# -------------------------------
variable "s3_bucket_name" {}
variable "schedule_expression" {}
variable "secret_name" {}
variable "query_name" {}
variable "private_snowflake_account" {}
variable "lambda_subnet_ids" {
  type = list(string)
}
variable "lambda_security_group_ids" {
  type = list(string)
}

# -------------------------------
# Create S3 bucket
# -------------------------------
resource "aws_s3_bucket" "data" {
  bucket = var.s3_bucket_name
}

# -------------------------------
# IAM Role for Lambda
# -------------------------------
resource "aws_iam_role" "lambda_role" {
  name = "lambda_snowflake_s3_role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = {
        Service = "lambda.amazonaws.com"
      }
    }]
  })
}

# Attach basic Lambda + S3 + Secrets Manager permissions
resource "aws_iam_role_policy_attachment" "lambda_basic_execution" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole"
}

resource "aws_iam_role_policy_attachment" "lambda_s3_access" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
}

resource "aws_iam_role_policy_attachment" "lambda_secrets_access" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = "arn:aws:iam::aws:policy/SecretsManagerReadWrite"
}

# -------------------------------
# Lambda Packaging
# -------------------------------
data "archive_file" "lambda_zip" {
  type        = "zip"
  source_dir  = "${path.module}/lambda"
  output_path = "${path.module}/lambda.zip"
}

# -------------------------------
# Lambda Function
# -------------------------------
resource "aws_lambda_function" "snowflake_to_s3" {
  function_name = "snowflake_to_s3_vpc"
  role          = aws_iam_role.lambda_role.arn
  handler       = "lambda_function.lambda_handler"
  runtime       = "python3.12"
  filename      = data.archive_file.lambda_zip.output_path
  timeout       = 900

  environment {
    variables = {
      AWS_REGION        = "us-east-1"
      S3_BUCKET         = var.s3_bucket_name
      QUERY_NAME        = var.query_name
      SECRET_NAME       = var.secret_name
    }
  }

  vpc_config {
    subnet_ids         = var.lambda_subnet_ids
    security_group_ids = var.lambda_security_group_ids
  }
}

# -------------------------------
# CloudWatch Scheduler
# -------------------------------
resource "aws_cloudwatch_event_rule" "daily_trigger" {
  name                = "lambda_snowflake_to_s3_daily"
  schedule_expression = var.schedule_expression
}

resource "aws_cloudwatch_event_target" "trigger_lambda" {
  rule      = aws_cloudwatch_event_rule.daily_trigger.name
  target_id = "snowflake_to_s3_lambda"
  arn       = aws_lambda_function.snowflake_to_s3.arn
}

resource "aws_lambda_permission" "allow_cloudwatch" {
  statement_id  = "AllowExecutionFromCloudWatch"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.snowflake_to_s3.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.daily_trigger.arn
}


â¸»

ğŸ” Secrets Manager Setup (before Terraform apply)

Manually create a secret in AWS Secrets Manager (or use Terraform if you prefer):

Secret Name: snowflake/lambda/creds
Secret JSON Value:

{
  "snowflake_user": "LAMBDA_CI",
  "snowflake_password": "StrongPassword!",
  "snowflake_account": "abc12345.us-east-1.privatelink",
  "snowflake_database": "MY_DB",
  "snowflake_schema": "PUBLIC",
  "snowflake_warehouse": "COMPUTE_WH"
}

âœ… Stored securely
âœ… Encrypted at rest
âœ… Lambda can read only this secret

â¸»

ğŸ“˜ variables.tf

variable "s3_bucket_name" {
  description = "S3 bucket to store query results"
  type        = string
}

variable "schedule_expression" {
  description = "CloudWatch cron schedule (e.g., cron(0 2 * * ? *))"
  type        = string
}

variable "secret_name" {
  description = "AWS Secrets Manager name containing Snowflake credentials"
  type        = string
}

variable "query_name" {
  description = "Name of the query in queries.yml to execute"
  type        = string
  default     = "daily_sales_report"
}

variable "private_snowflake_account" {
  description = "PrivateLink-enabled Snowflake account (e.g., abc12345.us-east-1.privatelink)"
  type        = string
}

variable "lambda_subnet_ids" {
  description = "List of subnet IDs for Lambda VPC config"
  type        = list(string)
}

variable "lambda_security_group_ids" {
  description = "List of security group IDs for Lambda"
  type        = list(string)
}


â¸»

ğŸ§¾ terraform.tfvars Example

s3_bucket_name              = "my-snowflake-daily-results"
schedule_expression         = "cron(0 2 * * ? *)" # 2 AM UTC daily
secret_name                 = "snowflake/lambda/creds"
query_name                  = "daily_sales_report"
private_snowflake_account   = "abc12345.us-east-1.privatelink"
lambda_subnet_ids           = ["subnet-0abc12345", "subnet-0def67890"]
lambda_security_group_ids   = ["sg-0123456789abcdef"]


â¸»

ğŸ“˜ README.md

ğŸ§© Overview

This Terraform + Lambda setup automates daily data extraction from Snowflake using a private, secure VPC connection and uploads the query results into S3.

âš™ï¸ Components

Component	Purpose
Lambda	Executes SQL queries and writes to S3
Secrets Manager	Stores Snowflake credentials securely
PrivateLink (VPC)	Ensures no public internet traffic
CloudWatch Event	Triggers Lambda daily
S3 Bucket	Stores CSV results


â¸»

ğŸ› ï¸ Steps to Deploy
	1.	Create Snowflake Secret

{
  "snowflake_user": "LAMBDA_CI",
  "snowflake_password": "StrongPassword!",
  "snowflake_account": "abc12345.us-east-1.privatelink",
  "snowflake_database": "MY_DB",
  "snowflake_schema": "PUBLIC",
  "snowflake_warehouse": "COMPUTE_WH"
}

Save as: snowflake/lambda/creds

	2.	Edit queries.yml
Define your query inside:

queries:
  daily_sales_report: |
    SELECT * FROM ORDERS WHERE ORDER_DATE >= CURRENT_DATE() - 1;


	3.	Configure Variables
Edit terraform.tfvars with your subnet IDs, security group, etc.
	4.	Deploy

terraform init
terraform apply -auto-approve


	5.	Verify
	â€¢	Check CloudWatch logs for Lambda output.
	â€¢	Check S3 for CSV:

s3://my-snowflake-daily-results/daily/2025-10-10/daily_sales_report.csv



â¸»

âœ… Best Practices
	â€¢	Rotate Snowflake secret in AWS Secrets Manager regularly.
	â€¢	Use a minimal-permission Snowflake â€œCIâ€ user.
	â€¢	Keep Lambda in private subnets (no NAT required for PrivateLink).
	â€¢	Restrict S3 bucket with IAM policy to only this Lambdaâ€™s role.

â¸»

Would you like me to include Terraform code to automatically create and populate the Secrets Manager secret as well (so you donâ€™t have to do it manually)?