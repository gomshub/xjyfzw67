Got it! Now, the uploaded file will be stored in S3 as:
	1.	Original file: test/280225/sample.csv
	2.	Parquet file: test/280225/sample_parquet.parquet

When retrieving, you can specify the exact file type you need.

Updated Flask Service

This version:
✅ Uploads any file type as-is
✅ Converts .csv, .xls, .xml, .out to Parquet with _parquet.parquet suffix
✅ Fetches the correct file based on requested format
✅ Uses S3 presigned URL for secure access

Install Dependencies

pip install boto3 flask pandas pyarrow openpyxl lxml

Flask Service Code

import os
import boto3
import datetime
import pandas as pd
from flask import Flask, request, jsonify

app = Flask(__name__)

# AWS S3 Configurations
S3_BUCKET = "your-s3-bucket-name"
S3_REGION = "your-region"
AWS_ACCESS_KEY = "your-access-key"
AWS_SECRET_KEY = "your-secret-key"

s3_client = boto3.client(
    "s3",
    region_name=S3_REGION,
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY
)

def convert_to_parquet(file_path):
    """ Convert file to Parquet and return new filename """
    try:
        file_stem, _ = os.path.splitext(file_path)
        parquet_path = f"{file_stem}_parquet.parquet"

        # Load data based on file type
        if file_path.endswith(".csv"):
            df = pd.read_csv(file_path)
        elif file_path.endswith((".xls", ".xlsx")):
            df = pd.read_excel(file_path)
        elif file_path.endswith(".xml"):
            df = pd.read_xml(file_path)
        elif file_path.endswith(".out"):
            df = pd.read_csv(file_path, delimiter="\t", header=None)  # Adjust delimiter if needed
        else:
            return None  # Unsupported type

        df.to_parquet(parquet_path, engine="pyarrow")
        return parquet_path
    except Exception as e:
        print(f"Error converting {file_path} to Parquet: {e}")
        return None

@app.route("/upload", methods=["POST"])
def upload_file():
    try:
        file_path = request.json.get("file_path")
        convert = request.json.get("convert_to_parquet", False)  # True/False
        folder_name = request.json.get("folder")  # e.g., "test"

        if not file_path or not os.path.exists(file_path):
            return jsonify({"error": "Invalid file path"}), 400

        filename = os.path.basename(file_path)
        date_str = datetime.datetime.now().strftime("%d%m%y")
        s3_key = f"{folder_name}/{date_str}/{filename}"

        # Convert to Parquet if requested
        parquet_filename = None
        if convert and file_path.endswith((".csv", ".xls", ".xlsx", ".xml", ".out")):
            parquet_file = convert_to_parquet(file_path)
            if parquet_file:
                parquet_filename = os.path.basename(parquet_file)
                parquet_s3_key = f"{folder_name}/{date_str}/{parquet_filename}"
                s3_client.upload_file(parquet_file, S3_BUCKET, parquet_s3_key)

        # Upload original file to S3
        s3_client.upload_file(file_path, S3_BUCKET, s3_key)

        return jsonify({
            "message": "File uploaded successfully",
            "original_file_s3_key": s3_key,
            "parquet_file_s3_key": parquet_s3_key if parquet_filename else None
        }), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/get-file", methods=["GET"])
def get_file():
    try:
        folder_name = request.args.get("folder")
        date_str = request.args.get("date")  # Expected format: DDMMYY
        filename = request.args.get("filename")
        file_type = request.args.get("file_type", "original")  # "original" or "parquet"

        if not folder_name or not date_str or not filename:
            return jsonify({"error": "Missing parameters"}), 400

        s3_key = f"{folder_name}/{date_str}/{filename}"
        if file_type == "parquet":
            file_stem, _ = os.path.splitext(filename)
            s3_key = f"{folder_name}/{date_str}/{file_stem}_parquet.parquet"

        # Generate presigned URL for temporary access
        presigned_url = s3_client.generate_presigned_url(
            "get_object",
            Params={"Bucket": S3_BUCKET, "Key": s3_key},
            ExpiresIn=3600  # URL valid for 1 hour
        )

        return jsonify({"file_url": presigned_url}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(debug=True)

How to Use It

1. Upload Any File Without Conversion

curl -X POST "http://127.0.0.1:5000/upload" -H "Content-Type: application/json" \
-d '{"file_path": "/path/to/sample.csv", "folder": "test"}'

2. Upload & Convert to Parquet

curl -X POST "http://127.0.0.1:5000/upload" -H "Content-Type: application/json" \
-d '{"file_path": "/path/to/sample.csv", "folder": "test", "convert_to_parquet": true}'

✅ This will upload:
	•	test/280225/sample.csv
	•	test/280225/sample_parquet.parquet

3. Retrieve Original File

curl -X GET "http://127.0.0.1:5000/get-file?folder=test&date=280225&filename=sample.csv"

4. Retrieve Parquet File

curl -X GET "http://127.0.0.1:5000/get-file?folder=test&date=280225&filename=sample.csv&file_type=parquet"

✅ This fetches test/280225/sample_parquet.parquet

Summary

✅ Uploads any file type
✅ Converts .csv, .xls, .xml, .out → Parquet (_parquet.parquet)
✅ Stores files in test/DDMMYY/
✅ Retrieves original or Parquet version
✅ Uses secure presigned URLs

Let me know if you need any tweaks!