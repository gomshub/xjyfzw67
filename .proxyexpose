Good catch üëç ‚Äî let me explain why I showed 8.8.8.8 there and when you should not use it.

‚∏ª

Why I showed 8.8.8.8
	‚Ä¢	8.8.8.8 is Google‚Äôs public DNS.
	‚Ä¢	I used it in the example because it works anywhere, including your local Windows Docker test.
	‚Ä¢	That way, if you run the Nginx container on your laptop, it can resolve swagger.clusterA.com without needing Kubernetes‚Äô internal DNS.

‚∏ª

Why you should not use it in Kubernetes

Inside a Kubernetes cluster:
	‚Ä¢	Services & hostnames are resolved by CoreDNS (kube-dns), not by Google.
	‚Ä¢	So if you use 8.8.8.8, it might fail for internal names (*.svc.cluster.local) or be slower.
	‚Ä¢	The right resolver is the Cluster DNS service IP, typically 10.96.0.10 (but check with:

kubectl get svc -n kube-system kube-dns

or

kubectl get svc -n kube-system coredns



‚∏ª

‚úÖ Correct approach
	‚Ä¢	Local Docker test (Windows/Mac/Linux) ‚Üí use resolver 8.8.8.8; (or your system DNS).
	‚Ä¢	Cluster B Helm deployment ‚Üí use resolver <kube-dns-service-ip>;

So your Helm values.yaml should allow overriding, for example:

resolver: "10.96.0.10"   # default for in-cluster

and in nginx.conf template:

resolver {{ .Values.resolver }};


‚∏ª

üëâ This way, the same Helm chart works for both:
	‚Ä¢	Local laptop test (set resolver=8.8.8.8)
	‚Ä¢	Kubernetes deployment (set resolver=10.96.0.10)

‚∏ª

Do you want me to extend the Helm chart configmap.yaml with a resolver parameter so you can swap it between local and kube easily?



Perfect üëç let‚Äôs make it ready-to-deploy on Cluster B with Helm (you can later templatize values).

‚∏ª

üîπ 1. ConfigMap with Nginx Proxy Rules

This contains the nginx.conf we discussed:

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx-conf
  namespace: default
data:
  nginx.conf: |
    events {}

    http {
      server {
        listen 80;

        # Any subdomain of clusterB.com
        server_name ~^(?<subdomain>.+)\.clusterB\.com$;

        location / {
          proxy_pass https://$subdomain.clusterA.com;

          proxy_set_header Host $subdomain.clusterA.com;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          proxy_connect_timeout 60s;
          proxy_read_timeout 300s;
        }
      }
    }


‚∏ª

üîπ 2. Deployment (Bitnami Nginx with custom config)

This mounts the above ConfigMap into the container so Nginx uses it.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
        - name: nginx
          image: bitnami/nginx:1.25.3   # replace with your Artifactory image if needed
          ports:
            - containerPort: 80
          volumeMounts:
            - name: nginx-conf
              mountPath: /opt/bitnami/nginx/conf/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: nginx-conf
          configMap:
            name: swagger-proxy-nginx-conf


‚∏ª

üîπ 3. Service (ClusterIP / LoadBalancer)

Expose Nginx inside Cluster B. If you use Ingress, just make it ClusterIP.

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy-svc
  namespace: default
spec:
  type: ClusterIP
  selector:
    app: swagger-proxy
  ports:
    - port: 80
      targetPort: 80


‚∏ª

üîπ 4. Ingress (Expose externally as *.clusterB.com)

Assuming you have an Ingress Controller in Cluster B:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  rules:
    - host: swagger.clusterB.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: 80
    - host: api-actor.clusterB.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: 80
  tls:
    - hosts:
        - swagger.clusterB.com
        - api-actor.clusterB.com
      secretName: clusterb-tls


‚∏ª

üîπ Flow
	‚Ä¢	User ‚Üí https://swagger.clusterB.com
	‚Ä¢	Ingress ‚Üí Service ‚Üí Nginx Pod
	‚Ä¢	Nginx proxies ‚Üí https://swagger.clusterA.com
	‚Ä¢	User ‚Üí https://api-actor.clusterB.com
	‚Ä¢	Ingress ‚Üí Service ‚Üí Nginx Pod
	‚Ä¢	Nginx proxies ‚Üí https://api-actor.clusterA.com

Browser always stays on *.clusterB.com.

‚∏ª

üëâ Do you want me to also show you the Helm chart folder structure (values.yaml, templates/ etc.) so you can package and push this proxy setup to Artifactory and later deploy via ArgoCD?
####


Great üöÄ ‚Äî let‚Äôs build a Helm chart for your generic Swagger proxy that runs on Cluster B.

This chart will:
	‚Ä¢	Deploy Nginx with a custom config (from a ConfigMap).
	‚Ä¢	Proxy swagger.clusterB.com ‚Üí swagger.clusterA.com.
	‚Ä¢	Proxy any API service via /api/{service}/... ‚Üí {service}.clusterA.com/....
	‚Ä¢	Expose via an Ingress so users can reach it.

‚∏ª

üìÇ Helm Chart Structure

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml


‚∏ª

üìå Chart.yaml

apiVersion: v2
name: swagger-proxy
description: A generic Nginx proxy for Swagger and APIs
type: application
version: 0.1.0
appVersion: "1.0"


‚∏ª

üìå values.yaml

image:
  repository: bitnami/nginx
  tag: 1.25.5
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  hosts:
    - host: swagger.clusterB.com
      paths:
        - path: /
          pathType: Prefix
  tls: []  # Add cert-manager TLS if needed


‚∏ª

üìå templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx-conf
data:
  nginx.conf: |
    worker_processes 1;
    events { worker_connections 1024; }

    http {
      include       mime.types;
      default_type  application/octet-stream;

      sendfile        on;
      keepalive_timeout  65;

      server {
        listen 8080;
        server_name swagger.clusterB.com;

        # Proxy Swagger UI
        location / {
          proxy_pass https://swagger.clusterA.com;
          proxy_ssl_server_name on;

          proxy_set_header Host swagger.clusterA.com;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Generic API proxy
        # /api/api-actor/v1/... ‚Üí https://api-actor.clusterA.com/v1/...
        location /api/ {
          resolver 8.8.8.8;   # Or kube-dns if in-cluster
          set $api_host "";
          if ($uri ~ ^/api/([^/]+)(/.*)$) {
            set $api_host $1.clusterA.com;
            set $api_path $2;
          }

          proxy_pass https://$api_host$api_path;
          proxy_ssl_server_name on;

          proxy_set_header Host $api_host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }
      }
    }


‚∏ª

üìå templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
        - name: nginx
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          volumeMounts:
            - name: nginx-conf
              mountPath: /opt/bitnami/nginx/conf/nginx.conf
              subPath: nginx.conf
          ports:
            - containerPort: 8080
      volumes:
        - name: nginx-conf
          configMap:
            name: swagger-proxy-nginx-conf
            items:
              - key: nginx.conf
                path: nginx.conf


‚∏ª

üìå templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  type: {{ .Values.service.type }}
  selector:
    app: swagger-proxy
  ports:
    - protocol: TCP
      port: {{ .Values.service.port }}
      targetPort: 8080


‚∏ª

üìå templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy
  annotations:
    kubernetes.io/ingress.class: {{ .Values.ingress.className }}
spec:
  rules:
  {{- range .Values.ingress.hosts }}
    - host: {{ .host }}
      http:
        paths:
        {{- range .paths }}
          - path: {{ .path }}
            pathType: {{ .pathType }}
            backend:
              service:
                name: swagger-proxy
                port:
                  number: {{ $.Values.service.port }}
        {{- end }}
  {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
  {{- toYaml .Values.ingress.tls | nindent 4 }}
  {{- end }}
{{- end }}


‚∏ª

üèÉ How it works
	1.	Users open https://swagger.clusterB.com.
‚Üí Nginx proxies to swagger.clusterA.com.
	2.	Swagger calls /api/api-actor/v1/... (Cluster B endpoint).
‚Üí Nginx rewrites it to https://api-actor.clusterA.com/v1/....
	3.	Works for any service (api-actor, api-user, etc.) without writing separate configs.

‚∏ª

üëâ Do you want me to also give you a Bamboo build + Helm package script that:
	‚Ä¢	Packages this Helm chart
	‚Ä¢	Pushes it to Artifactory (as swagger-proxy-0.1.0.tgz)
	‚Ä¢	Updates ArgoCD via paramArgoCDUpdater?

mm

Got it ‚úÖ thanks for clarifying your exact need.
Let me restate in plain words so we don‚Äôt miss the logic:
	‚Ä¢	Cluster A has:
	‚Ä¢	https://swagger.clusterA.com (Swagger UI)
	‚Ä¢	https://api-actor.clusterA.com (backend APIs)
	‚Ä¢	‚Ä¶ other backend APIs.
	‚Ä¢	Some users cannot reach Cluster A, but they can reach Cluster B.
	‚Ä¢	You want to deploy a proxy in Cluster B so that users can use:
	‚Ä¢	https://swagger.clusterB.com ‚Üí proxy ‚Üí https://swagger.clusterA.com
	‚Ä¢	https://api-actor.clusterB.com ‚Üí proxy ‚Üí https://api-actor.clusterA.com

‚ö° So Cluster B acts as a mirror/proxy for Cluster A.

‚∏ª

üìÇ Helm Chart Structure (swagger-proxy/)

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îî‚îÄ‚îÄ templates/
    ‚îú‚îÄ‚îÄ configmap.yaml
    ‚îú‚îÄ‚îÄ deployment.yaml
    ‚îú‚îÄ‚îÄ service.yaml
    ‚îî‚îÄ‚îÄ ingress.yaml


‚∏ª

Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Proxy from Cluster B ‚Üí Cluster A (swagger + APIs)
version: 0.1.0
appVersion: "1.0"


‚∏ª

values.yaml

replicaCount: 1

image:
  repository: bitnami/nginx
  tag: latest
  pullPolicy: IfNotPresent

# Map of proxy hosts (ClusterB ‚Üí ClusterA)
proxy:
  mappings:
    - sourceHost: swagger.clusterB.com
      targetHost: swagger.clusterA.com
    - sourceHost: api-actor.clusterB.com
      targetHost: api-actor.clusterA.com

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  annotations: {}
  hosts:
    - swagger.clusterB.com
    - api-actor.clusterB.com
  tls: []   # add tls secrets here if you terminate SSL in clusterB ingress


‚∏ª

templates/configmap.yaml

This generates nginx.conf dynamically with multiple server {} blocks ‚Äî one per mapping.

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "swagger-proxy.fullname" . }}-nginx
data:
  nginx.conf: |
    worker_processes 1;

    events { worker_connections 1024; }

    http {
      include       mime.types;
      default_type  application/octet-stream;

      sendfile        on;
      keepalive_timeout  65;

      {{- range .Values.proxy.mappings }}
      server {
        listen 8080;
        server_name {{ .sourceHost }};

        location / {
          proxy_pass https://{{ .targetHost }};
          proxy_ssl_server_name on;

          proxy_set_header Host {{ .targetHost }};
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          # Optional if target cluster has self-signed certs
          # proxy_ssl_verify off;
        }
      }
      {{- end }}
    }


‚∏ª

templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "swagger-proxy.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "swagger-proxy.name" . }}
    spec:
      containers:
        - name: nginx
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: nginx-config
              mountPath: /opt/bitnami/nginx/conf/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: nginx-config
          configMap:
            name: {{ include "swagger-proxy.fullname" . }}-nginx


‚∏ª

templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: 8080
  selector:
    app: {{ include "swagger-proxy.name" . }}


‚∏ª

templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
  annotations:
    {{- with .Values.ingress.annotations }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  ingressClassName: {{ .Values.ingress.className }}
  rules:
    {{- range .Values.ingress.hosts }}
    - host: {{ . }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: {{ include "swagger-proxy.fullname" $ }}
                port:
                  number: {{ $.Values.service.port }}
    {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
    {{- toYaml .Values.ingress.tls | nindent 4 }}
  {{- end }}
{{- end }}


‚∏ª

üîó How it works
	‚Ä¢	You list ClusterB ‚Üí ClusterA host mappings in values.yaml.
	‚Ä¢	Chart generates nginx.conf with one proxy server block per mapping.
	‚Ä¢	Users hit swagger.clusterB.com ‚Üí Ingress in ClusterB ‚Üí Nginx proxy ‚Üí swagger.clusterA.com.
	‚Ä¢	Same for api-actor.clusterB.com ‚Üí proxy ‚Üí api-actor.clusterA.com.

‚∏ª

‚ö° This chart is generic ‚Äî you can add any number of services into values.yaml.mappings and redeploy.

üëâ Do you also want me to include a Bamboo build script that:
	1.	packages this chart,
	2.	pushes to Artifactory (e.g., swagger-ui-charts/0.1.0/),
	3.	and triggers ArgoCD sync automatically?


=====


Yes üëç that‚Äôs exactly the reason it didn‚Äôt work.
Your config snippet is incomplete compared to what Nginx needs to actually boot with your proxy.

Here‚Äôs why:

‚∏ª

üîπ Problems in your nginx.conf
	1.	You overwrote the default Nginx config with only:

events {}
http {
  server {
    listen 80;
    location / { proxy_pass ... }
  }
}

	‚Ä¢	Missing mime.types, default_type, worker_processes.
	‚Ä¢	Bitnami‚Äôs base image expects a full config.

	2.	You didn‚Äôt enable SSL options (proxy_ssl_server_name on;) ‚Üí needed when proxying to https://swagger-clusterA.com.
	3.	On Windows Docker, you exposed port 8080 but your config was listening on 80. That mismatch means:
	‚Ä¢	You hit localhost:8080
	‚Ä¢	But Nginx inside was only listening on 80 ‚Üí request never reached your proxy block.

‚∏ª

üîπ Fixed minimal working nginx.conf

Try this instead:

worker_processes 1;

events {
    worker_connections 1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    keepalive_timeout  65;

    server {
        listen 8080;   # match docker -p 8080:8080

        location / {
            proxy_pass https://swagger-clusterA.com;
            proxy_ssl_server_name on;

            proxy_set_header Host swagger-clusterA.com;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Optional if swagger-clusterA has self-signed cert
            # proxy_ssl_verify off;
        }
    }
}


‚∏ª

üîπ Run on Windows Docker

docker run --rm -d -p 8080:8080 `
  -v ${PWD}\nginx.conf:/opt/bitnami/nginx/conf/nginx.conf:ro `
  --name swagger-proxy bitnami/nginx:latest

Then open:

http://localhost:8080/

It should forward to:

https://swagger-clusterA.com/

Check logs if unsure:

docker logs swagger-proxy


‚∏ª

‚úÖ This is now a fully working testable proxy.
Once confirmed locally, you can directly reuse this config in Kubernetes via a ConfigMap.

‚∏ª

Do you want me to show you how to reuse the exact same nginx.conf for both:
	1.	Local Docker testing
	2.	Kubernetes Helm deployment (ConfigMap mount)
so you don‚Äôt maintain 2 different versions?


=====
Yes ‚úÖ you can absolutely test this swagger-proxy locally on Windows Docker before deploying to Kubernetes.

Let me walk you through step by step:

‚∏ª

üîπ 1. Write your nginx.conf

Create a file nginx.conf on your Windows machine:

events {}
http {
  server {
    listen 80;

    location / {
      proxy_pass https://swagger-clusterA.com;
      proxy_set_header Host swagger-clusterA.com;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
    }
  }
}

üëâ Replace swagger-clusterA.com with the real domain you want to forward to.

‚∏ª

üîπ 2. Run Nginx Docker container on Windows

If you‚Äôre using official nginx image:

docker run --rm -d -p 8080:80 `
  -v ${PWD}\nginx.conf:/etc/nginx/nginx.conf:ro `
  --name swagger-proxy nginx:1.25

	‚Ä¢	-p 8080:80 ‚Üí exposes Nginx on localhost:8080
	‚Ä¢	-v ${PWD}\nginx.conf:/etc/nginx/nginx.conf:ro ‚Üí mounts your config file into the container
	‚Ä¢	nginx:1.25 ‚Üí official nginx image

If you want to use bitnami/nginx:

docker run --rm -d -p 8080:8080 `
  -v ${PWD}\nginx.conf:/opt/bitnami/nginx/conf/nginx.conf:ro `
  --name swagger-proxy bitnami/nginx:latest


‚∏ª

üîπ 3. Test it

Open browser or use curl:

curl http://localhost:8080/swagger-ui/

üëâ This should forward to https://swagger-clusterA.com/swagger-ui/ transparently.

‚∏ª

üîπ 4. Debugging
	‚Ä¢	If you see errors ‚Üí check logs with:

docker logs swagger-proxy


	‚Ä¢	If config reload fails ‚Üí Nginx will show error on container startup.

‚∏ª

‚úÖ This way, you can fully test the proxy logic on your local Windows Docker environment before deploying the same config to Kubernetes.

‚∏ª

Would you like me to also write a Dockerfile so you can package this proxy (with nginx.conf baked in) and push it to your internal Artifactory, instead of mounting the config each time? That way your K8s deployment just pulls a ready-made image.


====
Yes üëç you can always see the final YAMLs that Helm generates before/after packaging and deploying.

Here‚Äôs how:

‚∏ª

üîπ 1. Check templates without packaging

If you have a Helm chart folder (say swagger-proxy/), run:

helm template swagger-proxy ./swagger-proxy -f values.yaml

üëâ This renders all templates into plain Kubernetes YAMLs, substituting values from values.yaml.
It does not install, just prints to stdout.

If you want to save them:

helm template swagger-proxy ./swagger-proxy -f values.yaml > rendered.yaml


‚∏ª

üîπ 2. Check packaged chart

If you already ran:

helm package ./swagger-proxy

You‚Äôll get something like:

swagger-proxy-0.1.0.tgz

You can still inspect it without deploying:

helm template swagger-proxy swagger-proxy-0.1.0.tgz -f values.yaml


‚∏ª

üîπ 3. After deployment ‚Äî inspect applied YAMLs

If it‚Äôs already installed in Kubernetes:

helm get manifest swagger-proxy

üëâ This shows the exact YAMLs Helm applied to the cluster.
Useful for debugging when what you expected ‚â† what‚Äôs running.

‚∏ª

üîπ 4. Alternative: unpack .tgz chart

Since a Helm chart is just a tarball:

tar -xvzf swagger-proxy-0.1.0.tgz

This gives you back the /templates, values.yaml, and Chart.yaml.

‚∏ª

‚úÖ So yes ‚Äî you can fully unpack and inspect all YAMLs either:
	‚Ä¢	before deploying (helm template)
	‚Ä¢	after packaging (helm template .tgz)
	‚Ä¢	after deployment (helm get manifest)

‚∏ª

Do you want me to show you a Bamboo build step that does helm template into an artifact (rendered.yaml) so you can always see exactly what will be applied, before ArgoCD picks it up?


===
Nice üöÄ you want the end-to-end flow:
	1.	Helm chart for Nginx proxy (with multiple Swagger service mappings).
	2.	Bamboo build script to:
	‚Ä¢	Package the Helm chart.
	‚Ä¢	Push Docker image + Helm chart to Artifactory.
	‚Ä¢	Trigger ArgoCD to sync.
	3.	Use paramArgoCDUpdater (parametrized deployment).

Let‚Äôs break it step by step üëá

‚∏ª

1Ô∏è‚É£ Helm Chart Structure

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ   ‚îî‚îÄ‚îÄ configmap.yaml


‚∏ª

Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Reverse proxy for Swagger APIs across clusters
type: application
version: 0.1.0
appVersion: "1.25.3"


‚∏ª

values.yaml

image:
  repository: artifactory.company.com/docker/nginx
  tag: 1.25.3
  pullPolicy: IfNotPresent

ingress:
  enabled: true
  host: swagger-clusterB.com

proxies:
  - path: /service1/
    target: https://api-service1.clusterA.com/
    hostHeader: api-service1.clusterA.com

  - path: /service2/
    target: https://api-service2.clusterA.com/
    hostHeader: api-service2.clusterA.com


‚∏ª

templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
        - name: nginx
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          volumeMounts:
            - name: config
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: config
          configMap:
            name: swagger-proxy-config


‚∏ª

templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
  selector:
    app: swagger-proxy


‚∏ª

templates/ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
spec:
  rules:
    - host: {{ .Values.ingress.host }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy
                port:
                  number: 8080


‚∏ª

templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 8080;
        {{- range .Values.proxies }}
        location {{ .path }} {
          proxy_pass {{ .target }};
          proxy_set_header Host {{ .hostHeader }};
        }
        {{- end }}
      }
    }


‚∏ª

2Ô∏è‚É£ Bamboo Build Script (bamboo-specs.yaml or inline script)

Example task script in your Bamboo plan:

#!/bin/bash
set -e

CHART_NAME=swagger-proxy
CHART_VERSION=0.1.$(date +%Y%m%d%H%M)
HELM_REPO_URL=https://artifactory.company.com/helm
DOCKER_IMAGE=artifactory.company.com/docker/nginx:1.25.3

echo ">>> Packaging Helm Chart"
helm package ./swagger-proxy --version $CHART_VERSION

echo ">>> Pushing Helm Chart to Artifactory"
curl -u $ARTIFACTORY_USER:$ARTIFACTORY_PASSWORD \
     -T ${CHART_NAME}-${CHART_VERSION}.tgz \
     "$HELM_REPO_URL/${CHART_NAME}-${CHART_VERSION}.tgz"

echo ">>> Updating ArgoCD Deployment"
argocd app set swagger-proxy \
  --values values.yaml \
  --parameter image.tag=$CHART_VERSION \
  --grpc-web


‚∏ª

3Ô∏è‚É£ ArgoCD Deployment with paramArgoCDUpdater

In ArgoCD, you‚Äôll have an app like:

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: swagger-proxy
  namespace: argocd
spec:
  source:
    repoURL: https://artifactory.company.com/helm
    chart: swagger-proxy
    targetRevision: 0.1.*
    helm:
      valueFiles:
        - values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: swagger
  syncPolicy:
    automated:
      prune: true
      selfHeal: true


‚∏ª

‚úÖ Flow:
	1.	Bamboo builds chart + uploads to Artifactory.
	2.	Bamboo triggers ArgoCD CLI (argocd app set) or paramArgoCDUpdater plugin to update the chart version / values.
	3.	ArgoCD pulls Helm chart from Artifactory ‚Üí deploys to Cluster B.

‚∏ª

Would you like me to give you a ready-to-use Bamboo bamboo-specs.yaml (YAML DSL) so you don‚Äôt need to inline scripts, or do you prefer the bash script task approach?


=#=#=#
Perfect üëç let‚Äôs make this generic with Helm so you don‚Äôt have to hardcode every service proxy.

We‚Äôll build a small Helm chart called swagger-proxy.

‚∏ª

üìÇ Directory Structure

swagger-proxy/
  Chart.yaml
  values.yaml
  templates/
    configmap.yaml
    deployment.yaml
    service.yaml
    ingress.yaml


‚∏ª

üìù Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Proxy Swagger UI + API services from Cluster A into Cluster B
version: 0.1.0
appVersion: "1.0"


‚∏ª

üìù values.yaml

Here you configure all the endpoints to proxy:

replicaCount: 1

image:
  repository: nginx
  tag: alpine

ingress:
  enabled: true
  host: swagger-clusterB.com

# Proxy rules
# Each entry maps a path on Cluster B ‚Üí a target in Cluster A
proxies:
  - path: /
    target: https://swagger-clusterA.com/
    hostHeader: swagger-clusterA.com

  - path: /service1/
    target: https://api-service1.clusterA.com/
    hostHeader: api-service1.clusterA.com

  - path: /service2/
    target: https://api-service2.clusterA.com/
    hostHeader: api-service2.clusterA.com


‚∏ª

üìù templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 8080;

        {{- range .Values.proxies }}
        location {{ .path }} {
          proxy_pass {{ .target }};
          proxy_set_header Host {{ .hostHeader }};
        }
        {{- end }}
      }
    }


‚∏ª

üìù templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
      - name: nginx
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: swagger-proxy-config


‚∏ª

üìù templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  selector:
    app: swagger-proxy
  ports:
    - port: 8080
      targetPort: 8080
  type: ClusterIP


‚∏ª

üìù templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
spec:
  rules:
    - host: {{ .Values.ingress.host }}
      http:
        paths:
        {{- range .Values.proxies }}
          - path: {{ .path }}
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy
                port:
                  number: 8080
        {{- end }}
{{- end }}


‚∏ª

üöÄ Deploy

helm install swagger-proxy ./swagger-proxy


‚∏ª

üîπ How it Works
	‚Ä¢	Users go to https://swagger-clusterB.com
	‚Ä¢	Requests to / ‚Üí proxied to https://swagger-clusterA.com
	‚Ä¢	Requests to /service1/ ‚Üí proxied to https://api-service1.clusterA.com
	‚Ä¢	Requests to /service2/ ‚Üí proxied to https://api-service2.clusterA.com
	‚Ä¢	You can add/remove services just by editing values.yaml.

‚∏ª

Do you also want me to show how to make the proxy auto-rewrite the Swagger UI URLs (so that instead of api-service1.clusterA.com showing up in the docs, it automatically rewrites to swagger-clusterB.com/service1/)? That way users never even see the Cluster A domains.


======
Perfect üëç you want a Helmized proxy deployment in Cluster B that can forward to a list of Swagger services in Cluster A.

Here‚Äôs a clean Helm chart structure + YAML configs that you can adapt.

‚∏ª

üîπ Directory Structure

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml


‚∏ª

üîπ Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Reverse proxy for Swagger services in Cluster A
version: 0.1.0
appVersion: "1.0"


‚∏ª

üîπ values.yaml

Here you define the list of services in Cluster A you want to proxy.

replicaCount: 1

image:
  repository: nginx
  tag: alpine
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8080

ingress:
  enabled: true
  className: ""
  hosts:
    - host: swagger-proxy.clusterB.com
      paths:
        - path: /
          pathType: Prefix
          backendPort: 8080
  tls: []   # e.g., configure if you have certs

# List of Swagger services in Cluster A
services:
  - name: service1
    url: https://service1.clusterA.com/swagger/
  - name: service2
    url: https://service2.clusterA.com/swagger/
  - name: service3
    url: https://service3.clusterA.com/swagger/


‚∏ª

üîπ templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "swagger-proxy.fullname" . }}-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen {{ .Values.service.port }};
        {{- range .Values.services }}
        location /{{ .name }}/ {
          proxy_pass {{ .url }};
          proxy_set_header Host {{ regexReplaceAll "https?://" .url "" }};
        }
        {{- end }}
      }
    }


‚∏ª

üîπ templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "swagger-proxy.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "swagger-proxy.name" . }}
    spec:
      containers:
      - name: nginx
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - containerPort: {{ .Values.service.port }}
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: {{ include "swagger-proxy.fullname" . }}-config


‚∏ª

üîπ templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  type: {{ .Values.service.type }}
  selector:
    app: {{ include "swagger-proxy.name" . }}
  ports:
  - port: {{ .Values.service.port }}
    targetPort: {{ .Values.service.port }}


‚∏ª

üîπ templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  rules:
  {{- range .Values.ingress.hosts }}
  - host: {{ .host }}
    http:
      paths:
      {{- range .paths }}
      - path: {{ .path }}
        pathType: {{ .pathType }}
        backend:
          service:
            name: {{ include "swagger-proxy.fullname" $ }}
            port:
              number: {{ .backendPort }}
      {{- end }}
  {{- end }}
{{- end }}


‚∏ª

üîπ How It Works
	1.	Add your Cluster A Swagger services in values.yaml under .services.

services:
  - name: service1
    url: https://service1.clusterA.com/swagger/
  - name: service2
    url: https://service2.clusterA.com/swagger/


	2.	Deploy Helm chart in Cluster B:

helm install swagger-proxy ./swagger-proxy


	3.	Access endpoints from Cluster B users:

https://swagger-proxy.clusterB.com/service1/
https://swagger-proxy.clusterB.com/service2/



‚∏ª

‚úÖ This setup needs no changes in Cluster A (since it‚Äôs already exposed via Ingress).
‚úÖ Users who only have access to Cluster B can now consume Cluster A Swagger via the proxy.

‚∏ª

Do you want me to also include basic auth / IP restriction in this proxy Helm chart so only selected users can use it?

Perfect, now it‚Äôs very clear üëç
	‚Ä¢	Cluster A Swagger services are already exposed via Ingress (https://service1.clusterA.com/...).
	‚Ä¢	Some users cannot access them directly (due to zone / firewall restrictions).
	‚Ä¢	But those same users can access Cluster B.
	‚Ä¢	‚úÖ So we just need to give them a proxy endpoint inside Cluster B, which forwards their requests to Cluster A.

‚∏ª

üîπ Solution: Proxy in Cluster B for Cluster A‚Äôs Ingress

Step 1. Nginx Config

We‚Äôll create an Nginx reverse proxy in Cluster B.
Example for 2 services (service1 and service2):

events {}
http {
  server {
    listen 8080;

    location /service1/ {
      proxy_pass https://service1.clusterA.com/;
    }

    location /service2/ {
      proxy_pass https://service2.clusterA.com/;
    }
  }
}


‚∏ª

Step 2. Kubernetes YAML (in Cluster B)

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 8080;

        location /service1/ {
          proxy_pass https://service1.clusterA.com/;
          proxy_set_header Host service1.clusterA.com;
        }

        location /service2/ {
          proxy_pass https://service2.clusterA.com/;
          proxy_set_header Host service2.clusterA.com;
        }
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: swagger-proxy-config
---
apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  selector:
    app: swagger-proxy
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP


‚∏ª

Step 3. Optional: Expose in Cluster B

If users need to access it from outside Cluster B, expose it with an Ingress or a LoadBalancer Service:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
spec:
  rules:
  - host: swagger-proxy.clusterB.com
    http:
      paths:
      - path: /service1/
        pathType: Prefix
        backend:
          service:
            name: swagger-proxy
            port:
              number: 8080
      - path: /service2/
        pathType: Prefix
        backend:
          service:
            name: swagger-proxy
            port:
              number: 8080


‚∏ª

üîπ Final Access Flow
	‚Ä¢	User cannot access https://service1.clusterA.com directly.
	‚Ä¢	User goes to:

https://swagger-proxy.clusterB.com/service1/


	‚Ä¢	Request goes ‚Üí Cluster B Nginx ‚Üí forwards to https://service1.clusterA.com/ in Cluster A.
	‚Ä¢	Response comes back via the proxy.

‚∏ª

‚úÖ No changes needed in Cluster A (since it‚Äôs already exposed by Ingress).
‚úÖ Only deploy a proxy Deployment + Service + Ingress in Cluster B.

‚∏ª

Do you want me to extend this YAML so you can proxy an arbitrary list of services (like 10+ Swagger endpoints) with just one config?