

Perfect üëç ‚Äî your plan with two separate deployments (one for swagger-ui, one for swagger-proxy) is the cleanest way to avoid sub_filter headaches.

Here‚Äôs how it works:

‚∏ª

üèó Deployment Model
	1.	Swagger UI Deployment (Cluster B)
	‚Ä¢	Serves UI + swagger-config.yaml.
	‚Ä¢	All URLs in swagger-config.yaml point to https://swagger.clusterB.com (for UI spec) and https://actor.clusterB.com, https://agent.clusterB.com, etc.
	‚Ä¢	So Swagger never ‚Äúsees‚Äù Cluster A.
	2.	Swagger Proxy Deployment (Cluster B)
	‚Ä¢	NGINX that proxies each API hostname in Cluster B ‚Üí Cluster A (e.g., actor.clusterB.com ‚Üí actor.clusterA.com).
	‚Ä¢	Handles CORS, headers, TLS termination (via Ingress).

‚∏ª

üîÑ Request Flow
	‚Ä¢	User ‚Üí https://swagger.clusterB.com ‚Üí Swagger UI pod.
	‚Ä¢	Swagger UI ‚Üí requests https://actor.clusterB.com/v1/... ‚Üí Swagger Proxy pod ‚Üí https://actor.clusterA.com/v1/....
	‚Ä¢	Browser always sees only *.clusterB.com.

‚∏ª

‚úÖ Benefits
	‚Ä¢	No sub_filter needed (UI spec already points to B-hosts).
	‚Ä¢	No gzip hacks or response rewriting.
	‚Ä¢	Clear separation:
	‚Ä¢	swagger-ui = UI only.
	‚Ä¢	swagger-proxy = gateway for API calls.
	‚Ä¢	Works for both browser (Swagger UI) and direct curl calls to backend APIs.

‚∏ª

Next Step

Do you want me to give you:
	‚Ä¢	Helm templates for both swagger-ui and swagger-proxy (with ConfigMaps and Ingress),
	‚Ä¢	Or just the NGINX swagger-proxy (since Swagger UI is a standard container image and only needs config)?





==

Nice detective work ‚Äî that error is almost always caused by a malformed URL hitting a component that tries to parse a port and finds garbage instead of digits. In your case the symptoms (https;//actors.clusterA.com in earlier logs and the ‚ÄúPort number is not a decimal‚Ä¶‚Äù message) point to a typo or bad rewrite (missing colon after https: or an accidental scheme included inside a Host header) ‚Äî not a TLS/CORS runtime issue.

Below I give a short diagnosis checklist, then safe fixes (nginx conf changes + test commands) and a fallback. Apply the fixes in your proxy (Cluster B) only.

‚∏ª

Quick diagnosis checklist (run these first)
	1.	Fetch the OpenAPI/spec through the proxy and inspect it (this shows whether sub_filter produced a malformed URL):

# from a machine that can reach the proxy ingress
curl -sS -D - "https://swagger.clusterB.com/path/to/api-spec" | sed -n '1,120p'
# or save body
curl -sS "https://swagger.clusterB.com/path/to/api-spec" -o /tmp/spec.json && grep -n "clusterA" /tmp/spec.json && sed -n '1,120p' /tmp/spec.json

Look for bad strings like https;//actors.clusterA.com or https:/actors.clusterA.com or any : inside host where it shouldn‚Äôt be.
	2.	Check nginx error logs (proxy pod):

kubectl logs -l app=swagger-proxy -c nginx -n <ns> --tail=200

	3.	curl -v the failing URL via the proxy (shows Host header and response headers):

curl -vk -H "Host: actor.clusterB.com" "https://<ingress-ip-or-dns>/"
# or
curl -vk "https://actor.clusterB.com/public/v1/api-spec"

If output includes URL rejected: Port number..., that component that rejects will often log the exact malformed URL.

If you already see https;// or missing : in the spec or logs ‚Äî skip to the fix below.

‚∏ª

Root causes to watch for (most common)
	‚Ä¢	Typo in sub_filter replacement string (e.g. https;// instead of https://).
	‚Ä¢	sub_filter replaced scheme but not host properly, producing an invalid URL in JSON.
	‚Ä¢	Host header contains full URL (including https://) ‚Äî Host must be hostname (and optional :port) only.
	‚Ä¢	Using variable proxy_pass incorrectly so the final URL is malformed.
	‚Ä¢	Sub_filter applied to HTML/JS that later gets passed to code that tries to parse a port.

‚∏ª

Correct, safe fixes

1) Fix any bad sub_filter typos

Make sure every sub_filter replacement uses correct https:// (colon + two slashes). Example (clusterA ‚Üí clusterB):

# inside location that proxies swagger.spec (disable gzip for sub_filter)
proxy_set_header Accept-Encoding "";
sub_filter 'https://swagger.clusterA.com' 'https://swagger.clusterB.com';
sub_filter 'https://actor.clusterA.com'   'https://actor.clusterB.com';
sub_filter 'https://agent.clusterA.com'   'https://agent.clusterB.com';
sub_filter_once off;

Common mistake: sub_filter 'https;//actor.clusterA.com' ... ‚Äî note the semicolon instead of colon. Fix it.

‚∏ª

2) Ensure Host header is ONLY the hostname (no scheme)

Do not send scheme in Host. Use:

proxy_set_header Host actor.clusterA.com;
# or for dynamic mapping: (but avoid scheme)
proxy_set_header Host $upstream_host;   # where $upstream_host is just "actor.clusterA.com"

If you previously set proxy_set_header Host $host; that‚Äôs fine (it will be actor.clusterB.com) ‚Äî but never proxy_set_header Host https://actor.clusterA.com.

‚∏ª

3) Use explicit proxy_pass targets (no malformed variables)

Prefer static proxy_pass entries generated by Helm from values.yaml. Example:

location /actor {
  proxy_pass https://actor.clusterA.com;
  proxy_ssl_server_name on;
  proxy_set_header Host actor.clusterA.com;
}

If you must use variables in proxy_pass https://$upstream$request_uri; then you need resolver (not the cause of the port error, but a caution).

‚∏ª

4) Disable gzip only where you use sub_filter

In the location that runs sub_filter, ensure:

proxy_set_header Accept-Encoding "";
# and optionally
gzip off;

This guarantees Nginx receives plaintext to rewrite.

‚∏ª

5) Make sure sub_filter order & patterns are safe

If your spec contains http:// and https:// variants, include both patterns, and replace full hostnames (not partial). Example:

sub_filter 'https://actor.clusterA.com' 'https://actor.clusterB.com';
sub_filter 'http://actor.clusterA.com'  'https://actor.clusterB.com';  # if swagger uses http


‚∏ª

Example final nginx snippet (K8s ConfigMap) ‚Äî corrected and minimal

Put this in your ConfigMap (the location where the swagger spec is proxied):

http {
  gzip off;

  server {
    listen 8080;
    server_name swagger.clusterB.com;

    location / {
      # Proxy UI and rewrite spec URLs
      proxy_pass https://swagger.clusterA.com;
      proxy_ssl_server_name on;

      proxy_set_header Host swagger.clusterA.com;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;

      # Prevent upstream gzip so sub_filter can operate
      proxy_set_header Accept-Encoding "";

      # rewrite ClusterA urls in the spec -> ClusterB equivalents (must be exact)
      sub_filter 'https://swagger.clusterA.com' 'https://swagger.clusterB.com';
      sub_filter 'https://actor.clusterA.com'   'https://actor.clusterB.com';
      sub_filter 'https://agent.clusterA.com'   'https://agent.clusterB.com';
      sub_filter_once off;
    }

    # Swagger internal API paths (so UI can use /actor path)
    location /actor {
      proxy_pass https://actor.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host actor.clusterA.com;

      add_header 'Access-Control-Allow-Origin' '*' always;
      add_header 'Access-Control-Allow-Methods' 'GET,POST,PUT,DELETE,OPTIONS' always;
      add_header 'Access-Control-Allow-Headers' 'Authorization,Content-Type,Accept' always;

      if ($request_method = OPTIONS) {
        return 204;
      }
    }

    # ... additional /agent etc
  }

  # Direct host vhosts for curl (actor.clusterB.com)
  server {
    listen 8080;
    server_name actor.clusterB.com;
    location / {
      proxy_pass https://actor.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host actor.clusterA.com;
      add_header 'Access-Control-Allow-Origin' '*' always;
    }
  }

  # agent.server block...
}

Double-check there are no typos in 'https://...' strings.

‚∏ª

Testing steps (after updating ConfigMap & redeploy)
	1.	Get the spec via proxy and inspect (very important to detect malformed strings):

curl -sS "https://swagger.clusterB.com/path/to/api-spec" -o /tmp/spec.json
# inspect
grep -n "clusterA" /tmp/spec.json || true
sed -n '1,200p' /tmp/spec.json

You should not see https;// or https:/ or any incorrect scheme.
	2.	Curl the proxied host (simulate the browser request):

curl -vk "https://actor.clusterB.com/public/v1/api-spec"

If the request returns the spec JSON (HTTP 200) and includes Access-Control-Allow-Origin header, browser should be happy.
	3.	Test Swagger UI in browser (DevTools Network tab) ‚Äî watch for:
	‚Ä¢	The OpenAPI spec fetch coming from swagger.clusterB.com and references to actor.clusterB.com
	‚Ä¢	Response headers include CORS
	‚Ä¢	No console Fetch error https://actors.clusterA.com... entries
	4.	Check logs if error persists:

kubectl logs -l app=swagger-proxy -n <ns> --tail=200
# check ingress controller logs too (it may log rejections)
kubectl logs -l app.kubernetes.io/name=ingress-nginx -n ingress-nginx --tail=200


‚∏ª

Fallbacks if rewriting still causes trouble
	‚Ä¢	Modify the OpenAPI spec at source (Cluster A) to use relative servers: - url: / or set servers to use proxy hostnames ‚Äî this avoids ad-hoc sub_filter. Preferred if you control spec generation.
	‚Ä¢	Return a modified spec from Cluster B as a dedicated endpoint: Nginx can proxy_pass then sub_filter only the spec path (reduce risk).

‚∏ª

Final note about the exact error you saw

The exact message Port number is not a decimal number between 0 and 65535 strongly implies a malformed host:port string was parsed (usually because : is missing or extra characters present). Fixing sub_filter typos and ensuring Host header does not include https:// will resolve it.

‚∏ª

If you want I‚Äôll:
	‚Ä¢	Provide a ready Helm ConfigMap template with the corrected sub_filter lines (templated arrays for clusterA/clusterB hosts so you don‚Äôt hand-edit strings), and
	‚Ä¢	Give the exact curl commands you run and the expected output to confirm success.

Tell me to proceed and I‚Äôll paste the Helm-ready template next.





Perfect ‚úÖ let‚Äôs consolidate everything cleanly so you can copy-paste and run both on Kubernetes (Helm) and locally on Windows Docker.

‚∏ª

üöÄ Final Nginx Config (Kubernetes via ConfigMap)

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-conf
data:
  nginx.conf: |
    events {}

    http {
      gzip off;   # disable gzip so sub_filter can work

      server {
        listen 8080;
        server_name swagger.clusterB.com;

        # === Swagger UI proxy ===
        location / {
          proxy_pass https://swagger.clusterA.com;
          proxy_ssl_server_name on;

          proxy_set_header Host swagger.clusterA.com;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          proxy_set_header Accept-Encoding "";

          # Fix Swagger spec URLs by rewriting clusterA ‚Üí clusterB
          sub_filter 'https://swagger.clusterA.com' 'https://swagger.clusterB.com';
          sub_filter 'https://actor.clusterA.com'   'https://actor.clusterB.com';
          sub_filter 'https://agent.clusterA.com'   'https://agent.clusterB.com';
          sub_filter_once off;

          # CORS for UI
          add_header 'Access-Control-Allow-Origin' '*' always;
          add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;
          add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type, Accept, Origin, User-Agent, X-Requested-With' always;
          if ($request_method = OPTIONS) {
            return 204;
          }
        }

        # === Backend APIs via swagger.clusterB.com paths ===
        location /actor {
          proxy_pass https://actor.clusterA.com;
          proxy_ssl_server_name on;
          proxy_set_header Host actor.clusterA.com;

          add_header 'Access-Control-Allow-Origin' '*' always;
        }

        location /agent {
          proxy_pass https://agent.clusterA.com;
          proxy_ssl_server_name on;
          proxy_set_header Host agent.clusterA.com;

          add_header 'Access-Control-Allow-Origin' '*' always;
        }
      }

      # === Direct backend API proxies (curl) ===
      server {
        listen 8080;
        server_name actor.clusterB.com;

        location / {
          proxy_pass https://actor.clusterA.com;
          proxy_ssl_server_name on;
          proxy_set_header Host actor.clusterA.com;

          add_header 'Access-Control-Allow-Origin' '*' always;
        }
      }

      server {
        listen 8080;
        server_name agent.clusterB.com;

        location / {
          proxy_pass https://agent.clusterA.com;
          proxy_ssl_server_name on;
          proxy_set_header Host agent.clusterA.com;

          add_header 'Access-Control-Allow-Origin' '*' always;
        }
      }
    }

üëâ Notes for Kube:
	‚Ä¢	Container should expose port 8080.
	‚Ä¢	Service maps 80 ‚Üí 8080.
	‚Ä¢	Ingress points external hostnames (swagger.clusterB.com, actor.clusterB.com, etc.) to the service.
	‚Ä¢	sub_filter rewrites Swagger‚Äôs OpenAPI spec so browser never sees ClusterA URLs.

‚∏ª

üñ• Final Nginx Config (Local Windows Docker)

Save as nginx.conf in your project folder and mount into container:

events {}

http {
  gzip off;

  server {
    listen 80;
    server_name localhost;

    # Swagger UI proxy on localhost
    location / {
      proxy_pass https://swagger.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host swagger.clusterA.com;

      proxy_set_header Accept-Encoding "";

      sub_filter 'https://swagger.clusterA.com' 'http://localhost';
      sub_filter 'https://actor.clusterA.com'   'http://localhost/actor';
      sub_filter 'https://agent.clusterA.com'   'http://localhost/agent';
      sub_filter_once off;

      add_header 'Access-Control-Allow-Origin' '*' always;
      add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;
      add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type, Accept, Origin, User-Agent, X-Requested-With' always;
      if ($request_method = OPTIONS) {
        return 204;
      }
    }

    # Localhost direct paths
    location /actor {
      proxy_pass https://actor.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host actor.clusterA.com;
      add_header 'Access-Control-Allow-Origin' '*' always;
    }

    location /agent {
      proxy_pass https://agent.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host agent.clusterA.com;
      add_header 'Access-Control-Allow-Origin' '*' always;
    }
  }
}

üëâ Run it locally:

docker run --rm -p 80:80 -v ${PWD}/nginx.conf:/opt/bitnami/nginx/conf/nginx.conf:ro bitnami/nginx

Then open http://localhost ‚Üí it proxies to Swagger UI from ClusterA, but Swagger spec + backend API calls go through your proxy.

‚∏ª

üìå Summary
	‚Ä¢	Swagger UI (swagger.clusterB.com or localhost) shows UI and API docs.
	‚Ä¢	Backend APIs (actor, agent, etc.) proxied correctly both from Swagger UI and via curl.
	‚Ä¢	CORS enabled, gzip disabled, sub_filter rewrites clusterA ‚Üí clusterB.

‚∏ª

Do you also want me to provide the Helm values.yaml snippet (so you can just drop in your list of backend APIs instead of hardcoding /actor, /agent)?




Got it ‚úÖ
Here‚Äôs a unified curl-based health audit script you can run in Kubernetes (ClusterB) or on your local Windows Docker test.

The difference is just the hostnames:
	‚Ä¢	In Kubernetes, you hit real hosts like swagger.clusterB.com, actor.clusterB.com.
	‚Ä¢	In local Docker, you use localhost (since you mapped Nginx port to 8080/80) and test with paths.

‚∏ª

1. Kubernetes Health Audit Script (health-kube.sh)

#!/bin/bash
# Health audit for Swagger proxy (ClusterB Kubernetes)

# Hosts exposed via Ingress in ClusterB
SWAGGER_HOST="https://swagger.clusterB.com"
API_HOSTS=(
  "https://actor.clusterB.com/public/v1/api-spec"
  "https://agent.clusterB.com/public/v1/api-spec"
  "https://payment.clusterB.com/public/v1/api-spec"
)

echo "=== Swagger Proxy Health Audit (Kubernetes) ==="
echo "Checking Swagger UI: $SWAGGER_HOST"

swagger_status=$(curl -sk -o /dev/null -w "%{http_code}" "$SWAGGER_HOST")
if [[ "$swagger_status" =~ ^(200|401|403)$ ]]; then
  echo "‚úÖ Swagger UI reachable ($swagger_status)"
else
  echo "‚ùå Swagger UI failed ($swagger_status)"
fi
echo

for api in "${API_HOSTS[@]}"; do
  echo "Checking API: $api"
  status=$(curl -sk -o /dev/null -w "%{http_code}" "$api")
  if [[ "$status" =~ ^(200|401|403)$ ]]; then
    echo "  ‚úÖ OK ($status)"
  else
    echo "  ‚ùå FAIL ($status)"
  fi
done

echo "=== Audit complete ==="


‚∏ª

2. Local Docker Health Audit Script (health-local.sh)

When you run Nginx locally (on Windows Docker), you usually expose port 8080:80.
So, instead of swagger.clusterB.com, you test against http://localhost:8080.

#!/bin/bash
# Health audit for Swagger proxy (Local Docker test)

# Swagger UI via localhost
SWAGGER_HOST="http://localhost:8080"
# API endpoints mapped as /actor, /agent etc. in nginx.conf
API_PATHS=(
  "http://localhost:8080/actor/public/v1/api-spec"
  "http://localhost:8080/agent/public/v1/api-spec"
)

echo "=== Swagger Proxy Health Audit (Local Docker) ==="
echo "Checking Swagger UI: $SWAGGER_HOST"

swagger_status=$(curl -s -o /dev/null -w "%{http_code}" "$SWAGGER_HOST")
if [[ "$swagger_status" =~ ^(200|401|403)$ ]]; then
  echo "‚úÖ Swagger UI reachable ($swagger_status)"
else
  echo "‚ùå Swagger UI failed ($swagger_status)"
fi
echo

for api in "${API_PATHS[@]}"; do
  echo "Checking API: $api"
  status=$(curl -s -o /dev/null -w "%{http_code}" "$api")
  if [[ "$status" =~ ^(200|401|403)$ ]]; then
    echo "  ‚úÖ OK ($status)"
  else
    echo "  ‚ùå FAIL ($status)"
  fi
done

echo "=== Audit complete ==="


‚∏ª

üîë Key difference:
	‚Ä¢	ClusterB/Kube: uses real DNS hosts (swagger.clusterB.com).
	‚Ä¢	Local Docker: uses localhost:8080 and paths (/actor, /agent).

‚∏ª

üëâ Do you want me to also add this as a Kubernetes Job (post-sync hook in ArgoCD) so the audit runs automatically after every deploy?


Perfect üëç Let‚Äôs lock this down with a minimal, clean Nginx config that:
	‚Ä¢	Proxies swagger.clusterB.com ‚Üí swagger.clusterA.com
	‚Ä¢	Proxies backend APIs (actor.clusterB.com ‚Üí actor.clusterA.com, etc.)
	‚Ä¢	Adds CORS headers for Swagger UI
	‚Ä¢	Keeps the standard proxy headers (Host, X-Real-IP, etc.)

‚∏ª

üîπ ConfigMap (nginx.conf) for Helm

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx
data:
  nginx.conf: |
    worker_processes 1;

    events { worker_connections 1024; }

    http {
      server {
        listen 8080;

        # Swagger UI proxy
        server_name swagger.clusterB.com;

        location / {
          proxy_pass https://swagger.clusterA.com;

          # CORS for Swagger UI
          add_header Access-Control-Allow-Origin *;
          add_header Access-Control-Allow-Methods "GET, POST, PUT, DELETE, OPTIONS";
          add_header Access-Control-Allow-Headers "Authorization, Content-Type, X-Requested-With";

          if ($request_method = OPTIONS) {
            return 204;
          }

          # Standard proxy headers
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }
      }

      # Backend API proxies
      server {
        listen 8080;
        server_name actor.clusterB.com;

        location / {
          proxy_pass https://actor.clusterA.com;

          # Standard proxy headers
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          # CORS (for browser requests hitting backend APIs directly)
          add_header Access-Control-Allow-Origin *;
          add_header Access-Control-Allow-Methods "GET, POST, PUT, DELETE, OPTIONS";
          add_header Access-Control-Allow-Headers "Authorization, Content-Type, X-Requested-With";

          if ($request_method = OPTIONS) {
            return 204;
          }
        }
      }

      # üëÜ Repeat more `server {}` blocks for agent.clusterB.com, etc.
    }


‚∏ª

üîπ Ingress (Helm template)

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  rules:
    - host: {{ .Values.clusterB.swaggerHost }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: {{ .Values.service.port }}
    {{- range .Values.clusterB.apiHosts }}
    - host: {{ . }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: {{ $.Values.service.port }}
    {{- end }}
  tls:
    - hosts:
        - {{ .Values.clusterB.swaggerHost }}
        {{- range .Values.clusterB.apiHosts }}
        - {{ . }}
        {{- end }}
      secretName: {{ .Values.ingress.tls.secretName }}
{{- end }}


‚∏ª

üîπ Example values.yaml

clusterB:
  swaggerHost: swagger.clusterB.com
  apiHosts:
    - actor.clusterB.com
    - agent.clusterB.com
    - user.clusterB.com

service:
  port: 8080

ingress:
  enabled: true
  className: nginx
  tls:
    enabled: true
    secretName: clusterb-tls


‚∏ª

‚úÖ How this works
	‚Ä¢	Swagger UI: Browser ‚Üí swagger.clusterB.com ‚Üí Nginx ‚Üí swagger.clusterA.com
	‚Ä¢	Backend APIs:
	‚Ä¢	From Swagger UI calls (/actor) ‚Üí actor.clusterA.com (proxied)
	‚Ä¢	From direct curl (curl https://actor.clusterB.com/...) ‚Üí Nginx ‚Üí actor.clusterA.com

Both flows respect CORS and proxy headers.

‚∏ª

üëâ Do you want me to extend this to generate the API server blocks dynamically from values.yaml (so you don‚Äôt duplicate for 15 services), or is static repetition fine for you?




Ah, that makes total sense üëç

What‚Äôs happening:
	‚Ä¢	Your browser loads Swagger UI from swagger.clusterB.com.
	‚Ä¢	Swagger UI then makes XHR/fetch calls to swagger.clusterB.com/actor/....
	‚Ä¢	Nginx proxies those to actor.clusterA.com.
	‚Ä¢	The response from backend does not have CORS headers, so browser blocks it.

‚∏ª

‚úÖ Fix: Add CORS headers at the Nginx proxy layer

Here‚Äôs how you extend your Nginx config:

http {
  server {
    listen 8080;
    server_name localhost;

    # Common CORS headers (applies to all proxied locations)
    location / {
      proxy_pass https://swagger.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host swagger.clusterA.com;

      # Add CORS headers
      add_header 'Access-Control-Allow-Origin' '*' always;
      add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;
      add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type, Accept, Origin, User-Agent, DNT, Cache-Control, X-Mx-ReqToken, X-Requested-With' always;

      # Handle preflight requests
      if ($request_method = OPTIONS) {
        add_header 'Access-Control-Allow-Origin' '*' always;
        add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;
        add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type, Accept, Origin, User-Agent, DNT, Cache-Control, X-Mx-ReqToken, X-Requested-With' always;
        add_header 'Access-Control-Max-Age' 1728000;
        add_header 'Content-Length' 0;
        add_header 'Content-Type' 'text/plain charset=UTF-8';
        return 204;
      }
    }

    location /actor {
      proxy_pass https://actor.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host actor.clusterA.com;

      # Same CORS headers
      add_header 'Access-Control-Allow-Origin' '*' always;
      add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;
      add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type, Accept, Origin, User-Agent, DNT, Cache-Control, X-Mx-ReqToken, X-Requested-With' always;

      if ($request_method = OPTIONS) {
        add_header 'Access-Control-Allow-Origin' '*' always;
        add_header 'Access-Control-Allow-Methods' 'GET, POST, PUT, DELETE, OPTIONS' always;
        add_header 'Access-Control-Allow-Headers' 'Authorization, Content-Type, Accept, Origin, User-Agent, DNT, Cache-Control, X-Mx-ReqToken, X-Requested-With' always;
        add_header 'Access-Control-Max-Age' 1728000;
        add_header 'Content-Length' 0;
        add_header 'Content-Type' 'text/plain charset=UTF-8';
        return 204;
      }
    }
  }
}


‚∏ª

üõ† How it works
	‚Ä¢	add_header ... always; ‚Üí ensures headers are returned on both normal responses and errors.
	‚Ä¢	Preflight (OPTIONS) requests are intercepted and return 204 No Content with the correct headers.
	‚Ä¢	Now the browser accepts responses from /actor via swagger.clusterB.com.

‚∏ª

üëâ Question: Do you want me to make this CORS block reusable (so you don‚Äôt have to repeat it for all 15 API locations), or keep it inline per location for clarity?

====


Ahh üëç got it now ‚Äî thanks for clarifying again.
You want both behaviors in the same Nginx:
	1.	Swagger UI ‚Üí swagger.clusterB.com ‚Üí proxies to swagger.clusterA.com.
	‚Ä¢	When user browses Swagger, requests like /actor inside that UI must go to actor.clusterA.com.
	‚Ä¢	So /actor path under swagger.clusterB.com should proxy ‚Üí actor.clusterA.com.
	2.	Direct backend API curl ‚Üí actor.clusterB.com ‚Üí proxies to actor.clusterA.com.
	‚Ä¢	Similarly for agent.clusterB.com, etc.

So basically:
	‚Ä¢	One Swagger server block with multiple location rules.
	‚Ä¢	Plus multiple backend server blocks (one per API).

‚∏ª

üîπ Local Docker (Windows, localhost test)

nginx.conf

events {}

http {
  # Swagger UI
  server {
    listen 80;
    server_name swagger.localhost;

    # Swagger main UI
    location / {
      proxy_pass https://swagger.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host swagger.clusterA.com;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Swagger calling backend APIs
    location /actor {
      proxy_pass https://actor.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host actor.clusterA.com;
    }

    location /agent {
      proxy_pass https://agent.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host agent.clusterA.com;
    }
  }

  # Direct backend APIs (curl)
  server {
    listen 80;
    server_name actor.localhost;

    location / {
      proxy_pass https://actor.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host actor.clusterA.com;
    }
  }

  server {
    listen 80;
    server_name agent.localhost;

    location / {
      proxy_pass https://agent.clusterA.com;
      proxy_ssl_server_name on;
      proxy_set_header Host agent.clusterA.com;
    }
  }
}

üëâ Add to hosts file on Windows:

127.0.0.1 swagger.localhost actor.localhost agent.localhost

Then:
	‚Ä¢	Open http://swagger.localhost/ ‚Üí loads Swagger from swagger.clusterA.com.
	‚Ä¢	Inside Swagger, calling /actor ‚Üí proxies actor.clusterA.com.
	‚Ä¢	Curl direct: curl http://actor.localhost/ ‚Üí proxies to actor.clusterA.com.

‚∏ª

üîπ Helm ConfigMap Template

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx
data:
  nginx.conf: |
    events {}

    http {
      server {
        listen 80;
        server_name {{ .Values.clusterB.swaggerHost }};

        location / {
          proxy_pass https://{{ .Values.clusterA.swaggerHost }};
          proxy_ssl_server_name on;
          proxy_set_header Host {{ .Values.clusterA.swaggerHost }};
        }

        {{- range $path, $apiHost := .Values.clusterB.swaggerPaths }}
        location {{ $path }} {
          proxy_pass https://{{ $apiHost | replace "clusterB.com" "clusterA.com" }};
          proxy_ssl_server_name on;
          proxy_set_header Host {{ $apiHost | replace "clusterB.com" "clusterA.com" }};
        }
        {{- end }}
      }

      {{- range .Values.clusterB.apiHosts }}
      server {
        listen 80;
        server_name {{ . }};

        location / {
          proxy_pass https://{{ regexReplaceAll "clusterB.com" . "clusterA.com" }};
          proxy_ssl_server_name on;
          proxy_set_header Host {{ regexReplaceAll "clusterB.com" . "clusterA.com" }};
        }
      }
      {{- end }}
    }


‚∏ª

üîπ Helm values.yaml

clusterB:
  swaggerHost: swagger.clusterB.com
  swaggerPaths:
    /actor: actor.clusterB.com
    /agent: agent.clusterB.com
  apiHosts:
    - actor.clusterB.com
    - agent.clusterB.com

clusterA:
  swaggerHost: swagger.clusterA.com


‚∏ª

üîπ Ingress Helm Template

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
spec:
  rules:
    - host: {{ .Values.clusterB.swaggerHost }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: 80
    {{- range .Values.clusterB.apiHosts }}
    - host: {{ . }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: 80
    {{- end }}


‚∏ª

‚úÖ This setup:
	‚Ä¢	Swagger UI proxied at swagger.clusterB.com ‚Üí swagger.clusterA.com.
	‚Ä¢	Swagger API calls (/actor, /agent) also proxied correctly.
	‚Ä¢	Direct curl to backend (actor.clusterB.com, agent.clusterB.com) also works.

‚∏ª

Do you want me to also show you a single combined Helm values structure (swagger paths + backend hosts in one mapping), so you don‚Äôt repeat hostnames twice? That would simplify maintenance if you have ~15 backend services.
					........=



Got it ‚úÖ
If you are not using variables like $api_host in proxy_pass (i.e., you write the actual hostname directly from your Helm values.yaml), then Nginx resolves it at startup and uses normal DNS.
üëâ In that case, no resolver is needed because Kubernetes pods already use CoreDNS.

‚∏ª

üîπ Example Helm values.yaml

swaggerProxy:
  swaggerHost: swagger.clusterA.com
  apiHosts:
    - name: actor
      host: api-actor.clusterA.com
    - name: agent
      host: api-agent.clusterA.com
    - name: order
      host: api-order.clusterA.com


‚∏ª

üîπ ConfigMap Template (templates/configmap.yaml)

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-config
data:
  nginx.conf: |
    events {}

    http {
      server {
        listen 80;

        # Swagger UI proxy
        location / {
          proxy_pass https://{{ .Values.swaggerProxy.swaggerHost }};
          proxy_ssl_server_name on;

          proxy_set_header Host {{ .Values.swaggerProxy.swaggerHost }};
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }

        {{- range .Values.swaggerProxy.apiHosts }}
        # Backend API proxy for {{ .name }}
        server_name {{ .name }}.clusterB.com;

        location / {
          proxy_pass https://{{ .host }};
          proxy_ssl_server_name on;

          proxy_set_header Host {{ .host }};
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }
        {{- end }}
      }
    }


‚∏ª

üîπ How it Works
	‚Ä¢	Swagger UI
	‚Ä¢	Request: https://swagger.clusterB.com/
	‚Ä¢	Proxied to: https://swagger.clusterA.com/
	‚Ä¢	UI still shows browser URL as swagger.clusterB.com.
	‚Ä¢	API backends (from Swagger or direct curl)
	‚Ä¢	Request: https://actor.clusterB.com/v1/...
	‚Ä¢	Proxied to: https://api-actor.clusterA.com/v1/....
	‚Ä¢	Request: https://agent.clusterB.com/v1/...
	‚Ä¢	Proxied to: https://api-agent.clusterA.com/v1/....

No resolver is needed because proxy_pass uses static hostnames from values.yaml.

‚∏ª

‚ö° Question for you:
Do you want me to also provide the Ingress template (single file) for this Helm chart so that both swagger.clusterB.com and actor.clusterB.com etc. are exposed automatically?

==###
Ahh I see üëç ‚Äî you don‚Äôt want to bother with swagger.clusterB.com or api-actor.clusterB.com in local Docker Windows, only localhost should work.

That means in local testing you‚Äôll hit http://localhost/swagger and http://localhost/api-actor/... etc.

Here‚Äôs a clean localhost Nginx conf üëá

‚∏ª

üîπ nginx.conf (Localhost only)

events {}

http {
    resolver 8.8.8.8 ipv6=off;

    server {
        listen 80;
        server_name localhost;

        # Swagger UI proxy
        location /swagger/ {
            proxy_pass https://swagger.clusterA.com/;
            proxy_ssl_server_name on;

            proxy_set_header Host swagger.clusterA.com;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # API Actor proxy
        location /api-actor/ {
            proxy_pass https://api-actor.clusterA.com/;
            proxy_ssl_server_name on;

            proxy_set_header Host api-actor.clusterA.com;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Add more APIs like /api-agent/ ‚Üí https://api-agent.clusterA.com/
    }
}


‚∏ª

üîπ Run it locally

docker run --rm -d -p 80:80 -v %cd%/nginx.conf:/etc/nginx/nginx.conf:ro --name nginx-proxy nginx


‚∏ª

üîπ Test calls
	‚Ä¢	Browser ‚Üí http://localhost/swagger/ ‚Üí Proxies to swagger.clusterA.com
	‚Ä¢	Curl ‚Üí

curl http://localhost/api-actor/v1/getActors

Will internally call https://api-actor.clusterA.com/v1/getActors ‚úÖ

‚∏ª

‚ö° This way, you don‚Äôt need to touch hosts file or fake DNS, everything works on localhost only.

Do you want me to also make this dynamic (so you don‚Äôt hardcode /api-actor/, /api-agent/, etc.) for your local Docker test?


====
Good catch üëç ‚Äî let me clarify why my answer looked like ‚Äútwo responses‚Äù and which one is the cleanest.

‚∏ª

The ‚Äútwo‚Äù responses in my answer
	‚Ä¢	Part 1 (local Windows Docker test config):
That was for you to quickly spin up an Nginx container on your laptop and check the proxy behavior without Kubernetes/Helm.
	‚Ä¢	Part 2 (Helm chart templates + values):
That was for deploying the same logic cleanly in Kubernetes with Ingress and TLS.

Both implement Approach #2 (explicit host ‚Üí host mapping).
So it‚Äôs not two different strategies, just two environments (local test vs. K8s).

‚∏ª

Which is the best, simplest, cleanest, and most performant?

üëâ Approach #2 (explicit allow-list of backend APIs)
	‚Ä¢	Security: No wildcard exposure like Approach #3 (which risks exposing everything on cluster A).
	‚Ä¢	Correctness: Unlike Approach #1, this works for both Swagger UI and direct backend curls.
	‚Ä¢	Performance: 15‚Äì20 server_name entries in Nginx is nothing ‚Äî Nginx matches hosts in O(1) hash table lookup, so there‚Äôs no noticeable cost.
	‚Ä¢	Clarity: Each exposed API is explicit in values.yaml ‚Üí zero ambiguity about what‚Äôs reachable.

‚∏ª

The cleanest way to write it

You don‚Äôt actually need complicated regex rewrites.
The simplest effective config is:

# Swagger UI
server {
  listen 80;
  server_name swagger.clusterB.com;

  location / {
    proxy_pass https://swagger.clusterA.com;
    proxy_ssl_server_name on;
    proxy_set_header Host swagger.clusterA.com;
  }
}

# Backend APIs (all explicit)
server {
  listen 80;
  server_name
    api-actor.clusterB.com
    api-agent.clusterB.com
    api-movie.clusterB.com;

  location / {
    # Map B ‚Üí A directly by replacing suffix
    if ($host ~* "^([^.]+)\.clusterB\.com$") {
      set $upstream $1.clusterA.com;
    }
    proxy_pass https://$upstream$request_uri;
    proxy_ssl_server_name on;
    proxy_set_header Host $upstream;
  }
}

And in Helm, you‚Äôd just loop over the allowed apiHosts list in the Ingress rule and leave the mapping logic in Nginx.

‚∏ª

‚úÖ So the final recommendation:
	‚Ä¢	Use Approach #2 with an allow-list.
	‚Ä¢	Keep config simple (one server_name block for Swagger, one for APIs).
	‚Ä¢	Manage the hostnames via Helm values.yaml.

‚∏ª

Do you want me to refactor the Helm template I gave into this ‚Äúsimple two-block version‚Äù (Swagger block + API block) so you get a clean minimal chart without regex spaghetti?

...====
events {}

http {
  # Swagger UI proxy
  server {
    listen 80;
    server_name localhost;  # for testing, point Swagger to localhost

    location /swagger {
      proxy_pass http://localhost:8081;  # dummy Swagger server
      proxy_set_header Host localhost;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
    }
  }

  # Dynamic backend API proxy
  server {
    listen 80;
    server_name localhost;  # all other localhost requests

    resolver 8.8.8.8;

    location /api/ {
      set $upstream "";

      # Example: route /api/actor -> http://localhost:8082
      if ($request_uri ~ "^/api/actor") {
        set $upstream http://localhost:8082;
      }

      proxy_pass $upstream;
      proxy_set_header Host $upstream;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
    }
  }
}

Good question üëç

The ingressClassName in Kubernetes tells the Ingress object which Ingress Controller should process it.

‚∏ª

Why it exists
	‚Ä¢	Kubernetes supports multiple ingress controllers (Nginx, AWS ALB, Traefik, Istio, HAProxy, etc.) running in the same cluster.
	‚Ä¢	By default, an Ingress controller only looks at Ingress objects that match its class.
	‚Ä¢	If you don‚Äôt set it, the controller with --watch-ingress-without-class=true may pick it up (depends on your setup).

‚∏ª

Example:

ingressClassName: nginx

	‚Ä¢	This means: ‚ÄúOnly the Nginx Ingress Controller should handle this ingress.‚Äù
	‚Ä¢	If you had AWS ALB ingress installed, you‚Äôd instead use:

ingressClassName: alb


‚∏ª

TL;DR
	‚Ä¢	If your cluster has only one ingress controller (say Nginx) ‚Üí setting ingressClassName: nginx is best practice (explicit, avoids ambiguity).
	‚Ä¢	If you omit it, it might still work, but then you‚Äôre relying on defaults, which can break later if someone installs another ingress controller.

‚∏ª

üëâ In your case (Helm chart for swagger-proxy), keeping

ingressClassName: nginx

is the safest, because you‚Äôre targeting the Nginx ingress controller for swagger.clusterB.com and api-*.clusterB.com.

‚∏ª

Do you want me to show you how this looks in wildcard ingress (*.clusterB.com) so you don‚Äôt need to list every API host individually?


Perfect üëç let‚Äôs make your Helm dynamic ConfigMap production-ready by adding:
	‚Ä¢	‚úÖ Dynamic upstream hosts from values.yaml (swagger + APIs)
	‚Ä¢	‚úÖ Keepalive timeout (client reuse)
	‚Ä¢	‚úÖ Proxy timeouts (connect, send, read)
	‚Ä¢	‚úÖ MIME types for static content fallback

‚∏ª

1. values.yaml

swaggerProxy:
  swaggerHost: swagger.clusterA.com
  apiHosts:
    - api-actor.clusterA.com
    - api-movie.clusterA.com
    - api-user.clusterA.com
  timeouts:
    keepalive: 65s
    connect: 10s
    send: 60s
    read: 60s


‚∏ª

2. ConfigMap template (configmap.yaml)

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx-conf
data:
  nginx.conf: |
    worker_processes  1;

    events { worker_connections 1024; }

    http {
      include       mime.types;
      default_type  application/octet-stream;

      # Client keepalive timeout
      keepalive_timeout {{ .Values.swaggerProxy.timeouts.keepalive }};

      server {
        listen 80;

        # Swagger UI proxy
        location / {
          proxy_pass https://{{ .Values.swaggerProxy.swaggerHost }};
          proxy_ssl_server_name on;

          proxy_set_header Host {{ .Values.swaggerProxy.swaggerHost }};
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          proxy_connect_timeout {{ .Values.swaggerProxy.timeouts.connect }};
          proxy_send_timeout {{ .Values.swaggerProxy.timeouts.send }};
          proxy_read_timeout {{ .Values.swaggerProxy.timeouts.read }};
        }

        # Dynamic API proxy (wildcard *.clusterA.com)
        location /api/ {
          resolver 8.8.8.8;   # Or kube-dns in-cluster
          set $api_host "";
          if ($uri ~ ^/api/([^/]+)(/.*)$) {
            set $api_host $1.clusterA.com;
            set $api_path $2;
          }

          proxy_pass https://$api_host$api_path;
          proxy_ssl_server_name on;

          proxy_set_header Host $api_host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          proxy_connect_timeout {{ .Values.swaggerProxy.timeouts.connect }};
          proxy_send_timeout {{ .Values.swaggerProxy.timeouts.send }};
          proxy_read_timeout {{ .Values.swaggerProxy.timeouts.read }};
        }
      }
    }


‚∏ª

3. How this works

Component	Purpose
swagger.clusterB.com	UI entrypoint. Requests ‚Üí proxied to swagger.clusterA.com.
/api/<service>/...	Dynamic rule: extracts <service> from path ‚Üí rewrites to <service>.clusterA.com/....
resolver 8.8.8.8	Lets Nginx resolve *.clusterA.com. In cluster, replace with kube-dns.kube-system.svc.cluster.local.
Timeouts (connect, send, read)	Protects against hanging requests.
keepalive_timeout	Keeps client connection open for reuse (65s).
MIME types	Ensures static content fallback (Swagger UI assets, yaml/json).


‚∏ª

‚úÖ With this one ConfigMap, you cover:
	‚Ä¢	Swagger UI proxy
	‚Ä¢	Multiple API backends (dynamic)
	‚Ä¢	Production-safe timeouts
	‚Ä¢	Browser always sees swagger.clusterB.com, never clusterA.com.

‚∏ª

Do you want me to also provide the Ingress.yaml Helm template that pairs with this ConfigMap (supporting TLS + multiple hosts with wildcard)?

Perfect ‚úÖ let‚Äôs wrap it all into one Helm chart skeleton that does what you need:
	‚Ä¢	Dynamic Nginx ConfigMap ‚Üí proxies both Swagger UI & multiple backend APIs (*.clusterA.com)
	‚Ä¢	Single Ingress ‚Üí exposes swagger.clusterB.com + *.clusterB.com to Cluster B
	‚Ä¢	Deployment + Service ‚Üí runs the proxy pod

‚∏ª

üìÇ Chart structure

swagger-proxy/
  Chart.yaml
  values.yaml
  templates/
    configmap.yaml
    deployment.yaml
    service.yaml
    ingress.yaml


‚∏ª

üìÑ Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Nginx reverse proxy for Swagger + APIs (ClusterB ‚Üí ClusterA)
version: 0.1.0
appVersion: "1.0"


‚∏ª

üìÑ values.yaml

image:
  repository: bitnami/nginx
  tag: 1.27
  pullPolicy: IfNotPresent

replicaCount: 1

service:
  type: ClusterIP
  port: 80

# Proxy target cluster A
clusterA:
  swaggerHost: swagger.clusterA.com
  apiDomain: clusterA.com

# Ingress settings
ingress:
  enabled: true
  className: nginx
  hosts:
    - swagger.clusterB.com
    - "*.clusterB.com"
  tls:
    enabled: true
    secretName: clusterb-tls


‚∏ª

üìÑ templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx-conf
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 80;

        # Swagger UI proxy
        location / {
          proxy_pass https://{{ .Values.clusterA.swaggerHost }};
          proxy_ssl_server_name on;

          proxy_set_header Host {{ .Values.clusterA.swaggerHost }};
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Dynamic backend API proxy
        # Example: https://api-actor.clusterB.com -> https://api-actor.clusterA.com
        location / {
          resolver 8.8.8.8;   # (or kube-dns inside cluster)
          set $upstream "";

          if ($host ~* "^([^.]+)\.clusterB\.com$") {
            set $upstream $1.{{ .Values.clusterA.apiDomain }};
          }

          proxy_pass https://$upstream$request_uri;
          proxy_ssl_server_name on;

          proxy_set_header Host $upstream;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }
      }
    }


‚∏ª

üìÑ templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
        - name: nginx
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          volumeMounts:
            - name: nginx-conf
              mountPath: /opt/bitnami/nginx/conf/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: nginx-conf
          configMap:
            name: swagger-proxy-nginx-conf


‚∏ª

üìÑ templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy-svc
spec:
  type: {{ .Values.service.type }}
  selector:
    app: swagger-proxy
  ports:
    - port: {{ .Values.service.port }}
      targetPort: 8080
      protocol: TCP


‚∏ª

üìÑ templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  ingressClassName: {{ .Values.ingress.className }}
  rules:
  {{- range .Values.ingress.hosts }}
    - host: {{ . | quote }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: {{ $.Values.service.port }}
  {{- end }}
  {{- if .Values.ingress.tls.enabled }}
  tls:
    - hosts:
      {{- range .Values.ingress.hosts }}
        - {{ . | quote }}
      {{- end }}
      secretName: {{ .Values.ingress.tls.secretName }}
  {{- end }}
{{- end }}


‚∏ª

üìä How it works (flow)

Browser request	Goes to Ingress	Proxy Pod forwards to
https://swagger.clusterB.com	swagger-proxy	https://swagger.clusterA.com
https://actor.clusterB.com	swagger-proxy	https://actor.clusterA.com
curl https://xyz.clusterB.com	swagger-proxy	https://xyz.clusterA.com


‚∏ª

‚ö° So:
	‚Ä¢	One Ingress (*.clusterB.com)
	‚Ä¢	One Proxy Service
	‚Ä¢	Dynamic ConfigMap ‚Üí resolves any <api>.clusterB.com to <api>.clusterA.com

‚∏ª

Do you want me to also include a Bamboo build + Helm package + push to Artifactory + ArgoCD sync script that works with this chart so you can fully automate?

===

Yes ‚Äî this approach scales fine for 10+ backend APIs, but there are a few important points and best practices to keep in mind:

‚∏ª

üîπ How it works for multiple APIs
	1.	Generic API server block in Nginx:

server {
  listen 80;
  server_name api-actor.clusterB.com api-payment.clusterB.com ...; # all APIs

  resolver 10.96.0.10;

  location / {
    if ($host ~ ^(.+)\.clusterB\.com$) {
      set $api_host $1.clusterA.com;
    }

    proxy_pass https://$api_host$request_uri;
    proxy_ssl_server_name on;
    proxy_set_header Host $api_host;
    proxy_set_header X-Real-IP $remote_addr;
    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
  }
}

‚úÖ This one server block can handle any number of API hosts because the $host variable dynamically maps *.clusterB.com ‚Üí *.clusterA.com.

‚∏ª

üîπ Things to consider

Aspect	Notes / Recommendation
Nginx server_name limits	Nginx doesn‚Äôt really limit server_name, but 100+ names may slightly increase parsing time. Use wildcard *.clusterB.com if you have many APIs.
Resolver	Must point to in-cluster DNS (like 10.96.0.10) so dynamic $api_host works.
sub_filter	If you rewrite Swagger JSON for multiple APIs, ensure all backend URLs are replaced. Can use sub_filter multiple times or a regex-based solution.
Ingress	If you use wildcard ingress (*.clusterB.com) you don‚Äôt need 1 rule per API, just one wildcard rule pointing to the same service.
TLS	Requires wildcard certificate (*.clusterB.com) to cover all API subdomains.
Performance	Nginx handles many proxy_pass rules well, but monitor memory & connection limits if you have hundreds of APIs or high traffic.


‚∏ª

üîπ Recommendation for >10 APIs
	‚Ä¢	Use wildcard hostnames in Nginx: server_name *.clusterB.com;
	‚Ä¢	Use wildcard TLS cert: *.clusterB.com
	‚Ä¢	List APIs in values.yaml.hosts.apis if you want explicit mapping, or leave dynamic for all subdomains.

This way you don‚Äôt need to manually update the config for every new backend API.

‚∏ª

If you want, I can provide a ready-to-use Helm template for swagger-proxy with wildcard Nginx + wildcard ingress that automatically supports any number of APIs dynamically.

Do you want me to do that?


=====
Sure! Let‚Äôs make a simple local Docker test on Windows for one backend API with Swagger UI and Nginx proxy. This will mimic your Cluster B ‚Üí Cluster A proxy scenario.

‚∏ª

1Ô∏è‚É£ Folder Structure

C:\swagger-proxy-test\
‚îú‚îÄ nginx.conf


‚∏ª

2Ô∏è‚É£ nginx.conf (Local Test)

events {}

http {
    # Swagger UI proxy
    server {
        listen 8080;
        server_name localhost;

        location / {
            proxy_pass https://swagger.clusterA.com;   # Replace with real clusterA swagger URL
            proxy_ssl_server_name on;

            proxy_set_header Host swagger.clusterA.com;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Rewrite backend URL in Swagger JSON (optional)
        sub_filter 'https://api-actor.clusterA.com' 'http://localhost:8080/actor';
        sub_filter_once off;
    }

    # Backend API proxy
    server {
        listen 8081;
        server_name localhost;

        location /actor/ {
            proxy_pass https://api-actor.clusterA.com/;   # Replace with real backend
            proxy_ssl_server_name on;

            proxy_set_header Host api-actor.clusterA.com;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}


‚∏ª

3Ô∏è‚É£ Run Nginx Container on Windows

Open PowerShell in C:\swagger-proxy-test and run:

docker run --rm -d --name swagger-proxy -p 8080:8080 -p 8081:8081 `
  -v ${PWD}/nginx.conf:/opt/bitnami/nginx/conf/nginx.conf `
  bitnami/nginx:1.25.3

	‚Ä¢	8080 ‚Üí Swagger UI
	‚Ä¢	8081 ‚Üí Backend API proxy /actor/...

‚∏ª

4Ô∏è‚É£ Test the Proxy
	‚Ä¢	Open Swagger UI in browser:

http://localhost:8080

	‚Ä¢	API requests go through Nginx proxy:

http://localhost:8081/actor/getAllActors

	‚Ä¢	Browser still sees localhost:8080 for Swagger, localhost:8081 for backend API, while Nginx forwards requests to Cluster A.

‚∏ª

‚úÖ This is a minimal working local test for 1 backend API.

You can later expand /actor/ to /api/actor/ or add more backend services.

‚∏ª

If you want, I can provide a single-port version so both Swagger UI and backend API calls go through just localhost:8080 like it would on Cluster B, making it even closer to your final Kubernetes deployment.

Do you want me to do that?



====

Good catch üëç ‚Äî let me explain why I showed 8.8.8.8 there and when you should not use it.

‚∏ª

Why I showed 8.8.8.8
	‚Ä¢	8.8.8.8 is Google‚Äôs public DNS.
	‚Ä¢	I used it in the example because it works anywhere, including your local Windows Docker test.
	‚Ä¢	That way, if you run the Nginx container on your laptop, it can resolve swagger.clusterA.com without needing Kubernetes‚Äô internal DNS.

‚∏ª

Why you should not use it in Kubernetes

Inside a Kubernetes cluster:
	‚Ä¢	Services & hostnames are resolved by CoreDNS (kube-dns), not by Google.
	‚Ä¢	So if you use 8.8.8.8, it might fail for internal names (*.svc.cluster.local) or be slower.
	‚Ä¢	The right resolver is the Cluster DNS service IP, typically 10.96.0.10 (but check with:

kubectl get svc -n kube-system kube-dns

or

kubectl get svc -n kube-system coredns



‚∏ª

‚úÖ Correct approach
	‚Ä¢	Local Docker test (Windows/Mac/Linux) ‚Üí use resolver 8.8.8.8; (or your system DNS).
	‚Ä¢	Cluster B Helm deployment ‚Üí use resolver <kube-dns-service-ip>;

So your Helm values.yaml should allow overriding, for example:

resolver: "10.96.0.10"   # default for in-cluster

and in nginx.conf template:

resolver {{ .Values.resolver }};


‚∏ª

üëâ This way, the same Helm chart works for both:
	‚Ä¢	Local laptop test (set resolver=8.8.8.8)
	‚Ä¢	Kubernetes deployment (set resolver=10.96.0.10)

‚∏ª

Do you want me to extend the Helm chart configmap.yaml with a resolver parameter so you can swap it between local and kube easily?



Perfect üëç let‚Äôs make it ready-to-deploy on Cluster B with Helm (you can later templatize values).

‚∏ª

üîπ 1. ConfigMap with Nginx Proxy Rules

This contains the nginx.conf we discussed:

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx-conf
  namespace: default
data:
  nginx.conf: |
    events {}

    http {
      server {
        listen 80;

        # Any subdomain of clusterB.com
        server_name ~^(?<subdomain>.+)\.clusterB\.com$;

        location / {
          proxy_pass https://$subdomain.clusterA.com;

          proxy_set_header Host $subdomain.clusterA.com;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          proxy_connect_timeout 60s;
          proxy_read_timeout 300s;
        }
      }
    }


‚∏ª

üîπ 2. Deployment (Bitnami Nginx with custom config)

This mounts the above ConfigMap into the container so Nginx uses it.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
  namespace: default
spec:
  replicas: 2
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
        - name: nginx
          image: bitnami/nginx:1.25.3   # replace with your Artifactory image if needed
          ports:
            - containerPort: 80
          volumeMounts:
            - name: nginx-conf
              mountPath: /opt/bitnami/nginx/conf/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: nginx-conf
          configMap:
            name: swagger-proxy-nginx-conf


‚∏ª

üîπ 3. Service (ClusterIP / LoadBalancer)

Expose Nginx inside Cluster B. If you use Ingress, just make it ClusterIP.

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy-svc
  namespace: default
spec:
  type: ClusterIP
  selector:
    app: swagger-proxy
  ports:
    - port: 80
      targetPort: 80


‚∏ª

üîπ 4. Ingress (Expose externally as *.clusterB.com)

Assuming you have an Ingress Controller in Cluster B:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  rules:
    - host: swagger.clusterB.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: 80
    - host: api-actor.clusterB.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy-svc
                port:
                  number: 80
  tls:
    - hosts:
        - swagger.clusterB.com
        - api-actor.clusterB.com
      secretName: clusterb-tls


‚∏ª

üîπ Flow
	‚Ä¢	User ‚Üí https://swagger.clusterB.com
	‚Ä¢	Ingress ‚Üí Service ‚Üí Nginx Pod
	‚Ä¢	Nginx proxies ‚Üí https://swagger.clusterA.com
	‚Ä¢	User ‚Üí https://api-actor.clusterB.com
	‚Ä¢	Ingress ‚Üí Service ‚Üí Nginx Pod
	‚Ä¢	Nginx proxies ‚Üí https://api-actor.clusterA.com

Browser always stays on *.clusterB.com.

‚∏ª

üëâ Do you want me to also show you the Helm chart folder structure (values.yaml, templates/ etc.) so you can package and push this proxy setup to Artifactory and later deploy via ArgoCD?
####


Great üöÄ ‚Äî let‚Äôs build a Helm chart for your generic Swagger proxy that runs on Cluster B.

This chart will:
	‚Ä¢	Deploy Nginx with a custom config (from a ConfigMap).
	‚Ä¢	Proxy swagger.clusterB.com ‚Üí swagger.clusterA.com.
	‚Ä¢	Proxy any API service via /api/{service}/... ‚Üí {service}.clusterA.com/....
	‚Ä¢	Expose via an Ingress so users can reach it.

‚∏ª

üìÇ Helm Chart Structure

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml


‚∏ª

üìå Chart.yaml

apiVersion: v2
name: swagger-proxy
description: A generic Nginx proxy for Swagger and APIs
type: application
version: 0.1.0
appVersion: "1.0"


‚∏ª

üìå values.yaml

image:
  repository: bitnami/nginx
  tag: 1.25.5
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  hosts:
    - host: swagger.clusterB.com
      paths:
        - path: /
          pathType: Prefix
  tls: []  # Add cert-manager TLS if needed


‚∏ª

üìå templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-nginx-conf
data:
  nginx.conf: |
    worker_processes 1;
    events { worker_connections 1024; }

    http {
      include       mime.types;
      default_type  application/octet-stream;

      sendfile        on;
      keepalive_timeout  65;

      server {
        listen 8080;
        server_name swagger.clusterB.com;

        # Proxy Swagger UI
        location / {
          proxy_pass https://swagger.clusterA.com;
          proxy_ssl_server_name on;

          proxy_set_header Host swagger.clusterA.com;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Generic API proxy
        # /api/api-actor/v1/... ‚Üí https://api-actor.clusterA.com/v1/...
        location /api/ {
          resolver 8.8.8.8;   # Or kube-dns if in-cluster
          set $api_host "";
          if ($uri ~ ^/api/([^/]+)(/.*)$) {
            set $api_host $1.clusterA.com;
            set $api_path $2;
          }

          proxy_pass https://$api_host$api_path;
          proxy_ssl_server_name on;

          proxy_set_header Host $api_host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }
      }
    }


‚∏ª

üìå templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
        - name: nginx
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          volumeMounts:
            - name: nginx-conf
              mountPath: /opt/bitnami/nginx/conf/nginx.conf
              subPath: nginx.conf
          ports:
            - containerPort: 8080
      volumes:
        - name: nginx-conf
          configMap:
            name: swagger-proxy-nginx-conf
            items:
              - key: nginx.conf
                path: nginx.conf


‚∏ª

üìå templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  type: {{ .Values.service.type }}
  selector:
    app: swagger-proxy
  ports:
    - protocol: TCP
      port: {{ .Values.service.port }}
      targetPort: 8080


‚∏ª

üìå templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy
  annotations:
    kubernetes.io/ingress.class: {{ .Values.ingress.className }}
spec:
  rules:
  {{- range .Values.ingress.hosts }}
    - host: {{ .host }}
      http:
        paths:
        {{- range .paths }}
          - path: {{ .path }}
            pathType: {{ .pathType }}
            backend:
              service:
                name: swagger-proxy
                port:
                  number: {{ $.Values.service.port }}
        {{- end }}
  {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
  {{- toYaml .Values.ingress.tls | nindent 4 }}
  {{- end }}
{{- end }}


‚∏ª

üèÉ How it works
	1.	Users open https://swagger.clusterB.com.
‚Üí Nginx proxies to swagger.clusterA.com.
	2.	Swagger calls /api/api-actor/v1/... (Cluster B endpoint).
‚Üí Nginx rewrites it to https://api-actor.clusterA.com/v1/....
	3.	Works for any service (api-actor, api-user, etc.) without writing separate configs.

‚∏ª

üëâ Do you want me to also give you a Bamboo build + Helm package script that:
	‚Ä¢	Packages this Helm chart
	‚Ä¢	Pushes it to Artifactory (as swagger-proxy-0.1.0.tgz)
	‚Ä¢	Updates ArgoCD via paramArgoCDUpdater?

mm

Got it ‚úÖ thanks for clarifying your exact need.
Let me restate in plain words so we don‚Äôt miss the logic:
	‚Ä¢	Cluster A has:
	‚Ä¢	https://swagger.clusterA.com (Swagger UI)
	‚Ä¢	https://api-actor.clusterA.com (backend APIs)
	‚Ä¢	‚Ä¶ other backend APIs.
	‚Ä¢	Some users cannot reach Cluster A, but they can reach Cluster B.
	‚Ä¢	You want to deploy a proxy in Cluster B so that users can use:
	‚Ä¢	https://swagger.clusterB.com ‚Üí proxy ‚Üí https://swagger.clusterA.com
	‚Ä¢	https://api-actor.clusterB.com ‚Üí proxy ‚Üí https://api-actor.clusterA.com

‚ö° So Cluster B acts as a mirror/proxy for Cluster A.

‚∏ª

üìÇ Helm Chart Structure (swagger-proxy/)

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îî‚îÄ‚îÄ templates/
    ‚îú‚îÄ‚îÄ configmap.yaml
    ‚îú‚îÄ‚îÄ deployment.yaml
    ‚îú‚îÄ‚îÄ service.yaml
    ‚îî‚îÄ‚îÄ ingress.yaml


‚∏ª

Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Proxy from Cluster B ‚Üí Cluster A (swagger + APIs)
version: 0.1.0
appVersion: "1.0"


‚∏ª

values.yaml

replicaCount: 1

image:
  repository: bitnami/nginx
  tag: latest
  pullPolicy: IfNotPresent

# Map of proxy hosts (ClusterB ‚Üí ClusterA)
proxy:
  mappings:
    - sourceHost: swagger.clusterB.com
      targetHost: swagger.clusterA.com
    - sourceHost: api-actor.clusterB.com
      targetHost: api-actor.clusterA.com

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: true
  className: nginx
  annotations: {}
  hosts:
    - swagger.clusterB.com
    - api-actor.clusterB.com
  tls: []   # add tls secrets here if you terminate SSL in clusterB ingress


‚∏ª

templates/configmap.yaml

This generates nginx.conf dynamically with multiple server {} blocks ‚Äî one per mapping.

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "swagger-proxy.fullname" . }}-nginx
data:
  nginx.conf: |
    worker_processes 1;

    events { worker_connections 1024; }

    http {
      include       mime.types;
      default_type  application/octet-stream;

      sendfile        on;
      keepalive_timeout  65;

      {{- range .Values.proxy.mappings }}
      server {
        listen 8080;
        server_name {{ .sourceHost }};

        location / {
          proxy_pass https://{{ .targetHost }};
          proxy_ssl_server_name on;

          proxy_set_header Host {{ .targetHost }};
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;

          # Optional if target cluster has self-signed certs
          # proxy_ssl_verify off;
        }
      }
      {{- end }}
    }


‚∏ª

templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "swagger-proxy.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "swagger-proxy.name" . }}
    spec:
      containers:
        - name: nginx
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 8080
          volumeMounts:
            - name: nginx-config
              mountPath: /opt/bitnami/nginx/conf/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: nginx-config
          configMap:
            name: {{ include "swagger-proxy.fullname" . }}-nginx


‚∏ª

templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  type: {{ .Values.service.type }}
  ports:
    - port: {{ .Values.service.port }}
      targetPort: 8080
  selector:
    app: {{ include "swagger-proxy.name" . }}


‚∏ª

templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
  annotations:
    {{- with .Values.ingress.annotations }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  ingressClassName: {{ .Values.ingress.className }}
  rules:
    {{- range .Values.ingress.hosts }}
    - host: {{ . }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: {{ include "swagger-proxy.fullname" $ }}
                port:
                  number: {{ $.Values.service.port }}
    {{- end }}
  {{- if .Values.ingress.tls }}
  tls:
    {{- toYaml .Values.ingress.tls | nindent 4 }}
  {{- end }}
{{- end }}


‚∏ª

üîó How it works
	‚Ä¢	You list ClusterB ‚Üí ClusterA host mappings in values.yaml.
	‚Ä¢	Chart generates nginx.conf with one proxy server block per mapping.
	‚Ä¢	Users hit swagger.clusterB.com ‚Üí Ingress in ClusterB ‚Üí Nginx proxy ‚Üí swagger.clusterA.com.
	‚Ä¢	Same for api-actor.clusterB.com ‚Üí proxy ‚Üí api-actor.clusterA.com.

‚∏ª

‚ö° This chart is generic ‚Äî you can add any number of services into values.yaml.mappings and redeploy.

üëâ Do you also want me to include a Bamboo build script that:
	1.	packages this chart,
	2.	pushes to Artifactory (e.g., swagger-ui-charts/0.1.0/),
	3.	and triggers ArgoCD sync automatically?


=====


Yes üëç that‚Äôs exactly the reason it didn‚Äôt work.
Your config snippet is incomplete compared to what Nginx needs to actually boot with your proxy.

Here‚Äôs why:

‚∏ª

üîπ Problems in your nginx.conf
	1.	You overwrote the default Nginx config with only:

events {}
http {
  server {
    listen 80;
    location / { proxy_pass ... }
  }
}

	‚Ä¢	Missing mime.types, default_type, worker_processes.
	‚Ä¢	Bitnami‚Äôs base image expects a full config.

	2.	You didn‚Äôt enable SSL options (proxy_ssl_server_name on;) ‚Üí needed when proxying to https://swagger-clusterA.com.
	3.	On Windows Docker, you exposed port 8080 but your config was listening on 80. That mismatch means:
	‚Ä¢	You hit localhost:8080
	‚Ä¢	But Nginx inside was only listening on 80 ‚Üí request never reached your proxy block.

‚∏ª

üîπ Fixed minimal working nginx.conf

Try this instead:

worker_processes 1;

events {
    worker_connections 1024;
}

http {
    include       mime.types;
    default_type  application/octet-stream;

    sendfile        on;
    keepalive_timeout  65;

    server {
        listen 8080;   # match docker -p 8080:8080

        location / {
            proxy_pass https://swagger-clusterA.com;
            proxy_ssl_server_name on;

            proxy_set_header Host swagger-clusterA.com;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # Optional if swagger-clusterA has self-signed cert
            # proxy_ssl_verify off;
        }
    }
}


‚∏ª

üîπ Run on Windows Docker

docker run --rm -d -p 8080:8080 `
  -v ${PWD}\nginx.conf:/opt/bitnami/nginx/conf/nginx.conf:ro `
  --name swagger-proxy bitnami/nginx:latest

Then open:

http://localhost:8080/

It should forward to:

https://swagger-clusterA.com/

Check logs if unsure:

docker logs swagger-proxy


‚∏ª

‚úÖ This is now a fully working testable proxy.
Once confirmed locally, you can directly reuse this config in Kubernetes via a ConfigMap.

‚∏ª

Do you want me to show you how to reuse the exact same nginx.conf for both:
	1.	Local Docker testing
	2.	Kubernetes Helm deployment (ConfigMap mount)
so you don‚Äôt maintain 2 different versions?


=====
Yes ‚úÖ you can absolutely test this swagger-proxy locally on Windows Docker before deploying to Kubernetes.

Let me walk you through step by step:

‚∏ª

üîπ 1. Write your nginx.conf

Create a file nginx.conf on your Windows machine:

events {}
http {
  server {
    listen 80;

    location / {
      proxy_pass https://swagger-clusterA.com;
      proxy_set_header Host swagger-clusterA.com;
      proxy_set_header X-Real-IP $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header X-Forwarded-Proto $scheme;
    }
  }
}

üëâ Replace swagger-clusterA.com with the real domain you want to forward to.

‚∏ª

üîπ 2. Run Nginx Docker container on Windows

If you‚Äôre using official nginx image:

docker run --rm -d -p 8080:80 `
  -v ${PWD}\nginx.conf:/etc/nginx/nginx.conf:ro `
  --name swagger-proxy nginx:1.25

	‚Ä¢	-p 8080:80 ‚Üí exposes Nginx on localhost:8080
	‚Ä¢	-v ${PWD}\nginx.conf:/etc/nginx/nginx.conf:ro ‚Üí mounts your config file into the container
	‚Ä¢	nginx:1.25 ‚Üí official nginx image

If you want to use bitnami/nginx:

docker run --rm -d -p 8080:8080 `
  -v ${PWD}\nginx.conf:/opt/bitnami/nginx/conf/nginx.conf:ro `
  --name swagger-proxy bitnami/nginx:latest


‚∏ª

üîπ 3. Test it

Open browser or use curl:

curl http://localhost:8080/swagger-ui/

üëâ This should forward to https://swagger-clusterA.com/swagger-ui/ transparently.

‚∏ª

üîπ 4. Debugging
	‚Ä¢	If you see errors ‚Üí check logs with:

docker logs swagger-proxy


	‚Ä¢	If config reload fails ‚Üí Nginx will show error on container startup.

‚∏ª

‚úÖ This way, you can fully test the proxy logic on your local Windows Docker environment before deploying the same config to Kubernetes.

‚∏ª

Would you like me to also write a Dockerfile so you can package this proxy (with nginx.conf baked in) and push it to your internal Artifactory, instead of mounting the config each time? That way your K8s deployment just pulls a ready-made image.


====
Yes üëç you can always see the final YAMLs that Helm generates before/after packaging and deploying.

Here‚Äôs how:

‚∏ª

üîπ 1. Check templates without packaging

If you have a Helm chart folder (say swagger-proxy/), run:

helm template swagger-proxy ./swagger-proxy -f values.yaml

üëâ This renders all templates into plain Kubernetes YAMLs, substituting values from values.yaml.
It does not install, just prints to stdout.

If you want to save them:

helm template swagger-proxy ./swagger-proxy -f values.yaml > rendered.yaml


‚∏ª

üîπ 2. Check packaged chart

If you already ran:

helm package ./swagger-proxy

You‚Äôll get something like:

swagger-proxy-0.1.0.tgz

You can still inspect it without deploying:

helm template swagger-proxy swagger-proxy-0.1.0.tgz -f values.yaml


‚∏ª

üîπ 3. After deployment ‚Äî inspect applied YAMLs

If it‚Äôs already installed in Kubernetes:

helm get manifest swagger-proxy

üëâ This shows the exact YAMLs Helm applied to the cluster.
Useful for debugging when what you expected ‚â† what‚Äôs running.

‚∏ª

üîπ 4. Alternative: unpack .tgz chart

Since a Helm chart is just a tarball:

tar -xvzf swagger-proxy-0.1.0.tgz

This gives you back the /templates, values.yaml, and Chart.yaml.

‚∏ª

‚úÖ So yes ‚Äî you can fully unpack and inspect all YAMLs either:
	‚Ä¢	before deploying (helm template)
	‚Ä¢	after packaging (helm template .tgz)
	‚Ä¢	after deployment (helm get manifest)

‚∏ª

Do you want me to show you a Bamboo build step that does helm template into an artifact (rendered.yaml) so you can always see exactly what will be applied, before ArgoCD picks it up?


===
Nice üöÄ you want the end-to-end flow:
	1.	Helm chart for Nginx proxy (with multiple Swagger service mappings).
	2.	Bamboo build script to:
	‚Ä¢	Package the Helm chart.
	‚Ä¢	Push Docker image + Helm chart to Artifactory.
	‚Ä¢	Trigger ArgoCD to sync.
	3.	Use paramArgoCDUpdater (parametrized deployment).

Let‚Äôs break it step by step üëá

‚∏ª

1Ô∏è‚É£ Helm Chart Structure

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ   ‚îî‚îÄ‚îÄ configmap.yaml


‚∏ª

Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Reverse proxy for Swagger APIs across clusters
type: application
version: 0.1.0
appVersion: "1.25.3"


‚∏ª

values.yaml

image:
  repository: artifactory.company.com/docker/nginx
  tag: 1.25.3
  pullPolicy: IfNotPresent

ingress:
  enabled: true
  host: swagger-clusterB.com

proxies:
  - path: /service1/
    target: https://api-service1.clusterA.com/
    hostHeader: api-service1.clusterA.com

  - path: /service2/
    target: https://api-service2.clusterA.com/
    hostHeader: api-service2.clusterA.com


‚∏ª

templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
        - name: nginx
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          volumeMounts:
            - name: config
              mountPath: /etc/nginx/nginx.conf
              subPath: nginx.conf
      volumes:
        - name: config
          configMap:
            name: swagger-proxy-config


‚∏ª

templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  type: ClusterIP
  ports:
    - port: 8080
      targetPort: 8080
  selector:
    app: swagger-proxy


‚∏ª

templates/ingress.yaml

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
spec:
  rules:
    - host: {{ .Values.ingress.host }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy
                port:
                  number: 8080


‚∏ª

templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 8080;
        {{- range .Values.proxies }}
        location {{ .path }} {
          proxy_pass {{ .target }};
          proxy_set_header Host {{ .hostHeader }};
        }
        {{- end }}
      }
    }


‚∏ª

2Ô∏è‚É£ Bamboo Build Script (bamboo-specs.yaml or inline script)

Example task script in your Bamboo plan:

#!/bin/bash
set -e

CHART_NAME=swagger-proxy
CHART_VERSION=0.1.$(date +%Y%m%d%H%M)
HELM_REPO_URL=https://artifactory.company.com/helm
DOCKER_IMAGE=artifactory.company.com/docker/nginx:1.25.3

echo ">>> Packaging Helm Chart"
helm package ./swagger-proxy --version $CHART_VERSION

echo ">>> Pushing Helm Chart to Artifactory"
curl -u $ARTIFACTORY_USER:$ARTIFACTORY_PASSWORD \
     -T ${CHART_NAME}-${CHART_VERSION}.tgz \
     "$HELM_REPO_URL/${CHART_NAME}-${CHART_VERSION}.tgz"

echo ">>> Updating ArgoCD Deployment"
argocd app set swagger-proxy \
  --values values.yaml \
  --parameter image.tag=$CHART_VERSION \
  --grpc-web


‚∏ª

3Ô∏è‚É£ ArgoCD Deployment with paramArgoCDUpdater

In ArgoCD, you‚Äôll have an app like:

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: swagger-proxy
  namespace: argocd
spec:
  source:
    repoURL: https://artifactory.company.com/helm
    chart: swagger-proxy
    targetRevision: 0.1.*
    helm:
      valueFiles:
        - values.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: swagger
  syncPolicy:
    automated:
      prune: true
      selfHeal: true


‚∏ª

‚úÖ Flow:
	1.	Bamboo builds chart + uploads to Artifactory.
	2.	Bamboo triggers ArgoCD CLI (argocd app set) or paramArgoCDUpdater plugin to update the chart version / values.
	3.	ArgoCD pulls Helm chart from Artifactory ‚Üí deploys to Cluster B.

‚∏ª

Would you like me to give you a ready-to-use Bamboo bamboo-specs.yaml (YAML DSL) so you don‚Äôt need to inline scripts, or do you prefer the bash script task approach?


=#=#=#
Perfect üëç let‚Äôs make this generic with Helm so you don‚Äôt have to hardcode every service proxy.

We‚Äôll build a small Helm chart called swagger-proxy.

‚∏ª

üìÇ Directory Structure

swagger-proxy/
  Chart.yaml
  values.yaml
  templates/
    configmap.yaml
    deployment.yaml
    service.yaml
    ingress.yaml


‚∏ª

üìù Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Proxy Swagger UI + API services from Cluster A into Cluster B
version: 0.1.0
appVersion: "1.0"


‚∏ª

üìù values.yaml

Here you configure all the endpoints to proxy:

replicaCount: 1

image:
  repository: nginx
  tag: alpine

ingress:
  enabled: true
  host: swagger-clusterB.com

# Proxy rules
# Each entry maps a path on Cluster B ‚Üí a target in Cluster A
proxies:
  - path: /
    target: https://swagger-clusterA.com/
    hostHeader: swagger-clusterA.com

  - path: /service1/
    target: https://api-service1.clusterA.com/
    hostHeader: api-service1.clusterA.com

  - path: /service2/
    target: https://api-service2.clusterA.com/
    hostHeader: api-service2.clusterA.com


‚∏ª

üìù templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 8080;

        {{- range .Values.proxies }}
        location {{ .path }} {
          proxy_pass {{ .target }};
          proxy_set_header Host {{ .hostHeader }};
        }
        {{- end }}
      }
    }


‚∏ª

üìù templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
      - name: nginx
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: swagger-proxy-config


‚∏ª

üìù templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  selector:
    app: swagger-proxy
  ports:
    - port: 8080
      targetPort: 8080
  type: ClusterIP


‚∏ª

üìù templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
spec:
  rules:
    - host: {{ .Values.ingress.host }}
      http:
        paths:
        {{- range .Values.proxies }}
          - path: {{ .path }}
            pathType: Prefix
            backend:
              service:
                name: swagger-proxy
                port:
                  number: 8080
        {{- end }}
{{- end }}


‚∏ª

üöÄ Deploy

helm install swagger-proxy ./swagger-proxy


‚∏ª

üîπ How it Works
	‚Ä¢	Users go to https://swagger-clusterB.com
	‚Ä¢	Requests to / ‚Üí proxied to https://swagger-clusterA.com
	‚Ä¢	Requests to /service1/ ‚Üí proxied to https://api-service1.clusterA.com
	‚Ä¢	Requests to /service2/ ‚Üí proxied to https://api-service2.clusterA.com
	‚Ä¢	You can add/remove services just by editing values.yaml.

‚∏ª

Do you also want me to show how to make the proxy auto-rewrite the Swagger UI URLs (so that instead of api-service1.clusterA.com showing up in the docs, it automatically rewrites to swagger-clusterB.com/service1/)? That way users never even see the Cluster A domains.


======
Perfect üëç you want a Helmized proxy deployment in Cluster B that can forward to a list of Swagger services in Cluster A.

Here‚Äôs a clean Helm chart structure + YAML configs that you can adapt.

‚∏ª

üîπ Directory Structure

swagger-proxy/
‚îú‚îÄ‚îÄ Chart.yaml
‚îú‚îÄ‚îÄ values.yaml
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ   ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ   ‚îú‚îÄ‚îÄ service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ ingress.yaml


‚∏ª

üîπ Chart.yaml

apiVersion: v2
name: swagger-proxy
description: Reverse proxy for Swagger services in Cluster A
version: 0.1.0
appVersion: "1.0"


‚∏ª

üîπ values.yaml

Here you define the list of services in Cluster A you want to proxy.

replicaCount: 1

image:
  repository: nginx
  tag: alpine
  pullPolicy: IfNotPresent

service:
  type: ClusterIP
  port: 8080

ingress:
  enabled: true
  className: ""
  hosts:
    - host: swagger-proxy.clusterB.com
      paths:
        - path: /
          pathType: Prefix
          backendPort: 8080
  tls: []   # e.g., configure if you have certs

# List of Swagger services in Cluster A
services:
  - name: service1
    url: https://service1.clusterA.com/swagger/
  - name: service2
    url: https://service2.clusterA.com/swagger/
  - name: service3
    url: https://service3.clusterA.com/swagger/


‚∏ª

üîπ templates/configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "swagger-proxy.fullname" . }}-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen {{ .Values.service.port }};
        {{- range .Values.services }}
        location /{{ .name }}/ {
          proxy_pass {{ .url }};
          proxy_set_header Host {{ regexReplaceAll "https?://" .url "" }};
        }
        {{- end }}
      }
    }


‚∏ª

üîπ templates/deployment.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "swagger-proxy.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "swagger-proxy.name" . }}
    spec:
      containers:
      - name: nginx
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - containerPort: {{ .Values.service.port }}
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: {{ include "swagger-proxy.fullname" . }}-config


‚∏ª

üîπ templates/service.yaml

apiVersion: v1
kind: Service
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  type: {{ .Values.service.type }}
  selector:
    app: {{ include "swagger-proxy.name" . }}
  ports:
  - port: {{ .Values.service.port }}
    targetPort: {{ .Values.service.port }}


‚∏ª

üîπ templates/ingress.yaml

{{- if .Values.ingress.enabled }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: {{ include "swagger-proxy.fullname" . }}
spec:
  rules:
  {{- range .Values.ingress.hosts }}
  - host: {{ .host }}
    http:
      paths:
      {{- range .paths }}
      - path: {{ .path }}
        pathType: {{ .pathType }}
        backend:
          service:
            name: {{ include "swagger-proxy.fullname" $ }}
            port:
              number: {{ .backendPort }}
      {{- end }}
  {{- end }}
{{- end }}


‚∏ª

üîπ How It Works
	1.	Add your Cluster A Swagger services in values.yaml under .services.

services:
  - name: service1
    url: https://service1.clusterA.com/swagger/
  - name: service2
    url: https://service2.clusterA.com/swagger/


	2.	Deploy Helm chart in Cluster B:

helm install swagger-proxy ./swagger-proxy


	3.	Access endpoints from Cluster B users:

https://swagger-proxy.clusterB.com/service1/
https://swagger-proxy.clusterB.com/service2/



‚∏ª

‚úÖ This setup needs no changes in Cluster A (since it‚Äôs already exposed via Ingress).
‚úÖ Users who only have access to Cluster B can now consume Cluster A Swagger via the proxy.

‚∏ª

Do you want me to also include basic auth / IP restriction in this proxy Helm chart so only selected users can use it?

Perfect, now it‚Äôs very clear üëç
	‚Ä¢	Cluster A Swagger services are already exposed via Ingress (https://service1.clusterA.com/...).
	‚Ä¢	Some users cannot access them directly (due to zone / firewall restrictions).
	‚Ä¢	But those same users can access Cluster B.
	‚Ä¢	‚úÖ So we just need to give them a proxy endpoint inside Cluster B, which forwards their requests to Cluster A.

‚∏ª

üîπ Solution: Proxy in Cluster B for Cluster A‚Äôs Ingress

Step 1. Nginx Config

We‚Äôll create an Nginx reverse proxy in Cluster B.
Example for 2 services (service1 and service2):

events {}
http {
  server {
    listen 8080;

    location /service1/ {
      proxy_pass https://service1.clusterA.com/;
    }

    location /service2/ {
      proxy_pass https://service2.clusterA.com/;
    }
  }
}


‚∏ª

Step 2. Kubernetes YAML (in Cluster B)

apiVersion: v1
kind: ConfigMap
metadata:
  name: swagger-proxy-config
data:
  nginx.conf: |
    events {}
    http {
      server {
        listen 8080;

        location /service1/ {
          proxy_pass https://service1.clusterA.com/;
          proxy_set_header Host service1.clusterA.com;
        }

        location /service2/ {
          proxy_pass https://service2.clusterA.com/;
          proxy_set_header Host service2.clusterA.com;
        }
      }
    }
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: swagger-proxy
spec:
  replicas: 1
  selector:
    matchLabels:
      app: swagger-proxy
  template:
    metadata:
      labels:
        app: swagger-proxy
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: config
          mountPath: /etc/nginx/nginx.conf
          subPath: nginx.conf
      volumes:
      - name: config
        configMap:
          name: swagger-proxy-config
---
apiVersion: v1
kind: Service
metadata:
  name: swagger-proxy
spec:
  selector:
    app: swagger-proxy
  ports:
  - port: 8080
    targetPort: 8080
  type: ClusterIP


‚∏ª

Step 3. Optional: Expose in Cluster B

If users need to access it from outside Cluster B, expose it with an Ingress or a LoadBalancer Service:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: swagger-proxy-ingress
spec:
  rules:
  - host: swagger-proxy.clusterB.com
    http:
      paths:
      - path: /service1/
        pathType: Prefix
        backend:
          service:
            name: swagger-proxy
            port:
              number: 8080
      - path: /service2/
        pathType: Prefix
        backend:
          service:
            name: swagger-proxy
            port:
              number: 8080


‚∏ª

üîπ Final Access Flow
	‚Ä¢	User cannot access https://service1.clusterA.com directly.
	‚Ä¢	User goes to:

https://swagger-proxy.clusterB.com/service1/


	‚Ä¢	Request goes ‚Üí Cluster B Nginx ‚Üí forwards to https://service1.clusterA.com/ in Cluster A.
	‚Ä¢	Response comes back via the proxy.

‚∏ª

‚úÖ No changes needed in Cluster A (since it‚Äôs already exposed by Ingress).
‚úÖ Only deploy a proxy Deployment + Service + Ingress in Cluster B.

‚∏ª

Do you want me to extend this YAML so you can proxy an arbitrary list of services (like 10+ Swagger endpoints) with just one config?