
Perfect ‚úÖ ‚Äî let‚Äôs make this one comprehensive, deploy-ready block that includes:
	1.	Example output report (CSV) ‚Üí what the script produces
	2.	Best Kubernetes rollout YAML strategy ‚Üí to safely roll out updates (especially memory increase)

‚∏ª

üßæ 1Ô∏è‚É£ Example Output CSV (capacity_check_report_20251018_131512.csv)

service	namespace	replicas	current_request_per_pod_mib	current_limit_per_pod_mib	current_total_alloc_mib	proposed_input	proposed_input_mib	recommended_new_limit_per_pod_mib	extra_needed_total_mib	proposed_safe	found_by
svc-a	default	3	512	1024	3072			1280	768	TRUE	deployment
svc-b	payments	2	1024	2048	4096	2Gi	2048	2048	0	TRUE	deployment
svc-crash	default	4	1024	1536	6144	2Gi	2048	2048	2048	FALSE ‚ö†Ô∏è	deployment
svc-c	api	2	256	512	1024			768	512	TRUE	deployment
svc-db	db	1	2048	4096	4096			4096	0	TRUE	deployment

üí° Legend:
	‚Ä¢	current_limit_per_pod_mib: current memory limit per pod
	‚Ä¢	recommended_new_limit_per_pod_mib: suggested safe new limit (MiB)
	‚Ä¢	extra_needed_total_mib: total cluster memory impact (across replicas)
	‚Ä¢	proposed_safe: whether the change fits within safe headroom
	‚Ä¢	FALSE = risky; do not roll out until cluster capacity increases

‚∏ª

üöÄ 2Ô∏è‚É£ Recommended Kubernetes Deployment YAML Snippet

Use this rollout strategy to ensure no downtime, safe memory rollout, and cluster stability
(even without node metrics, relying on requests/limits and ArgoCD sync):

apiVersion: apps/v1
kind: Deployment
metadata:
  name: svc-crash
  namespace: default
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0            # ensure no downtime (all replicas stay available)
      maxSurge: 1                  # add one extra pod during rollout
  revisionHistoryLimit: 5          # keep rollback history
  progressDeadlineSeconds: 600     # allow 10min rollout window
  minReadySeconds: 15              # wait 15s between pod readiness checks
  selector:
    matchLabels:
      app: svc-crash
  template:
    metadata:
      labels:
        app: svc-crash
    spec:
      containers:
        - name: svc-crash
          image: myrepo/svc-crash:v2
          ports:
            - containerPort: 8080
          resources:
            requests:
              cpu: "500m"
              memory: "1024Mi"
            limits:
              cpu: "1000m"
              memory: "2048Mi"     # updated limit based on capacity check
          readinessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 10
            periodSeconds: 5
          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: svc-crash


‚∏ª

üß† Why this YAML works best

Setting	Purpose
maxUnavailable: 0	No replicas are killed before new ones are ready ‚Äî ensures zero downtime.
maxSurge: 1	Allows adding 1 extra pod during rollout ‚Äî safe for slight capacity bumps.
progressDeadlineSeconds	Ensures rollout doesn‚Äôt hang forever if capacity runs short.
minReadySeconds	Prevents marking pods ready too early.
topologySpreadConstraints	Ensures pods distribute evenly across nodes to avoid one node OOM.
readinessProbe / livenessProbe	Protects from routing traffic to unhealthy pods.


‚∏ª

‚úÖ Deployment Workflow with ArgoCD + This Script
	1.	Developer proposes new memory limits in manifests.
	2.	CI pipeline runs the Python script ‚Üí validates capacity from cluster via kubectl.
	3.	If safe ‚úÖ ‚Üí ArgoCD sync proceeds.
If unsafe ‚ùå ‚Üí fail PR / block sync, and attach CSV report.
	4.	ArgoCD applies deployment ‚Üí RollingUpdate happens safely with maxSurge=1.
	5.	Monitor rollout status using:

kubectl rollout status deploy/svc-crash -n default



‚∏ª

Would you like me to extend this so the Python script automatically generates this YAML patch (e.g., for services flagged as ‚Äúsafe‚Äù)?
That way, you can feed it directly to ArgoCD.


=====

Nice ‚Äî below is a single, self-contained automation script you can run locally that:
	‚Ä¢	Reads a CSV of your 15 services (name + namespace + optional proposed memory limit per pod),
	‚Ä¢	Uses kubectl to read cluster node allocatable capacity and current Deployments (replicas + container requests/limits),
	‚Ä¢	Normalizes mixed memory units (Mi/Gi/MB/GB),
	‚Ä¢	Calculates current cluster requested memory, free headroom, and a safe recommended new limit per service (respecting safety margin and not exceeding a configurable multiplier),
	‚Ä¢	Writes a human-readable Excel/CSV report and returns non-zero (fail) if any requested/proposed change would exceed safe capacity (so you can plug it into CI / block an ArgoCD sync).

Everything here is designed to work when you only have kubectl access for node capacity and pod/deployment specs.

‚∏ª

What it does (high level)
	1.	Load services.csv listing: service,namespace,proposed_limit (proposed_limit optional, units allowed: Mi, Gi, MB, GB).
	2.	Query cluster node allocatable memory (via kubectl get nodes).
	3.	Query each Deployment to read number of replicas and container resources for the named service.
	4.	Compute total requested memory across cluster (requests √ó replicas).
	5.	Compute free memory = allocatable_total - requested_total.
	6.	For each service:
	‚Ä¢	If proposed_limit provided ‚Üí check if cluster can absorb it.
	‚Ä¢	If not provided ‚Üí compute a safe suggested limit (based on free headroom, safety margin, and multiplier cap).
	7.	Output capacity_check_report_<timestamp>.xlsx and capacity_check_report_<timestamp>.csv.
	8.	Exit code non-zero if any proposed/inferred changes are unsafe.

‚∏ª

Requirements
	‚Ä¢	Python 3.8+
	‚Ä¢	kubectl configured to target the cluster (kubeconfig/context) where you will apply changes.
	‚Ä¢	Python packages: pandas, openpyxl
Install:

pip install pandas openpyxl


‚∏ª

Sample input file (services.csv)

Create services.csv with header:

service,namespace,proposed_limit_per_pod
svc-a,default,
svc-b,payments,2Gi
svc-c,default,
svc-d,db, # usually stateful; include for reporting
svc-crash,default,
...

	‚Ä¢	proposed_limit_per_pod optional ‚Äî when provided the script will validate it; when empty the script recommends a safe limit.

‚∏ª

The Script ‚Äî pre_deploy_capacity_check.py

Save this file and run with python pre_deploy_capacity_check.py --services-file services.csv

#!/usr/bin/env python3
"""
pre_deploy_capacity_check.py

Usage:
  python pre_deploy_capacity_check.py --services-file services.csv [--safety-margin 0.9] [--max-mult 2.0]

This script:
 - Reads a CSV listing services (service, namespace, optional proposed_limit_per_pod)
 - Uses kubectl to query node allocatable memory and deployment specs for those services
 - Normalizes memory units to MiB, computes cluster requested memory, free headroom
 - Validates proposed limits (if provided) or recommends safe new limits
 - Outputs Excel and CSV report and exits non-zero if any proposed changes are NOT safe
"""
import subprocess, json, sys, argparse, re
import pandas as pd
from datetime import datetime
from math import floor

# -------------------------
# Unit conversion utilities
# -------------------------
def to_mib(v):
    if v is None: return 0.0
    s = str(v).strip()
    if s == "": return 0.0
    m = re.match(r"^([0-9.]+)\s*([a-zA-Z]*)$", s)
    if not m:
        try:
            return float(s)
        except:
            return 0.0
    num, unit = m.groups()
    num = float(num)
    unit = unit.lower()
    if unit in ("", "mi", "mib"): return num
    if unit in ("gi", "gib"): return num * 1024.0
    if unit == "mb": return num * 0.9537
    if unit == "gb": return num * 953.7
    if unit in ("k", "ki", "kb"): return num / 1024.0
    # kubernetes sometimes uses plain integer bytes? fallback:
    return num

def mib_to_human(mi):
    if mi >= 1024:
        return f"{mi/1024:.2f}Gi"
    return f"{mi:.0f}Mi"

# -------------------------
# kubectl helpers
# -------------------------
def kubectl_json(args):
    cmd = ["kubectl"] + args + ["-o", "json"]
    # run, capture output
    try:
        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT)
        return json.loads(out)
    except subprocess.CalledProcessError as e:
        print("ERROR calling kubectl:", " ".join(cmd))
        print(e.output.decode())
        raise

def get_nodes_allocatable():
    data = kubectl_json(["get", "nodes"])
    total_mi = 0.0
    nodes = []
    for n in data.get("items", []):
        name = n["metadata"]["name"]
        alloc = n.get("status", {}).get("allocatable", {})
        mem = alloc.get("memory")  # example "32761156Ki" or "31Gi"
        # kubernetes often returns memory in Ki
        # convert numeric Ki to Mi: if string ends with Ki -> divide by 1024
        if mem is None:
            mi = 0.0
        else:
            # handle Ki (e.g. "32761156Ki") or Gi / Mi
            m = re.match(r"^([0-9]+)Ki$", mem)
            if m:
                mi = float(m.group(1)) / 1024.0
            else:
                mi = to_mib(mem)
        nodes.append({"name": name, "alloc_mib": mi, "raw": mem})
        total_mi += mi
    return nodes, total_mi

def get_deployment_info(name, namespace):
    # returns dict: replicas, containers: list of {name, req_mem_mi, lim_mem_mi}
    try:
        data = kubectl_json(["get", "deploy", name, "-n", namespace])
    except Exception:
        # Try to get ReplicaSet or fallback to pods by label
        print(f"Warning: deployment {name} in {namespace} not found via kubectl get deploy. Attempting fallback get pods by name...")
        # fallback: get pods in namespace and try to match name prefix
        pods = kubectl_json(["get", "pods", "-n", namespace])
        matched = [p for p in pods.get("items", []) if p["metadata"]["name"].startswith(name)]
        if not matched:
            raise RuntimeError(f"Cannot find deployment or pods for {name} in {namespace}.")
        # combine pod spec container resources as representative
        pod = matched[0]
        containers = []
        for c in pod["spec"]["containers"]:
            r = c.get("resources", {}).get("requests", {}).get("memory")
            l = c.get("resources", {}).get("limits", {}).get("memory")
            containers.append({
                "name": c.get("name"),
                "req_mib": to_mib(r),
                "lim_mib": to_mib(l)
            })
        replicas = len(matched)
        return {"replicas": replicas, "containers": containers, "found_by": "pod-fallback"}
    spec = data.get("spec", {})
    replicas = spec.get("replicas") or 1
    template = spec.get("template", {}).get("spec", {})
    containers = []
    for c in template.get("containers", []):
        r = c.get("resources", {}).get("requests", {}).get("memory")
        l = c.get("resources", {}).get("limits", {}).get("memory")
        containers.append({
            "name": c.get("name"),
            "req_mib": to_mib(r),
            "lim_mib": to_mib(l)
        })
    return {"replicas": replicas, "containers": containers, "found_by": "deployment"}

# -------------------------
# Main logic
# -------------------------
def main(args):
    df = pd.read_csv(args.services_file, dtype=str).fillna("")
    # expected columns: service, namespace, proposed_limit_per_pod (optional)
    if not {"service","namespace"}.issubset(set(df.columns)):
        print("CSV must contain at least 'service' and 'namespace' columns.")
        sys.exit(2)

    # Query cluster nodes
    print("Querying cluster nodes (kubectl)...")
    nodes, total_cluster_mib = get_nodes_allocatable()
    print(f"Total cluster allocatable memory: {mib_to_human(total_cluster_mib)} ({total_cluster_mib:.0f} Mi)")

    # Gather current cluster requested memory by enumerating all deployments in all namespaces? We'll
    # compute requested sum only for the services present + all other deployments (best-effort).
    # For safety we should sum across all Deployments; do that:
    print("Querying all deployments to compute current requested memory (this may take a few seconds)...")
    all_deps = kubectl_json(["get", "deploy", "-A"])
    total_requested_mib = 0.0
    # iterate deployments
    for dep in all_deps.get("items", []):
        spec = dep.get("spec", {})
        replicas = spec.get("replicas") or 1
        templ = spec.get("template", {}).get("spec", {})
        for c in templ.get("containers", []):
            r = c.get("resources", {}).get("requests", {}).get("memory")
            reqmi = to_mib(r)
            total_requested_mib += reqmi * replicas

    free_mib = total_cluster_mib - total_requested_mib
    print(f"Total requested memory (all deployments): {mib_to_human(total_requested_mib)}")
    print(f"Free headroom (allocatable - requested): {mib_to_human(free_mib)}")

    # For per-service evaluation, gather current limits & requests for each service listed
    results = []
    unsafe_flag = False
    safety_margin = float(args.safety_margin)
    max_mult = float(args.max_mult)
    # We'll allocate safety buffer of free_mib * safety_margin for all suggestions.
    safe_free = max(0.0, free_mib * safety_margin)

    for idx, row in df.iterrows():
        svc = row["service"].strip()
        ns = row["namespace"].strip()
        proposed = row.get("proposed_limit_per_pod", "").strip()
        try:
            info = get_deployment_info(svc, ns)
        except Exception as e:
            print(f"ERROR: {e}")
            results.append({
                "service": svc, "namespace": ns,
                "error": str(e)
            })
            unsafe_flag = True
            continue

        replicas = int(info["replicas"] or 1)
        # choose the container with the highest limit as representative (or single container)
        if len(info["containers"]) == 0:
            print(f"Warning: no containers found for {svc} in {ns}")
            req_mib = 0.0
            lim_mib = 0.0
        else:
            # if multiple containers, sum them (safe approach) per pod
            req_mib = sum([c.get("req_mib",0.0) for c in info["containers"]])
            lim_mib = sum([c.get("lim_mib",0.0) for c in info["containers"]])

        current_total_by_service = lim_mib * replicas
        # compute how much additional memory per pod could be allocated if we split safe_free across all replicas of all services:
        # A pragmatic approach: allow each evaluated service to take safe_free divided by number of services (equal share)
        # but we also compute exact feasibility when proposed set.
        equal_share_per_service_total = safe_free / max(1, len(df))
        equal_share_per_pod = equal_share_per_service_total / max(1, replicas)

        # recommended limit per pod: don't exceed current * max_mult nor current + equal_share_per_pod
        recommended = min(lim_mib * max_mult if lim_mib>0 else lim_mib + equal_share_per_pod,
                          lim_mib + equal_share_per_pod)
        # if no current limit set (lim_mib == 0) but request exists, use request as baseline
        if lim_mib == 0 and req_mib > 0:
            recommended = min(req_mib * max_mult, req_mib + equal_share_per_pod)

        # If user provided a proposed value, parse and check
        proposed_mib = None
        proposed_safe = None
        if proposed != "":
            proposed_mib = to_mib(proposed)
            extra_needed_total = (proposed_mib - lim_mib) * replicas
            if extra_needed_total <= safe_free:
                proposed_safe = True
            else:
                proposed_safe = False
                unsafe_flag = True
        else:
            # when no proposed, mark recommendation safe if extra needed <= safe_free
            extra_needed_total = (recommended - lim_mib) * replicas
            if extra_needed_total <= safe_free:
                proposed_safe = True
            else:
                proposed_safe = False
                unsafe_flag = True

        results.append({
            "service": svc,
            "namespace": ns,
            "found_by": info.get("found_by",""),
            "replicas": replicas,
            "current_request_per_pod_mib": req_mib,
            "current_limit_per_pod_mib": lim_mib,
            "current_total_alloc_mib": current_total_by_service,
            "proposed_input": proposed or "",
            "proposed_input_mib": proposed_mib if proposed_mib is not None else "",
            "recommended_new_limit_per_pod_mib": recommended,
            "extra_needed_total_mib": round(extra_needed_total,1),
            "proposed_safe": proposed_safe
        })

    # Save report
    rdf = pd.DataFrame(results)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_xlsx = f"capacity_check_report_{ts}.xlsx"
    out_csv = f"capacity_check_report_{ts}.csv"

    summary = {
        "total_cluster_alloc_mib": total_cluster_mib,
        "total_requested_mib": total_requested_mib,
        "free_mib": free_mib,
        "safe_free_mib": safe_free,
        "services_checked": len(df),
        "unsafe_detected": unsafe_flag
    }

    with pd.ExcelWriter(out_xlsx, engine="openpyxl") as w:
        pd.DataFrame([summary]).to_excel(w, sheet_name="summary", index=False)
        rdf.to_excel(w, sheet_name="service_checks", index=False)
    rdf.to_csv(out_csv, index=False)

    print("\nReport written:")
    print(" -", out_xlsx)
    print(" -", out_csv)
    print("\nSummary:")
    print(f" Total cluster allocatable: {mib_to_human(summary['total_cluster_alloc_mib'])}")
    print(f" Total requested (current): {mib_to_human(summary['total_requested_mib'])}")
    print(f" Free headroom: {mib_to_human(summary['free_mib'])}")
    print(f" Safety margin applied: {safety_margin*100:.0f}% -> safe_free = {mib_to_human(summary['safe_free_mib'])}")
    print(f" Services checked: {summary['services_checked']}")
    print(f" Unsafe proposals found: {1 if unsafe_flag else 0}")

    if unsafe_flag:
        print("\n‚ö†Ô∏è  Some proposed changes or recommendations exceed safe cluster headroom.")
        print("Return code 2 (unsafe).")
        sys.exit(2)
    else:
        print("\n‚úÖ All proposals/recommendations fit within safe headroom.")
        sys.exit(0)

if __name__ == "__main__":
    p = argparse.ArgumentParser(description="Pre-deploy cluster capacity check for multiple services (kubectl required).")
    p.add_argument("--services-file", required=True, help="CSV file with columns: service,namespace,proposed_limit_per_pod(optional)")
    p.add_argument("--safety-margin", default=0.9, help="Fraction of free memory to consider safe (default 0.9 -> leave 10% buffer)")
    p.add_argument("--max-mult", default=2.0, help="Maximum multiplier on current limits for recommended increase (default 2.0)")
    args = p.parse_args()
    main(args)


‚∏ª

Sample run & sample output

Command:

python pre_deploy_capacity_check.py --services-file services.csv --safety-margin 0.9 --max-mult 2.0

Console (example):

Querying cluster nodes (kubectl)...
Total cluster allocatable memory: 210.00Gi (215040 Mi)
Querying all deployments to compute current requested memory (this may take a few seconds)...
Total requested memory (all deployments): 160.00Gi
Free headroom (allocatable - requested): 50.00Gi
Report written:
 - capacity_check_report_20251018_131512.xlsx
 - capacity_check_report_20251018_131512.csv

Summary:
 Total cluster allocatable: 210.00Gi
 Total requested (current): 160.00Gi
 Free headroom: 50.00Gi
 Safety margin applied: 90% -> safe_free = 45.00Gi
 Services checked: 15
 Unsafe proposals found: 0

‚úÖ All proposals/recommendations fit within safe headroom.

If a proposed limit exceeds safe space, the script prints warnings and exits with code 2 so a CI job can block the merge/sync.

‚∏ª

How to use this in your GitOps flow (ArgoCD)
	‚Ä¢	Add this script as a pre-sync / pre-merge check step in your CI pipeline (GitHub Actions / Jenkins / Bamboo).
	‚Ä¢	When a PR updates a service manifest (memory increases), the pipeline:
	1.	Ensures kubeconfig is available to run kubectl against the target cluster.
	2.	Runs this script using the services file (which can be compiled from the changed manifests or static list).
	3.	If exit code 0: allow merge / ArgoCD sync.
If exit code 2: block merge and surface the Excel report for the team to review.

‚∏ª

Notes & caveats
	‚Ä¢	This script uses resource requests (not actual runtime usage) to determine scheduling safety ‚Äî this is the correct conservative approach to avoid scheduling failures / OOMs.
	‚Ä¢	If you have multiple containers per pod, the script sums their memory (safe approach).
	‚Ä¢	For more advanced placement checks (node-level bin-packing), you‚Äôd need to implement scheduling simulation or call the Kubernetes scheduler (complex). This script is a pragmatic, cluster-level headroom check suitable for pre-deploy gating.
	‚Ä¢	If a service is stateful (StatefulSet, PVCs) the script still reports data but you should treat stateful changes more carefully (backup, scale tests).

‚∏ª

If you want, I can:
	‚Ä¢	Produce an alternate version that reads proposed manifests from a directory (the Git branch) and automatically extracts proposed resources.limits.memory values, or
	‚Ä¢	Add support to check CPU headroom as well,
	‚Ä¢	Or help you convert this into a GitHub Action / Bamboo task that runs automatically on PRs.

Which of those would you like next?